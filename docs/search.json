[
  {
    "objectID": "notebooks/3_perturbation_neoclassical.html",
    "href": "notebooks/3_perturbation_neoclassical.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "Perturbation: Neoclassical Growth Model\nWarm-up: install the ForwardDiff library. Use it to differentiate the function below. Check the jacobian function.\nNote: the signature of function f needs to be fixed first to allow for dual numbers as arguments.\n\nfunction f(x::Vector{Float64})::Vector{Float64}\n    a = x[1]\n    b = x[2]\n    x1 = a+b\n    x2 = a*exp(b)\n    return [x1,x2]\nend\n\nf (generic function with 1 method)\n\n\nCreate a NamedTuple to hold the model parameters.\nDefine two functions: - transition(z::Number, k::Number, p)::Tuple{Float64} which returns productivity and capital at date t+1 as a function of productivity, capital and investment at date t - arbitrage(z::Number, k::Number, i::Number, Z::Number, K::Number, I::Number, p)::Float64 which returns the residual of the euler equation (lower case variable for date t, upper case for date t+1)\nUsing multiple dispatch, define two variants of the same functions, that take vectors as input and output arguments: - arbitrage(s::Vector{T}, x::Vector{T}, S::Vector{T}, X::Vector{T}, p) where T<:Number - transition(s::Vector{T}, x::Vector{T}, p) where T<:Number\nWrite a function steady_state(p)::Tuple{Vector,Vector} which computes the steady-state of the model computed by hand. It returns two vectors, one for the states, one for the controls. Check that the steady-state satisfies the model equations.\nThe first order system satisfies: \\[\\begin{align}A s_t + B x_t + C s_{t+1} + D x_{t+1} & = & 0 \\\\\\\\\ns_{t+1} & = & E s_t + F x_t\n\\end{align}\\]\nDefine a structure PerturbedModel to hold matrices A,B,C,D,E,F.\nWrite a function first_order_model(s::Vector, x::Vector, p)::PerturbedModel, which returns the first order model, given the steady-state and the calibration. Suggestion: use ForwardDiff.jl library.\nWe look for a linear solution \\(x_t = X s_t\\) . Write the matrix equation which X must satisfy. Write a function residual(X::Array, M::PerturbedModel) which computes the residual of this equation for a given X.\nWrite a function T(X, M::PerturbedModel) which implements the time iteration step.\nWrite function linear_time_iteration(X_0::Matrix, m::PerturbedModel)::Matrix which implements the time iteration algorithm. Apply it to X0 = rand(1,2) and check that the result satisfies the first order model.\nDefine two linear operators L_S(S::Matrix, X_0::Matrix, m::PerturbedModel)::Matrix and L_T(S::Matrix, X_0::Matrix, m::PerturbedModel)::Matrix which implement the derivatives of the simulation and the time-iteration operator respectively.\nImplement a function spectral_radius(f::Function)::Float64 which implements the power iteration method to compute the biggest eigenvalues of the two previously defined operators. Check that Blamnchard-Kahn conditions are met.\nWrite a function simulate(s0::Vector, X::Matrix, p::Calibration, T::Int64)::Tuple{Matrix, Matrix} to simulate the model over \\(T\\) periods (by using the formula \\(\\Delta s_t = (E + F X) s_{t-1}\\). Return a matrix for the states (one line per date) and another matrix for the controls. Bonus: add a keyword option to compute variables levels or log-deviations. If possible, return a DataFrame object.\nMake some nice plots."
  },
  {
    "objectID": "notebooks/1_Julia_Basics.html",
    "href": "notebooks/1_Julia_Basics.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "developped at MIT on top of opensource technologies\n\nlinux / git / llvm\n\nsyntax inspired by Matlab but:\n\nmore consistent\nlots of features from high level languages\n\neverything is JIT-compiled\n\ninterpreted vs compiled treadeoff\n-> very fast\nmost of the base library is written in Julia\n\nopensource/free + vibrant community\n\nSome useful links from QuantEcon:\n\nJulia cheatsheet\nJulia-Matlab comparison\nJulia essentials\nVectors, arrays and matrices\n\nExcellent resources at: julialang - checkout JuliaAcademy, it’s free - ongoing MOOC at MIT\n\n\n\nHow I learnt: interpreted code is slow, so vectorize your coe.\n\nfunction stupid_loop(I,J,K)\n    t = 0.0\n    for i=1:I\n        for j=1:J\n            for k = 1:K\n                t += 1.0\n            end        \n        end\n    end\n    return t\nend\n@time [ stupid_loop(1000,1000,i) for i =1:10]\n\n  0.108801 seconds (66.67 k allocations: 3.520 MiB, 24.75% compilation time)\n\n\n10-element Vector{Float64}:\n 1.0e6\n 2.0e6\n 3.0e6\n 4.0e6\n 5.0e6\n 6.0e6\n 7.0e6\n 8.0e6\n 9.0e6\n 1.0e7\n\n\nCode is translated to LLVM code then to instructions for the processor. Note that processor instructions are shorter than LLVM code.\n\n@code_llvm stupid_loop(10,10,10)\n\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:1 within `stupid_loop`\ndefine double @julia_stupid_loop_1277(i64 signext %0\n\n\n, i64 signext %1, i64 signext %2) #0 {\ntop:\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:3 within `stupid_loop`\n; ┌ @ range.jl:5 within `Colon`\n; │┌ @ range.jl:393 within `UnitRange`\n; ││┌ @ range.jl:400 within `unitrange_last`\n     %.inv = icmp sgt i64 %0, 0\n     %. = select i1 %.inv, i64 %0, i64 0\n; └└└\n  br i1 %.inv, label %L17.preheader, label %L94\n\nL17.preheader:                                    ; preds = %top\n  %.inv26 = icmp sgt i64 %1, 0\n  %.24 = select i1 %.inv26, i64 %1, i64 0\n  %.inv27 = icmp sgt i64 %2, 0\n  %.25 = select i1 %.inv27, i64 %2, i64 0\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:4 within `stupid_loop`\n  %3 = select i1 %.inv26, i1 %.inv27, i1 false\n  br i1 %3, label %L35.preheader.split.us.us.us, label %L94\n\nL81.us.us:                                        ; preds = %L68.us.us.us\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:9 within `stupid_loop`\n; ┌ @ range.jl:883 within `iterate`\n; │┌ @ promotion.jl:477 within `==`\n    %.not29.us.us = icmp eq i64 %value_phi3.us.us, %.\n; │└\n   %4 = add nuw i64 %value_phi3.us.us, 1\n; └\n  br i1 %.not29.us.us, label %L94, label %L35.preheader.split.us.us.us\n\nL35.preheader.split.us.us.us:                     ; preds = %L81.us.us, %L17.preheader\n  %value_phi3.us.us = phi i64 [ %4, %L81.us.us ], [ 1, %L17.preheader ]\n  %value_phi4.us.us = phi double [ %6, %L81.us.us ], [ 0.000000e+00, %L17.preheader ]\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:5 within `stupid_loop`\n  br label %L53.preheader.us.us.us\n\nL53.preheader.us.us.us:                           ; preds = %L68.us.us.us, %L35.preheader.split.us.us.us\n  %value_phi8.us.us.us = phi double [ %6, %L68.us.us.us ], [ %value_phi4.us.us, %L35.preheader.split.us.us.us ]\n  %value_phi9.us.us.us = phi i64 [ %5, %L68.us.us.us ], [ 1, %L35.preheader.split.us.us.us ]\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:7 within `stupid_loop`\n  br label %L53.us.us.us\n\nL68.us.us.us:                                     ; preds = %L53.us.us.us\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:8 within `stupid_loop`\n; ┌ @ range.jl:883 within `iterate`\n; │┌ @ promotion.jl:477 within `==`\n    %.not28.us.us.us = icmp eq i64 %value_phi9.us.us.us, %.24\n; │└\n   %5 = add nuw i64 %value_phi9.us.us.us, 1\n; └\n  br i1 %.not28.us.us.us, label %L81.us.us, label %L53.preheader.us.us.us\n\nL53.us.us.us:                                     ; preds = %L53.us.us.us, %L53.preheader.us.us.us\n  %value_phi13.us.us.us = phi double [ %6, %L53.us.us.us ], [ %value_phi8.us.us.us, %L53.preheader.us.us.us ]\n  %value_phi14.us.us.us = phi i64 [ %7, %L53.us.us.us ], [ 1, %L53.preheader.us.us.us ]\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:6 within `stupid_loop`\n; ┌ @ float.jl:383 within `+`\n   %6 = fadd double %value_phi13.us.us.us, 1.000000e+00\n; └\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:7 within `stupid_loop`\n; ┌ @ range.jl:883 within `iterate`\n; │┌ @ promotion.jl:477 within `==`\n    %.not.us.us.us = icmp eq i64 %value_phi14.us.us.us, %.25\n; │└\n   %7 = add nuw i64 %value_phi14.us.us.us, 1\n; └\n  br i1 %.not.us.us.us, label %L68.us.us.us, label %L53.us.us.us\n\nL94:                                              ; preds = %L81.us.us, %L17.preheader, %top\n  %value_phi23 = phi double [ 0.000000e+00, %top ], [ %6, %L81.us.us ], [ 0.000000e+00, %L17.preheader ]\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:10 within `stupid_loop`\n  ret double %value_phi23\n}\n\n\n\n@code_native stupid_loop(10,10,10)\n\n    .text\n    .file   \"stupid_loop\"\n    .section    .rodata.cst8,\"aM\",@progbits,8\n    .p2align    3                               # -- Begin function julia_stupid_loop_1312\n.LCPI0_0:\n    .quad   0x3ff0000000000000              # double 1\n    .text\n    .globl  julia_stupid_loop_1312\n    .p2align    4, 0x90\n    .type   julia_stupid_loop_1312,@function\njulia_stupid_loop_1312:                 # @julia_stupid_loop_1312\n; ┌ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:1 within `stupid_loop`\n    .cfi_startproc\n# %bb.0:                                # %top\n    vxorpd  %xmm0, %xmm0, %xmm0\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:3 within `stupid_loop`\n; │┌ @ range.jl:5 within `Colon`\n; ││┌ @ range.jl:393 within `UnitRange`\n; │││┌ @ range.jl:400 within `unitrange_last`\n    testq   %rdi, %rdi\n; │└└└\n    jle .LBB0_9\n# %bb.1:                                # %L17.preheader\n    testq   %rsi, %rsi\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:4 within `stupid_loop`\n    jle .LBB0_9\n# %bb.2:                                # %L17.preheader\n    testq   %rdx, %rdx\n    jle .LBB0_9\n# %bb.3:                                # %L35.preheader.split.us.us.us.preheader\n    movq    %rdi, %rax\n    sarq    $63, %rax\n    andnq   %rdi, %rax, %r8\n    movq    %rsi, %rax\n    sarq    $63, %rax\n    andnq   %rsi, %rax, %rcx\n    movq    %rdx, %rax\n    sarq    $63, %rax\n    andnq   %rdx, %rax, %rax\n    vxorpd  %xmm0, %xmm0, %xmm0\n    movl    $1, %esi\n    movabsq $.LCPI0_0, %rdx\n    vmovsd  (%rdx), %xmm1                   # xmm1 = mem[0],zero\n    .p2align    4, 0x90\n.LBB0_5:                                # %L35.preheader.split.us.us.us\n                                        # =>This Loop Header: Depth=1\n                                        #     Child Loop BB0_6 Depth 2\n                                        #       Child Loop BB0_7 Depth 3\n    movl    $1, %edi\n    .p2align    4, 0x90\n.LBB0_6:                                # %L53.preheader.us.us.us\n                                        #   Parent Loop BB0_5 Depth=1\n                                        # =>  This Loop Header: Depth=2\n                                        #       Child Loop BB0_7 Depth 3\n    movq    %rax, %rdx\n    .p2align    4, 0x90\n.LBB0_7:                                # %L53.us.us.us\n                                        #   Parent Loop BB0_5 Depth=1\n                                        #     Parent Loop BB0_6 Depth=2\n                                        # =>    This Inner Loop Header: Depth=3\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:6 within `stupid_loop`\n; │┌ @ float.jl:383 within `+`\n    vaddsd  %xmm1, %xmm0, %xmm0\n; │└\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:7 within `stupid_loop`\n; │┌ @ range.jl:883 within `iterate`\n; ││┌ @ promotion.jl:477 within `==`\n    decq    %rdx\n; │└└\n    jne .LBB0_7\n# %bb.8:                                # %L68.us.us.us\n                                        #   in Loop: Header=BB0_6 Depth=2\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:8 within `stupid_loop`\n; │┌ @ range.jl:883 within `iterate`\n    leaq    1(%rdi), %rdx\n; ││┌ @ promotion.jl:477 within `==`\n    cmpq    %rcx, %rdi\n    movq    %rdx, %rdi\n; │└└\n    jne .LBB0_6\n# %bb.4:                                # %L81.us.us\n                                        #   in Loop: Header=BB0_5 Depth=1\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:9 within `stupid_loop`\n; │┌ @ range.jl:883 within `iterate`\n    leaq    1(%rsi), %rdx\n; ││┌ @ promotion.jl:477 within `==`\n    cmpq    %r8, %rsi\n    movq    %rdx, %rsi\n; │└└\n    jne .LBB0_5\n.LBB0_9:                                # %L94\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:10 within `stupid_loop`\n    retq\n.Lfunc_end0:\n    .size   julia_stupid_loop_1312, .Lfunc_end0-julia_stupid_loop_1312\n    .cfi_endproc\n; └\n                                        # -- End function\n    .section    \".note.GNU-stack\",\"\",@progbits\n\n\n\n\n\n\n\nAssignement operator is = (equality is ==, identity is ===)\n\n# Assign the value 10 to the variable x\nx = 10\n\n10\n\n\n\nx\n\n10\n\n\n\n2 == 3\n\nfalse\n\n\n\n# Variable names can have Unicode characters\n# To get ϵ in the REPL, type \\epsilon<TAB>\nσ = 34\n🦆 = 23\n🦈 = σ + 🦆\nϵ = 1e-4\n\n0.0001\n\n\nDefault semantic is pass-by-reference:\n\na = [1,2,3,4]\nb = a\na[1] = 10\nb\n\nTo work on a copy: copy or deepcopy\n\na = [1,2,3,4]\nb = copy(a)\na[1]=10\nb\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\na .== b\n\n4-element BitVector:\n 0\n 1\n 1\n 1\n\n\n\nc = b\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\nb = [1,2,3,4]\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\na .== b\n\n4-element BitVector:\n 1\n 1\n 1\n 1\n\n\n\nc === b\n\nfalse\n\n\n\n\n\n\n# for any object `typeof` returns the type\n?any\n\nUndefVarError: UndefVarError: help not defined\n  Welcome to Julia 1.8.4. The full manual is available at\n\n  https://docs.julialang.org\n\n  as well as many great tutorials and learning resources:\n\n  https://julialang.org/learning/\n\n  For help on a specific function or macro, type ? followed by its name, e.g.\n  ?cos, or ?@time, and press enter. Type ; to enter shell mode, ] to enter\n  package mode.\n\n  To exit the interactive session, type CTRL-D (press the control key together\n  with the d key), or type exit().\n\n\n\ntypeof(a)\n\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\n\n\n\ny = 2 + 2\n\n4\n\n\n\n-y\n\n-4\n\n\n\n0.34*23\n\n7.82\n\n\n\n3//4 + 2//3\n\n17//12\n\n\n\n# Scalar multiplication doesn't require *\n3(4 - 2)\n\n6\n\n\n\nx = 4\n2x\n\n\ntypeof(x)\n\n\nsizeof(a)\n\n\n\n\nEquality\n\n0 == 1\n\nfalse\n\n\n\n2 != 3\n\ntrue\n\n\n\n3 <= 4\n\ntrue\n\n\nIdentity\n\na = [34, 35]\nb = [34, 35]\nc = a\n\n\nc === a\n\n\nb === a\n\nBoolean operator\n\ntrue && false\n\nfalse\n\n\n\ntrue || false\n\ntrue\n\n\n\n!true\n\nfalse\n\n\n\n\n\n\n# Strings are written using double quotes\nstr = \"This is a string\"\n\n\"This is a string\"\n\n\n\nch = '🦆' # this is a character\n\n'🦆': Unicode U+1F986 (category So: Symbol, other)\n\n\n\n# Strings can also contain Unicode characters\nfancy_str = \"α is a string\"\n\n\n# String interpolation using $\n# The expression in parentheses is evaluated and the result is \n# inserted into the string\na = 2+2\n\"2 + 2 = $(a)\"\n\n\"2 + 2 = 4\"\n\n\n\nprintln(\"It took me $(a) iterations\")\n\nIt took me 4 iterations\n\n\n\n# String concatenation using *\n\"hello\" * \"world\"\n\n\"helloworld\"\n\n\n\nprintln(\"hello \", \"world\")\n\nhello world\n\n\n\n\n\nJulia has one-dimensional arrays. They are also called Vector.\n\nA = [1, 2]\n\n2-element Vector{Int64}:\n 1\n 2\n\n\nAll elements have the type:\n\nA = [1, 1.4]\n\n2-element Vector{Float64}:\n 1.0\n 1.4\n\n\n\ntypeof(A) == Vector{Int64}\n\nfalse\n\n\n\nA''\n\n2-element Vector{Float64}:\n 1.0\n 1.4\n\n\nTo get the size of an array:\n\nsize(A)\n\n(2,)\n\n\nArrays are mutable\n\nA[1] = 10\n\n10\n\n\n\nA\n\n2-element Vector{Float64}:\n 10.0\n  1.4\n\n\nJulia has one-based indexing: you refer to the first element as 1 (\\(\\neq\\) zero-based indexing in C or Python)\n\nA[2]\n\n1.4\n\n\nArrays are mutable and their size can be changed too:\n\npush!(A, 29)\nA\n\n6-element Vector{Float64}:\n 10.0\n  1.4\n 29.0\n 29.0\n 29.0\n 29.0\n\n\n\nA\n\n3-element Array{Float64,1}:\n 10.0\n  1.4\n 29.0\n\n\nTwo comments: - the push! operation is fast - ! is a julia convention to express the fact that push! mutates its first argument\n\n\n\n\nsize(A)  # is a tuple\n\n(6,)\n\n\n\n# you can create tuples with (,,,)\nt = (1,2,3,4)\n\n(1, 2, 3, 4)\n\n\n\nt\n\n(1, 2, 3, 4)\n\n\ntuples differ from arrays in two ways: - they are immutable - they can contain non-homogenous objects\n\nt[1]\n\n1\n\n\n\nt[1] = 2\n\nMethodError: MethodError: no method matching setindex!(::NTuple{4, Int64}, ::Int64, ::Int64)\n\n\n\ntypeof((1, \"1\", [1]))\n\nTuple{Int64, String, Vector{Int64}}\n\n\n2d arrays are also called matrices… and can be used for matrix multiplications.\n\n[3 4; 5 6]\n\n2×2 Matrix{Int64}:\n 3  4\n 5  6\n\n\n\na1 = [1,2,3,4]\na2 = [1,2,3,4]  .+ 4\n[a1 ;; a2]\n# cat(a1, a2; dims=2)\n\n4×2 Matrix{Int64}:\n 1  5\n 2  6\n 3  7\n 4  8\n\n\n\nb = [1 0.6 0]\n\n1×3 Array{Float64,2}:\n 1.0  0.6  0.0\n\n\n\nB = [0.1 0.2 0.3; 4 5 6]\n\nOther ways to construct arrays:\n\n# zero array\nt = zeros(2,3)\nt[1,2] = 23.2\nt\n\n2×3 Matrix{Float64}:\n 0.0  23.2  0.0\n 0.0   0.0  0.0\n\n\n\n# random array (uniform distribution)\nt= rand(3,3)\nt\n\n3×3 Matrix{Float64}:\n 0.151296  0.390327  0.239194\n 0.726286  0.371063  0.133779\n 0.037311  0.183624  0.72499\n\n\n\n# random array (normal distribution)\nt= randn(3,3)\nt\n\n3×3 Array{Float64,2}:\n -0.149832     0.973627  -0.407871\n -0.00251947  -1.46936   -0.141511\n  0.676479    -0.774655   0.349923\n\n\nVectorized operations take a ., even comparisons (pointwise operations)\n\nB = [1 2;3 4]\n\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\n\n\nB*B\n\n2×2 Matrix{Int64}:\n  7  10\n 15  22\n\n\n\nB .* B\n\n2×2 Matrix{Int64}:\n 1   4\n 9  16\n\n\n\nf(x) = x^2+1\n\nf (generic function with 1 method)\n\n\n\nf(43)\n\n1850\n\n\n\nf.(B)\n\n2×2 Matrix{Int64}:\n  2   5\n 10  17\n\n\nElements are always accessed with square brackets:\n\nB = [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\n\nYou get element $B_{ij}$ with `B[i,j]`\n\n\nB[1,2]\n\n2\n\n\nYou select a whole row/column with :\n\nB\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\n\nB[:,1]\n\n2-element Vector{Int64}:\n 1\n 4\n\n\n\nB[1,:]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\nB[:,1:2]\n\n2×2 Matrix{Int64}:\n 1  2\n 4  5\n\n\n\nB[:,1:end-1]\n\n2×2 Matrix{Int64}:\n 1  2\n 4  5\n\n\n\n\n\nConditions\n\nx = 0\nif x<0\n    # block\n    println(\"x is negative\")\nelseif (x > 0) # optional and unlimited\n    println(\"x is positive\")\nelse         # optional\n    println(\"x is zero\")\nend\n\nx is zero\n\n\nWhile\n\ni = 3\nwhile i > 0\n    println(i)\n    i -= 1 # decrement\nend\n\n3\n2\n1\n\n\nFor loops: your iterate over any iterable object: - range i1:i2 - vector - tuple\n\n# Iterate through ranges of numbers\nfor i ∈ (1:3)\n    println(i)\nend\n\n1\n2\n3\n\n\n\n# Iterate through arrays\ncities = [\"Boston\", \"New York\", \"Philadelphia\"]\nfor city in cities\n    println(city)\nend\n\nBoston\nNew York\nPhiladelphia\n\n\n\ncities\n\n3-element Vector{String}:\n \"Boston\"\n \"New York\"\n \"Philadelphia\"\n\n\n\nstates = [\"Massachussets\", \"New York\", \"Pennsylvania\"]\n\n3-element Vector{String}:\n \"Massachussets\"\n \"New York\"\n \"Pennsylvania\"\n\n\n\ntwo_by_two_iterable = zip(cities, states)\n\nzip([\"Boston\", \"New York\", \"Philadelphia\"], [\"Massachussets\", \"New York\", \"Pennsylvania\"])\n\n\n\ncollect(two_by_two_iterable)\n\n3-element Vector{Tuple{String, String}}:\n (\"Boston\", \"Massachussets\")\n (\"New York\", \"New York\")\n (\"Philadelphia\", \"Pennsylvania\")\n\n\n\n[two_by_two_iterable...]\n\n3-element Vector{Tuple{String, String}}:\n (\"Boston\", \"Massachussets\")\n (\"New York\", \"New York\")\n (\"Philadelphia\", \"Pennsylvania\")\n\n\n\n# Iterate through arrays of tuples using zip\nfor kw in zip(cities, states)\n    println(kw)\nend\n\n(\"Boston\", \"Massachussets\")\n(\"New York\", \"New York\")\n(\"Philadelphia\", \"Pennsylvania\")\n\n\n\n# Iterate through arrays of tuples using zip\nfor (city, state) in zip(cities, states)\n    println(\"$city, $state\")\nend\n\nBoston, Massachussets\nNew York, New York\nPhiladelphia, Pennsylvania\n\n\n\n# Iterate through arrays and their indices using enumerate\nfor (i, city) in enumerate(cities)\n    println(\"City $i is $city\")\nend\n\nCity 1 is Boston\nCity 2 is New York\nCity 3 is Philadelphia\n\n\n\nt = (;a = 2, b = 3)\nt[1]\nt.a\n\n2\n\n\n\n\n\n\n[1:10. ...] # unpack operator\n\n10-element Vector{Float64}:\n  1.0\n  2.0\n  3.0\n  4.0\n  5.0\n  6.0\n  7.0\n  8.0\n  9.0\n 10.0\n\n\n\n[i^2 for i  in 1:10] # collect with comprehension syntax\n\n10-element Vector{Int64}:\n   1\n   4\n   9\n  16\n  25\n  36\n  49\n  64\n  81\n 100\n\n\n\n[i^2 for i=1:100000000 if mod(i,2)==0] ;\n\n\ngen = (i^2 for i=1:100000000 if mod(i,2)==0)\n\nBase.Generator{Base.Iterators.Filter{var\"#42#44\", UnitRange{Int64}}, var\"#41#43\"}(var\"#41#43\"(), Base.Iterators.Filter{var\"#42#44\", UnitRange{Int64}}(var\"#42#44\"(), 1:100000000))\n\n\n\nsum(gen)\n\n338960700901149440\n\n\n\n\n\n\n\n\nA composite type is a collection of named fields that can be treated as a single value. They bear a passing resemblance to MATLAB structs.\nAll fields must be declared ahead of time. The double colon, ::, constrains a field to contain values of a certain type. This is optional for any field.\n\n# Type definition with 4 fields\nstruct ParameterFree\n    value  \n    transformation  \n    tex_label\n    description \nend\n\n\npf = ParameterFree(\"1\", x->x^2, \"\\\\sqrt{1+x^2}\", (\"a\",1))\n\nParameterFree(\"1\", var\"#9#10\"(), \"\\\\sqrt{1+x^2}\", (\"a\", 1))\n\n\n\npf.value\n\n\"1\"\n\n\nTwo reasons to create structures: - syntactic shortcut (you access the fields with .) - specify the types of the fields\n\n# Type definition\nstruct Parameter\n    value ::Float64\n    transformation ::Function # Function is a type!\n    tex_label::String\n    description::String\nend\n\n\np = Parameter(\"1\", x->x^2, \"\\\\sqrt{1+x^2}\", (\"a\",1))\n\nLoadError: \u001b[91mMethodError: \u001b[0mCannot `convert` an object of type \u001b[92mString\u001b[39m\u001b[0m to an object of type \u001b[91mFloat64\u001b[39m\u001b[39m\n\u001b[91m\u001b[0mClosest candidates are:\u001b[39m\n\u001b[91m\u001b[0m  convert(::Type{T}, \u001b[91m::T\u001b[39m) where T<:Number at number.jl:6\u001b[39m\n\u001b[91m\u001b[0m  convert(::Type{T}, \u001b[91m::Number\u001b[39m) where T<:Number at number.jl:7\u001b[39m\n\u001b[91m\u001b[0m  convert(::Type{T}, \u001b[91m::Base.TwicePrecision\u001b[39m) where T<:Number at twiceprecision.jl:250\u001b[39m\n\u001b[91m\u001b[0m  ...\u001b[39m\n\n\n\np = Parameter(0.43, x->x^2, \"\\\\sqrt{1+x^2}\", \"This is a description\")\n\nParameter(0.43, var\"#13#14\"(), \"\\\\sqrt{1+x^2}\", \"This is a description\")\n\n\n\np.value\n\n0.43\n\n\nWhen a type with \\(n\\) fields is defined, a constructor (function that creates an instance of that type) that takes \\(n\\) ordered arguments is automatically created. Additional constructors can be defined for convenience.\n\n# Creating an instance of the Parameter type using the default\n# constructor\nβ = Parameter(0.9, identity, \"\\\\beta\", \"Discount rate\")\n\nParameter(0.9, identity, \"\\\\beta\", \"Discount rate\")\n\n\n\nfunction Parameter(value)\n    return Parameter(value, x->x, \"x\", \"Anonymous\")\nend\n\nParameter\n\n\n\nParameter(0.4)\n\nParameter(0.4, var\"#15#16\"(), \"x\", \"Anonymous\")\n\n\n\nParameter(value, transformation, tex) = Parameter(value, transformation, tex, \"no description\")\n\nParameter\n\n\n\nmethods( Parameter )\n\n# 4 methods for type constructor: Parameter(value::Float64, transformation::Function, tex_label::String, description::String) in Main at In[100]:3  Parameter(value) in Main at In[106]:1  Parameter(value, transformation, tex) in Main at In[108]:1  Parameter(value, transformation, tex_label, description) in Main at In[100]:3 \n\n\n\n# Alternative constructors end with an appeal to the default\n# constructor\nfunction Parameter(value::Float64, tex_label::String)\n    transformation = identity\n    description = \"No description available\"\n    return Parameter(value, transformation, tex_label, description)\nend\n\nα = Parameter(0.5, \"\\alpha\")\n\nParameter(0.5, identity, \"\\alpha\", \"No description available\")\n\n\nNow the function Parameter has two different methods with different signatures:\n\nmethods(Parameter)\n\n# 4 methods for type constructor: Parameter(value::Float64, transformation::Function, tex_label::String, description::String) in Main at In[1]:3  Parameter(value::Float64, tex_label::String) in Main at In[8]:4  Parameter(value, transformation, tex) in Main at In[5]:1  Parameter(value, transformation, tex_label, description) in Main at In[1]:3 \n\n\nWe have seen that a function can have several implementations, called methods, for different number of arguments, or for different types of arguments.\n\nfun(x::Int64, y::Int64) = x^3 + y\n\nfun (generic function with 1 method)\n\n\n\nfun(x::Float64, y::Int64) = x/2 + y\n\nfun (generic function with 2 methods)\n\n\n\nfun(2, 2)\n\n10\n\n\n\nfun(2.0, 2)\n\n3.0\n\n\n\nα.tex_label\n\n\n# Access a particular field using .\nα.value\n\n0.5\n\n\n\n# Fields are modifiable and can be assigned to, like \n# ordinary variables\nα.value = 0.75\n\n\n\n\n\nby default structures in Julia are non-mutable\n\np.value = 3.0\n\nLoadError: \u001b[91msetfield! immutable struct of type Parameter cannot be changed\u001b[39m\n\n\n\nmutable struct Params\n    x:: Float64\n    y:: Float64\nend\n\n\npos = Params(0.4, 0.2)\n\nParams(0.4, 0.2)\n\n\n\npos.x = 0.5\n\n0.5\n\n\n\n\n\nParameterized types are data types that are defined to handle values identically regardless of the type of those values.\nArrays are a familiar example. An Array{T,1} is a one-dimensional array filled with objects of any type T (e.g. Float64, String).\n\n# Defining a parametric point\nstruct Duple{T} # T is a parameter to the type Duple\n    x::T\n    y::T\nend\n\n\nDuple(3, 3)\n\nDuple{Int64}(3, 3)\n\n\n\nDuple(3, -1.0)\n\nLoadError: \u001b[91mMethodError: no method matching Duple(::Int64, ::Float64)\u001b[39m\n\u001b[91m\u001b[0mClosest candidates are:\u001b[39m\n\u001b[91m\u001b[0m  Duple(::T, \u001b[91m::T\u001b[39m) where T at In[127]:3\u001b[39m\n\n\n\nstruct Truple{T}\n    x::Duple{T}\n    z::T\nend\n\nThis single declaration defines an unlimited number of new types: Duple{String}, Duple{Float64}, etc. are all immediately usable.\n\nsizeof(3.0)\n\n8\n\n\n\nsizeof( Duple(3.0, -15.0) )\n\n16\n\n\n\n# What happens here?\nDuple(1.5, 3)\n\n\nstruct Truple3{T,S}\n    x::Tuple{T,S}\n    z::S\nend\n\nWe can also restrict the type parameter T:\n\ntypeof(\"S\") <: Number\n\nfalse\n\n\n\ntypeof(4) <: Number\n\ntrue\n\n\n\n# T can be any subtype of Number, but nothing else\nstruct PlanarCoordinate{T<:Number}\n    x::T\n    y::T\nend\n\n\nPlanarCoordinate(\"4th Ave\", \"14th St\")\n\nMethodError: MethodError: no method matching PlanarCoordinate(::String, ::String)\n\n\n\nPlanarCoordinate(2//3, 8//9)\n\nPlanarCoordinate{Rational{Int64}}(2//3, 8//9)\n\n\nArrays are an exemple of mutable, parameterized types\n\n\n\nYou can write all your code without thinking about types at all. If you do this, however, you’ll be missing out on some of the biggest benefits of using Julia.\nIf you understand types, you can:\n\nWrite faster code\nWrite expressive, clear, and well-structured programs (keep this in mind when we talk about functions)\nReason more clearly about how your code works\n\nEven if you only use built-in functions and types, your code still takes advantage of Julia’s type system. That’s why it’s important to understand what types are and how to use them.\n\n# Example: writing type-stable functions\nfunction sumofsins_unstable(n::Integer)  \n    sum = 0:: Integer\n    for i in 1:n  \n        sum += sin(3.4)  \n    end  \n    return sum \nend  \n\nfunction sumofsins_stable(n::Integer)  \n    sum = 0.0 :: Float64\n    for i in 1:n  \n        sum += sin(3.4)  \n    end  \n    return sum \nend\n\n# Compile and run\nsumofsins_unstable(Int(1e5))\nsumofsins_stable(Int(1e5))\n\n-25554.110202663698\n\n\n\n@time sumofsins_unstable(Int(1e5))\n\n  0.000176 seconds\n\n\n-25554.110202663698\n\n\n\n@time sumofsins_stable(Int(1e5))\n\n  0.000168 seconds\n\n\n-25554.110202663698\n\n\nIn sumofsins_stable, the compiler is guaranteed that sum is of type Float64 throughout; therefore, it saves time and memory. On the other hand, in sumofsins_unstable, the compiler must check the type of sum at each iteration of the loop. Let’s look at the LLVM intermediate representation.\n\n\n\nSo far we have defined functions over argument lists of any type. Methods allow us to define functions “piecewise”. For any set of input arguments, we can define a method, a definition of one possible behavior for a function.\n\n# Define one method of the function print_type\nfunction print_type(x::Number)\n    println(\"$x is a number\")\nend\n\nprint_type (generic function with 1 method)\n\n\n\n# Define another method\nfunction print_type(x::String)\n    println(\"$x is a string\")\nend\n\nprint_type (generic function with 2 methods)\n\n\n\n# Define yet another method\nfunction print_type(x::Number, y::Number)\n    println(\"$x and $y are both numbers\")\nend\n\nprint_type (generic function with 3 methods)\n\n\n\n# See all methods for a given function\nmethods(print_type)\n\n# 3 methods for generic function print_type: print_type(x::String) in Main at In[53]:3  print_type(x::Number) in Main at In[51]:3  print_type(x::Number, y::Number) in Main at In[54]:3 \n\n\nJulia uses multiple dispatch to decide which method of a function to execute when a function is applied. In particular, Julia compares the types of all arguments to the signatures of the function’s methods in order to choose the applicable one, not just the first (hence “multiple”).\n\nprint_type(5)\n\n5 is a number\n\n\n\nprint_type(\"foo\")\n\nfoo is a string\n\n\n\nprint_type([1, 2, 3])\n\nMethodError: MethodError: no method matching print_type(::Array{Int64,1})\nClosest candidates are:\n  print_type(!Matched::String) at In[53]:3\n  print_type(!Matched::Number) at In[51]:3\n  print_type(!Matched::Number, !Matched::Number) at In[54]:3\n\n\n\n\nJulia supports a short function definition for one-liners\n\nf(x::Float64) = x^2.0\nf(x::Int64) = x^3\n\nAs well as a special syntax for anonymous functions\n\nu->u^2\n\n\nmap(u->u^2, [1,2,3,4])\n\n\n\n\n\n\nf(a,b,c=true; algo=\"newton\")\n\nUndefVarError: UndefVarError: f not defined\n\n\n\n\n\n\nt = (1,2,4)\n\n(1, 2, 4)\n\n\n\na,b,c = t\n\n(1, 2, 4)\n\n\n\n[(1:10)...]\n\n10-element Array{Int64,1}:\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n\n\n\ncat([4,3], [0,1]; dims=1)\n\n4-element Array{Int64,1}:\n 4\n 3\n 0\n 1\n\n\n\nl = [[4,3], [0,1], [0, 0], [1, 1]]\n# how do I concatenate it ?\n\ncat(l...; dims=1) ### see python's f(*s)\n\n8-element Array{Int64,1}:\n 4\n 3\n 0\n 1\n 0\n 0\n 1\n 1\n\n\n\n\n\nAs we’ve seen, you can use Julia just like you use MATLAB and get faster code. However, to write faster and better code, attempt to write in a “Julian” manner:\n\nDefine composite types as logically needed\nWrite type-stable functions for best performance\nTake advantage of multiple dispatch to write code that looks like math\nAdd methods to existing functions\n\n\n\n\nHow is Julia so fast? Julia is just-in-time (JIT) compiled, which means (according to this StackExchange answer):\n\nA JIT compiler runs after the program has started and compiles the code (usually bytecode or some kind of VM instructions) on the fly (or just-in-time, as it’s called) into a form that’s usually faster, typically the host CPU’s native instruction set. A JIT has access to dynamic runtime information whereas a standard compiler doesn’t and can make better optimizations like inlining functions that are used frequently.\n\n\nThis is in contrast to a traditional compiler that compiles all the code to machine language before the program is first run.\n\nIn particular, Julia uses type information at runtime to optimize how your code is compiled. This is why writing type-stable code makes such a difference in speed!\n\n\n\n\nTaken from QuantEcon’s Julia Essentials and Vectors, Arrays, and Matrices lectures.\n\nConsider the polynomial \\[p(x) = \\sum_{i=0}^n a_0 x^0\\] Using enumerate, write a function p such that p(x, coeff) computes the value of the polynomial with coefficients coeff evaluated at x.\n\n\n\n\nppp (generic function with 1 method)\n\n\n\nWrite a function solve_discrete_lyapunov that solves the discrete Lyapunov equation \\[S = ASA' + \\Sigma \\Sigma'\\] using the iterative procedure \\[S_0 = \\Sigma \\Sigma'\\] \\[S_{t+1} = A S_t A' + \\Sigma \\Sigma'\\] taking in as arguments the \\(n \\times n\\) matrix \\(A\\), the \\(n \\times k\\) matrix \\(\\Sigma\\), and a number of iterations."
  },
  {
    "objectID": "notebooks/2_solow.html",
    "href": "notebooks/2_solow.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "Tutorial: Convergence\n\nSolow Model\nA representative agent uses capital \\(k_t\\) to produce \\(y_t\\) using the following production function:\n\\[y_t = k_t^{\\alpha}\\]\nHe chooses to consume an amount \\(c_t \\in ]0, y_t]\\) and invests what remains:\n\\[i_t = y_t - c_t\\]\nHe accumulates capital \\(k_t\\) according to:\n\\[k_{t+1} = \\left( 1-\\delta \\right) k_{t} + i_{t}\\]\nwhere \\(\\delta\\) is the depreciation rate and \\(i_t\\) is the amount invested.\nThe goal of the representative agent is to maximize:\n\\[\\sum_{t\\geq 0} \\beta^t U(c_t)\\]\nwhere \\(U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}\\) and \\(\\beta<1\\) is the discount factor.\nFor now, we ignore the objective and assume that the saving rate \\(s=\\frac{c_t}{y_t}\\) is constant over time.\nCreate a NamedTuple to hold parameter values \\(\\beta=0.96\\), \\(\\delta=0.1\\), \\(\\alpha=0.3\\), \\(\\gamma=4\\).\nWrite down the formula of function \\(f\\) such that \\(k_{t+1}\\): \\(k_{t+1} = f(k_t)\\).\nDefine a function f(k::Float64, p::NamedTuple)::Float64 to represent \\(f\\) for a given calibration\nWrite a function simulate(k0::Float64, T::Int, p::NamedTuple)::Vector{Float64} to compute the simulation over T periods starting from initial capital level k0.\nMake a nice plot to illustrate the convergence. Do we get convergence from any initial level of capital?\nSuppose you were interested in using f to compute the steady-state. What would you propose to measure convergence speed? To speed-up convergence? Implement these ideas."
  },
  {
    "objectID": "notebooks/2_solow_correction.html",
    "href": "notebooks/2_solow_correction.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "Convergence: Solow Model\n\nSolow Model\nA representative agent uses capital \\(k_t\\) to produce \\(y_t\\) using the following production function:\n\\[y_t = k_t^{\\alpha}\\]\nHe chooses to consume an amount \\(c_t \\in ]0, y_t]\\) and invests what remains:\n\\[i_t = y_t - c_t\\]\nHe accumulates capital \\(k_t\\) according to:\n\\[k_{t+1} = \\left( 1-\\delta \\right) k_{t} + i_{t}\\]\nwhere \\(\\delta\\) is the depreciation rate and \\(i_t\\) is the amount invested.\nThe goal of the representative agent is to maximize:\n\\[\\sum_{t\\geq 0} \\beta^t U(c_t)\\]\nwhere \\(U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}\\) and \\(\\beta<1\\) is the discount factor.\nFor now, we ignore the objective and assume that the saving rate \\(s=\\frac{c_t}{y_t}\\) is constant over time.\nCreate a NamedTuple to hold parameter values \\(\\beta=0.96\\), \\(\\delta=0.1\\), \\(\\alpha=0.3\\), \\(\\gamma=4\\).\n\np = (;β=0.96, δ=0.1, α=0.3, γ=4.0)\n\n(β = 0.96, δ = 0.1, α = 0.3, γ = 4.0)\n\n\nWrite down the formula of function \\(f\\) such that \\(k_{t+1}\\): \\(k_{t+1} = f(k_t)\\).\n\\[k_{t+1}= (1-\\delta) k_t + s k_t^{\\alpha}\\]\nDefine a function f(k::Float64, p::NamedTuple)::Float64 to represent \\(f\\) for a given calibration\n\n# function f(k::Float64, p::NamedTuple)\n\n# we added a keyword argument to specify the saving rate\nfunction f(k, p; s=0.2)\n    val = k*(1-p.δ) + s*k^p.α\n    return val \nend\n\nf (generic function with 2 methods)\n\n\n\nf(0.1, p)\n\n0.1902374467254545\n\n\nWrite a function simulate(k0::Float64, T::Int, p::NamedTuple)::Vector{Float64} to compute the simulation over T periods starting from initial capital level k0.\n\nfunction simulate(k, T, p; s=0.2)\n    v = [k]\n    for t=1:T\n        k0 = v[end]\n        push!(v, f(k0, p; s=s))\n    end\n    return v\nend\n\nsimulate (generic function with 1 method)\n\n\n\nsimulate(0.2, 100, p)\n\n101-element Vector{Float64}:\n 0.2\n 0.30340677254400195\n 0.4129080792968781\n 0.525003373019628\n 0.6373491155565568\n 0.7483344417178636\n 0.856841626057419\n 0.9620988898971137\n 1.0635841029776287\n 1.1609587708409257\n ⋮\n 2.6876186883735818\n 2.6879113388853018\n 2.6881835130861362\n 2.6884366430712516\n 2.6886720608576273\n 2.688891005366757\n 2.689094628921639\n 2.6892840032916374\n 2.6894601253165105\n\n\n\nfunction simulate_preallocate(k, T, p; s=0.2)\n    v = zeros(T)\n    v[1] = k\n    for t = 1:(T-1)\n        k0 = v[t]\n        v[t+1] = f(k0, p; s=s)\n    end\n    return v\nend\n\nsimulate_preallocate (generic function with 1 method)\n\n\n\n@time simulate(0.2, 1000000, p);\n@time simulate_preallocate(0.2, 1000000, p);\n\n  0.067737 seconds (14 allocations: 9.781 MiB)\n  0.055613 seconds (2 allocations: 7.629 MiB)\n\n\nMake a nice plot to illustrate the convergence. Do we get convergence from any initial level of capital?\nSuppose you were interested in using f to compute the steady-state. What would you propose to measure convergence speed? To speed-up convergence? Implement these ideas.\n\nfunction steady_state(p; s=0.2)\n    sim = simulate(0.1, 1000, p)\n    return sim[end]\nend\n\nsteady_state (generic function with 1 method)\n\n\n\nsteady_state(p)\n\n2.691800385264708"
  },
  {
    "objectID": "notebooks/1_Julia_Basics_correction.html",
    "href": "notebooks/1_Julia_Basics_correction.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "developped at MIT on top of opensource technologies\n\nlinux / git / llvm\n\nsyntax inspired by Matlab but:\n\nmore consistent\nlots of features from high level languages\n\neverything is JIT-compiled\n\ninterpreted vs compiled treadeoff\n-> very fast\nmost of the base library is written in Julia\n\nopensource/free + vibrant community\n\nSome useful links from QuantEcon:\n\nJulia cheatsheet\nJulia-Matlab comparison\nJulia essentials\nVectors, arrays and matrices\n\nExcellent resources at: julialang - checkout JuliaAcademy, it’s free - ongoing MOOC at MIT\n\n\n\nHow I learnt: interpreted code is slow, so vectorize your coe.\n\nfunction stupid_loop(I,J,K)\n    t = 0.0\n    for i=1:I\n        for j=1:J\n            for k = 1:K\n                t += 1.0\n            end        \n        end\n    end\n    return t\nend\n@time [ stupid_loop(1000,1000,i) for i =1:10]\n\n  0.108801 seconds (66.67 k allocations: 3.520 MiB, 24.75% compilation time)\n\n\n10-element Vector{Float64}:\n 1.0e6\n 2.0e6\n 3.0e6\n 4.0e6\n 5.0e6\n 6.0e6\n 7.0e6\n 8.0e6\n 9.0e6\n 1.0e7\n\n\nCode is translated to LLVM code then to instructions for the processor. Note that processor instructions are shorter than LLVM code.\n\n@code_llvm stupid_loop(10,10,10)\n\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:1 within `stupid_loop`\ndefine double @julia_stupid_loop_1277(i64 signext %0\n\n\n, i64 signext %1, i64 signext %2) #0 {\ntop:\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:3 within `stupid_loop`\n; ┌ @ range.jl:5 within `Colon`\n; │┌ @ range.jl:393 within `UnitRange`\n; ││┌ @ range.jl:400 within `unitrange_last`\n     %.inv = icmp sgt i64 %0, 0\n     %. = select i1 %.inv, i64 %0, i64 0\n; └└└\n  br i1 %.inv, label %L17.preheader, label %L94\n\nL17.preheader:                                    ; preds = %top\n  %.inv26 = icmp sgt i64 %1, 0\n  %.24 = select i1 %.inv26, i64 %1, i64 0\n  %.inv27 = icmp sgt i64 %2, 0\n  %.25 = select i1 %.inv27, i64 %2, i64 0\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:4 within `stupid_loop`\n  %3 = select i1 %.inv26, i1 %.inv27, i1 false\n  br i1 %3, label %L35.preheader.split.us.us.us, label %L94\n\nL81.us.us:                                        ; preds = %L68.us.us.us\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:9 within `stupid_loop`\n; ┌ @ range.jl:883 within `iterate`\n; │┌ @ promotion.jl:477 within `==`\n    %.not29.us.us = icmp eq i64 %value_phi3.us.us, %.\n; │└\n   %4 = add nuw i64 %value_phi3.us.us, 1\n; └\n  br i1 %.not29.us.us, label %L94, label %L35.preheader.split.us.us.us\n\nL35.preheader.split.us.us.us:                     ; preds = %L81.us.us, %L17.preheader\n  %value_phi3.us.us = phi i64 [ %4, %L81.us.us ], [ 1, %L17.preheader ]\n  %value_phi4.us.us = phi double [ %6, %L81.us.us ], [ 0.000000e+00, %L17.preheader ]\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:5 within `stupid_loop`\n  br label %L53.preheader.us.us.us\n\nL53.preheader.us.us.us:                           ; preds = %L68.us.us.us, %L35.preheader.split.us.us.us\n  %value_phi8.us.us.us = phi double [ %6, %L68.us.us.us ], [ %value_phi4.us.us, %L35.preheader.split.us.us.us ]\n  %value_phi9.us.us.us = phi i64 [ %5, %L68.us.us.us ], [ 1, %L35.preheader.split.us.us.us ]\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:7 within `stupid_loop`\n  br label %L53.us.us.us\n\nL68.us.us.us:                                     ; preds = %L53.us.us.us\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:8 within `stupid_loop`\n; ┌ @ range.jl:883 within `iterate`\n; │┌ @ promotion.jl:477 within `==`\n    %.not28.us.us.us = icmp eq i64 %value_phi9.us.us.us, %.24\n; │└\n   %5 = add nuw i64 %value_phi9.us.us.us, 1\n; └\n  br i1 %.not28.us.us.us, label %L81.us.us, label %L53.preheader.us.us.us\n\nL53.us.us.us:                                     ; preds = %L53.us.us.us, %L53.preheader.us.us.us\n  %value_phi13.us.us.us = phi double [ %6, %L53.us.us.us ], [ %value_phi8.us.us.us, %L53.preheader.us.us.us ]\n  %value_phi14.us.us.us = phi i64 [ %7, %L53.us.us.us ], [ 1, %L53.preheader.us.us.us ]\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:6 within `stupid_loop`\n; ┌ @ float.jl:383 within `+`\n   %6 = fadd double %value_phi13.us.us.us, 1.000000e+00\n; └\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:7 within `stupid_loop`\n; ┌ @ range.jl:883 within `iterate`\n; │┌ @ promotion.jl:477 within `==`\n    %.not.us.us.us = icmp eq i64 %value_phi14.us.us.us, %.25\n; │└\n   %7 = add nuw i64 %value_phi14.us.us.us, 1\n; └\n  br i1 %.not.us.us.us, label %L68.us.us.us, label %L53.us.us.us\n\nL94:                                              ; preds = %L81.us.us, %L17.preheader, %top\n  %value_phi23 = phi double [ 0.000000e+00, %top ], [ %6, %L81.us.us ], [ 0.000000e+00, %L17.preheader ]\n;  @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:10 within `stupid_loop`\n  ret double %value_phi23\n}\n\n\n\n@code_native stupid_loop(10,10,10)\n\n    .text\n    .file   \"stupid_loop\"\n    .section    .rodata.cst8,\"aM\",@progbits,8\n    .p2align    3                               # -- Begin function julia_stupid_loop_1312\n.LCPI0_0:\n    .quad   0x3ff0000000000000              # double 1\n    .text\n    .globl  julia_stupid_loop_1312\n    .p2align    4, 0x90\n    .type   julia_stupid_loop_1312,@function\njulia_stupid_loop_1312:                 # @julia_stupid_loop_1312\n; ┌ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:1 within `stupid_loop`\n    .cfi_startproc\n# %bb.0:                                # %top\n    vxorpd  %xmm0, %xmm0, %xmm0\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:3 within `stupid_loop`\n; │┌ @ range.jl:5 within `Colon`\n; ││┌ @ range.jl:393 within `UnitRange`\n; │││┌ @ range.jl:400 within `unitrange_last`\n    testq   %rdi, %rdi\n; │└└└\n    jle .LBB0_9\n# %bb.1:                                # %L17.preheader\n    testq   %rsi, %rsi\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:4 within `stupid_loop`\n    jle .LBB0_9\n# %bb.2:                                # %L17.preheader\n    testq   %rdx, %rdx\n    jle .LBB0_9\n# %bb.3:                                # %L35.preheader.split.us.us.us.preheader\n    movq    %rdi, %rax\n    sarq    $63, %rax\n    andnq   %rdi, %rax, %r8\n    movq    %rsi, %rax\n    sarq    $63, %rax\n    andnq   %rsi, %rax, %rcx\n    movq    %rdx, %rax\n    sarq    $63, %rax\n    andnq   %rdx, %rax, %rax\n    vxorpd  %xmm0, %xmm0, %xmm0\n    movl    $1, %esi\n    movabsq $.LCPI0_0, %rdx\n    vmovsd  (%rdx), %xmm1                   # xmm1 = mem[0],zero\n    .p2align    4, 0x90\n.LBB0_5:                                # %L35.preheader.split.us.us.us\n                                        # =>This Loop Header: Depth=1\n                                        #     Child Loop BB0_6 Depth 2\n                                        #       Child Loop BB0_7 Depth 3\n    movl    $1, %edi\n    .p2align    4, 0x90\n.LBB0_6:                                # %L53.preheader.us.us.us\n                                        #   Parent Loop BB0_5 Depth=1\n                                        # =>  This Loop Header: Depth=2\n                                        #       Child Loop BB0_7 Depth 3\n    movq    %rax, %rdx\n    .p2align    4, 0x90\n.LBB0_7:                                # %L53.us.us.us\n                                        #   Parent Loop BB0_5 Depth=1\n                                        #     Parent Loop BB0_6 Depth=2\n                                        # =>    This Inner Loop Header: Depth=3\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:6 within `stupid_loop`\n; │┌ @ float.jl:383 within `+`\n    vaddsd  %xmm1, %xmm0, %xmm0\n; │└\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:7 within `stupid_loop`\n; │┌ @ range.jl:883 within `iterate`\n; ││┌ @ promotion.jl:477 within `==`\n    decq    %rdx\n; │└└\n    jne .LBB0_7\n# %bb.8:                                # %L68.us.us.us\n                                        #   in Loop: Header=BB0_6 Depth=2\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:8 within `stupid_loop`\n; │┌ @ range.jl:883 within `iterate`\n    leaq    1(%rdi), %rdx\n; ││┌ @ promotion.jl:477 within `==`\n    cmpq    %rcx, %rdi\n    movq    %rdx, %rdi\n; │└└\n    jne .LBB0_6\n# %bb.4:                                # %L81.us.us\n                                        #   in Loop: Header=BB0_5 Depth=1\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:9 within `stupid_loop`\n; │┌ @ range.jl:883 within `iterate`\n    leaq    1(%rsi), %rdx\n; ││┌ @ promotion.jl:477 within `==`\n    cmpq    %r8, %rsi\n    movq    %rdx, %rsi\n; │└└\n    jne .LBB0_5\n.LBB0_9:                                # %L94\n; │ @ /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:10 within `stupid_loop`\n    retq\n.Lfunc_end0:\n    .size   julia_stupid_loop_1312, .Lfunc_end0-julia_stupid_loop_1312\n    .cfi_endproc\n; └\n                                        # -- End function\n    .section    \".note.GNU-stack\",\"\",@progbits\n\n\n\n\n\n\n\nAssignement operator is = (equality is ==, identity is ===)\n\n# Assign the value 10 to the variable x\nx = 10\n\n10\n\n\n\nx\n\n10\n\n\n\n2 == 3\n\nfalse\n\n\n\n# Variable names can have Unicode characters\n# To get ϵ in the REPL, type \\epsilon<TAB>\nσ = 34\n🦆 = 23\n🦈 = σ + 🦆\nϵ = 1e-4\n\n0.0001\n\n\nDefault semantic is pass-by-reference:\n\na = [1,2,3,4]\nb = a\na[1] = 10\nb\n\nTo work on a copy: copy or deepcopy\n\na = [1,2,3,4]\nb = copy(a)\na[1]=10\nb\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\na .== b\n\n4-element BitVector:\n 0\n 1\n 1\n 1\n\n\n\nc = b\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\nb = [1,2,3,4]\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\na .== b\n\n4-element BitVector:\n 1\n 1\n 1\n 1\n\n\n\nc === b\n\nfalse\n\n\n\n\n\n\n# for any object `typeof` returns the type\n?any\n\nUndefVarError: UndefVarError: help not defined\n  Welcome to Julia 1.8.4. The full manual is available at\n\n  https://docs.julialang.org\n\n  as well as many great tutorials and learning resources:\n\n  https://julialang.org/learning/\n\n  For help on a specific function or macro, type ? followed by its name, e.g.\n  ?cos, or ?@time, and press enter. Type ; to enter shell mode, ] to enter\n  package mode.\n\n  To exit the interactive session, type CTRL-D (press the control key together\n  with the d key), or type exit().\n\n\n\ntypeof(a)\n\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\n\n\n\ny = 2 + 2\n\n4\n\n\n\n-y\n\n-4\n\n\n\n0.34*23\n\n7.82\n\n\n\n3//4 + 2//3\n\n17//12\n\n\n\n# Scalar multiplication doesn't require *\n3(4 - 2)\n\n6\n\n\n\nx = 4\n2x\n\n\ntypeof(x)\n\n\nsizeof(a)\n\n\n\n\nEquality\n\n0 == 1\n\nfalse\n\n\n\n2 != 3\n\ntrue\n\n\n\n3 <= 4\n\ntrue\n\n\nIdentity\n\na = [34, 35]\nb = [34, 35]\nc = a\n\n\nc === a\n\n\nb === a\n\nBoolean operator\n\ntrue && false\n\nfalse\n\n\n\ntrue || false\n\ntrue\n\n\n\n!true\n\nfalse\n\n\n\n\n\n\n# Strings are written using double quotes\nstr = \"This is a string\"\n\n\"This is a string\"\n\n\n\nch = '🦆' # this is a character\n\n'🦆': Unicode U+1F986 (category So: Symbol, other)\n\n\n\n# Strings can also contain Unicode characters\nfancy_str = \"α is a string\"\n\n\n# String interpolation using $\n# The expression in parentheses is evaluated and the result is \n# inserted into the string\na = 2+2\n\"2 + 2 = $(a)\"\n\n\"2 + 2 = 4\"\n\n\n\nprintln(\"It took me $(a) iterations\")\n\nIt took me 4 iterations\n\n\n\n# String concatenation using *\n\"hello\" * \"world\"\n\n\"helloworld\"\n\n\n\nprintln(\"hello \", \"world\")\n\nhello world\n\n\n\n\n\nJulia has one-dimensional arrays. They are also called Vector.\n\nA = [1, 2]\n\n2-element Vector{Int64}:\n 1\n 2\n\n\nAll elements have the type:\n\nA = [1, 1.4]\n\n2-element Vector{Float64}:\n 1.0\n 1.4\n\n\n\ntypeof(A) == Vector{Int64}\n\nfalse\n\n\n\nA''\n\n2-element Vector{Float64}:\n 1.0\n 1.4\n\n\nTo get the size of an array:\n\nsize(A)\n\n(2,)\n\n\nArrays are mutable\n\nA[1] = 10\n\n10\n\n\n\nA\n\n2-element Vector{Float64}:\n 10.0\n  1.4\n\n\nJulia has one-based indexing: you refer to the first element as 1 (\\(\\neq\\) zero-based indexing in C or Python)\n\nA[2]\n\n1.4\n\n\nArrays are mutable and their size can be changed too:\n\npush!(A, 29)\nA\n\n6-element Vector{Float64}:\n 10.0\n  1.4\n 29.0\n 29.0\n 29.0\n 29.0\n\n\n\nA\n\n3-element Array{Float64,1}:\n 10.0\n  1.4\n 29.0\n\n\nTwo comments: - the push! operation is fast - ! is a julia convention to express the fact that push! mutates its first argument\n\n\n\n\nsize(A)  # is a tuple\n\n(6,)\n\n\n\n# you can create tuples with (,,,)\nt = (1,2,3,4)\n\n(1, 2, 3, 4)\n\n\n\nt\n\n(1, 2, 3, 4)\n\n\ntuples differ from arrays in two ways: - they are immutable - they can contain non-homogenous objects\n\nt[1]\n\n1\n\n\n\nt[1] = 2\n\nMethodError: MethodError: no method matching setindex!(::NTuple{4, Int64}, ::Int64, ::Int64)\n\n\n\ntypeof((1, \"1\", [1]))\n\nTuple{Int64, String, Vector{Int64}}\n\n\n2d arrays are also called matrices… and can be used for matrix multiplications.\n\n[3 4; 5 6]\n\n2×2 Matrix{Int64}:\n 3  4\n 5  6\n\n\n\na1 = [1,2,3,4]\na2 = [1,2,3,4]  .+ 4\n[a1 ;; a2]\n# cat(a1, a2; dims=2)\n\n4×2 Matrix{Int64}:\n 1  5\n 2  6\n 3  7\n 4  8\n\n\n\nb = [1 0.6 0]\n\n1×3 Array{Float64,2}:\n 1.0  0.6  0.0\n\n\n\nB = [0.1 0.2 0.3; 4 5 6]\n\nOther ways to construct arrays:\n\n# zero array\nt = zeros(2,3)\nt[1,2] = 23.2\nt\n\n2×3 Matrix{Float64}:\n 0.0  23.2  0.0\n 0.0   0.0  0.0\n\n\n\n# random array (uniform distribution)\nt= rand(3,3)\nt\n\n3×3 Matrix{Float64}:\n 0.151296  0.390327  0.239194\n 0.726286  0.371063  0.133779\n 0.037311  0.183624  0.72499\n\n\n\n# random array (normal distribution)\nt= randn(3,3)\nt\n\n3×3 Array{Float64,2}:\n -0.149832     0.973627  -0.407871\n -0.00251947  -1.46936   -0.141511\n  0.676479    -0.774655   0.349923\n\n\nVectorized operations take a ., even comparisons (pointwise operations)\n\nB = [1 2;3 4]\n\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\n\n\nB*B\n\n2×2 Matrix{Int64}:\n  7  10\n 15  22\n\n\n\nB .* B\n\n2×2 Matrix{Int64}:\n 1   4\n 9  16\n\n\n\nf(x) = x^2+1\n\nf (generic function with 1 method)\n\n\n\nf(43)\n\n1850\n\n\n\nf.(B)\n\n2×2 Matrix{Int64}:\n  2   5\n 10  17\n\n\nElements are always accessed with square brackets:\n\nB = [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\n\nYou get element $B_{ij}$ with `B[i,j]`\n\n\nB[1,2]\n\n2\n\n\nYou select a whole row/column with :\n\nB\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\n\nB[:,1]\n\n2-element Vector{Int64}:\n 1\n 4\n\n\n\nB[1,:]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\nB[:,1:2]\n\n2×2 Matrix{Int64}:\n 1  2\n 4  5\n\n\n\nB[:,1:end-1]\n\n2×2 Matrix{Int64}:\n 1  2\n 4  5\n\n\n\n\n\nConditions\n\nx = 0\nif x<0\n    # block\n    println(\"x is negative\")\nelseif (x > 0) # optional and unlimited\n    println(\"x is positive\")\nelse         # optional\n    println(\"x is zero\")\nend\n\nx is zero\n\n\nWhile\n\ni = 3\nwhile i > 0\n    println(i)\n    i -= 1 # decrement\nend\n\n3\n2\n1\n\n\nFor loops: your iterate over any iterable object: - range i1:i2 - vector - tuple\n\n# Iterate through ranges of numbers\nfor i ∈ (1:3)\n    println(i)\nend\n\n1\n2\n3\n\n\n\n# Iterate through arrays\ncities = [\"Boston\", \"New York\", \"Philadelphia\"]\nfor city in cities\n    println(city)\nend\n\nBoston\nNew York\nPhiladelphia\n\n\n\ncities\n\n3-element Vector{String}:\n \"Boston\"\n \"New York\"\n \"Philadelphia\"\n\n\n\nstates = [\"Massachussets\", \"New York\", \"Pennsylvania\"]\n\n3-element Vector{String}:\n \"Massachussets\"\n \"New York\"\n \"Pennsylvania\"\n\n\n\ntwo_by_two_iterable = zip(cities, states)\n\nzip([\"Boston\", \"New York\", \"Philadelphia\"], [\"Massachussets\", \"New York\", \"Pennsylvania\"])\n\n\n\ncollect(two_by_two_iterable)\n\n3-element Vector{Tuple{String, String}}:\n (\"Boston\", \"Massachussets\")\n (\"New York\", \"New York\")\n (\"Philadelphia\", \"Pennsylvania\")\n\n\n\n[two_by_two_iterable...]\n\n3-element Vector{Tuple{String, String}}:\n (\"Boston\", \"Massachussets\")\n (\"New York\", \"New York\")\n (\"Philadelphia\", \"Pennsylvania\")\n\n\n\n# Iterate through arrays of tuples using zip\nfor kw in zip(cities, states)\n    println(kw)\nend\n\n(\"Boston\", \"Massachussets\")\n(\"New York\", \"New York\")\n(\"Philadelphia\", \"Pennsylvania\")\n\n\n\n# Iterate through arrays of tuples using zip\nfor (city, state) in zip(cities, states)\n    println(\"$city, $state\")\nend\n\nBoston, Massachussets\nNew York, New York\nPhiladelphia, Pennsylvania\n\n\n\n# Iterate through arrays and their indices using enumerate\nfor (i, city) in enumerate(cities)\n    println(\"City $i is $city\")\nend\n\nCity 1 is Boston\nCity 2 is New York\nCity 3 is Philadelphia\n\n\n\nt = (;a = 2, b = 3)\nt[1]\nt.a\n\n2\n\n\n\n\n\n\n[1:10. ...] # unpack operator\n\n10-element Vector{Float64}:\n  1.0\n  2.0\n  3.0\n  4.0\n  5.0\n  6.0\n  7.0\n  8.0\n  9.0\n 10.0\n\n\n\n[i^2 for i  in 1:10] # collect with comprehension syntax\n\n10-element Vector{Int64}:\n   1\n   4\n   9\n  16\n  25\n  36\n  49\n  64\n  81\n 100\n\n\n\n[i^2 for i=1:100000000 if mod(i,2)==0] ;\n\n\ngen = (i^2 for i=1:100000000 if mod(i,2)==0)\n\nBase.Generator{Base.Iterators.Filter{var\"#42#44\", UnitRange{Int64}}, var\"#41#43\"}(var\"#41#43\"(), Base.Iterators.Filter{var\"#42#44\", UnitRange{Int64}}(var\"#42#44\"(), 1:100000000))\n\n\n\nsum(gen)\n\n338960700901149440\n\n\n\n\n\n\n\n\nA composite type is a collection of named fields that can be treated as a single value. They bear a passing resemblance to MATLAB structs.\nAll fields must be declared ahead of time. The double colon, ::, constrains a field to contain values of a certain type. This is optional for any field.\n\n# Type definition with 4 fields\nstruct ParameterFree\n    value  \n    transformation  \n    tex_label\n    description \nend\n\n\npf = ParameterFree(\"1\", x->x^2, \"\\\\sqrt{1+x^2}\", (\"a\",1))\n\nParameterFree(\"1\", var\"#9#10\"(), \"\\\\sqrt{1+x^2}\", (\"a\", 1))\n\n\n\npf.value\n\n\"1\"\n\n\nTwo reasons to create structures: - syntactic shortcut (you access the fields with .) - specify the types of the fields\n\n# Type definition\nstruct Parameter\n    value ::Float64\n    transformation ::Function # Function is a type!\n    tex_label::String\n    description::String\nend\n\n\np = Parameter(\"1\", x->x^2, \"\\\\sqrt{1+x^2}\", (\"a\",1))\n\nMethodError: MethodError: Cannot `convert` an object of type String to an object of type Float64\nClosest candidates are:\n  convert(::Type{T}, !Matched::T) where T<:Number at number.jl:6\n  convert(::Type{T}, !Matched::Number) where T<:Number at number.jl:7\n  convert(::Type{T}, !Matched::Base.TwicePrecision) where T<:Number at twiceprecision.jl:273\n  ...\n\n\n\np = Parameter(0.43, x->x^2, \"\\\\sqrt{1+x^2}\", \"This is a description\")\n\nParameter(0.43, var\"#13#14\"(), \"\\\\sqrt{1+x^2}\", \"This is a description\")\n\n\n\np.value\n\n0.43\n\n\nWhen a type with \\(n\\) fields is defined, a constructor (function that creates an instance of that type) that takes \\(n\\) ordered arguments is automatically created. Additional constructors can be defined for convenience.\n\n# Creating an instance of the Parameter type using the default\n# constructor\nβ = Parameter(0.9, identity, \"\\\\beta\", \"Discount rate\")\n\nParameter(0.9, identity, \"\\\\beta\", \"Discount rate\")\n\n\n\nfunction Parameter(value)\n    return Parameter(value, x->x, \"x\", \"Anonymous\")\nend\n\nParameter\n\n\n\nParameter(0.4)\n\nParameter(0.4, var\"#15#16\"(), \"x\", \"Anonymous\")\n\n\n\nParameter(value, transformation, tex) = Parameter(value, transformation, tex, \"no description\")\n\nParameter\n\n\n\nmethods( Parameter )\n\n# 4 methods for type constructor: Parameter(value::Float64, transformation::Function, tex_label::String, description::String) in Main at /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:3  Parameter(value) in Main at /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:1  Parameter(value, transformation, tex) in Main at /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:1  Parameter(value, transformation, tex_label, description) in Main at /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:3 \n\n\n\n# Alternative constructors end with an appeal to the default\n# constructor\nfunction Parameter(value::Float64, tex_label::String)\n    transformation = identity\n    description = \"No description available\"\n    return Parameter(value, transformation, tex_label, description)\nend\n\nα = Parameter(0.5, \"\\alpha\")\n\nParameter(0.5, identity, \"\\alpha\", \"No description available\")\n\n\nNow the function Parameter has two different methods with different signatures:\n\nmethods(Parameter)\n\n# 4 methods for type constructor: Parameter(value::Float64, transformation::Function, tex_label::String, description::String) in Main at In[1]:3  Parameter(value::Float64, tex_label::String) in Main at In[8]:4  Parameter(value, transformation, tex) in Main at In[5]:1  Parameter(value, transformation, tex_label, description) in Main at In[1]:3 \n\n\nWe have seen that a function can have several implementations, called methods, for different number of arguments, or for different types of arguments.\n\nfun(x::Int64, y::Int64) = x^3 + y\n\nfun (generic function with 1 method)\n\n\n\nfun(x::Float64, y::Int64) = x/2 + y\n\nfun (generic function with 2 methods)\n\n\n\nfun(2, 2)\n\n10\n\n\n\nfun(2.0, 2)\n\n3.0\n\n\n\nα.tex_label\n\n\"\\alpha\"\n\n\n\n# Access a particular field using .\nα.value\n\n0.5\n\n\n\n# Fields are modifiable and can be assigned to, like \n# ordinary variables\nα.value = 0.75\n\nErrorException: setfield!: immutable struct of type Parameter cannot be changed\n\n\n\n\n\n\nby default structures in Julia are non-mutable\n\np.value = 3.0\n\nErrorException: setfield!: immutable struct of type Parameter cannot be changed\n\n\n\nmutable struct Params\n    x:: Float64\n    y:: Float64\nend\n\n\npos = Params(0.4, 0.2)\n\nParams(0.4, 0.2)\n\n\n\npos.x = \"pos\"\n\nMethodError: MethodError: Cannot `convert` an object of type String to an object of type Float64\nClosest candidates are:\n  convert(::Type{T}, !Matched::T) where T<:Number at number.jl:6\n  convert(::Type{T}, !Matched::Number) where T<:Number at number.jl:7\n  convert(::Type{T}, !Matched::Base.TwicePrecision) where T<:Number at twiceprecision.jl:273\n  ...\n\n\n\n\n\nParameterized types are data types that are defined to handle values identically regardless of the type of those values.\nArrays are a familiar example. An Array{T,1} is a one-dimensional array filled with objects of any type T (e.g. Float64, String).\n\ntypeof( [1,2,3])\n\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\n# Defining a parametric point\nstruct Duple{T} # T is a parameter to the type Duple\n    x::T\n    y::T\nend\n\n\nDuple(3, 3)\n\nDuple{Int64}(3, 3)\n\n\n\nDuple(1//2, 2//3)\n\nDuple{Rational{Int64}}(1//2, 2//3)\n\n\n\nDuple(3, -1.0)\n\nMethodError: MethodError: no method matching Duple(::Int64, ::Float64)\nClosest candidates are:\n  Duple(::T, !Matched::T) where T at /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:3\n\n\n\nstruct Truple{T}\n    x::Duple{T}\n    z::T\nend\n\nThis single declaration defines an unlimited number of new types: Duple{String}, Duple{Float64}, etc. are all immediately usable.\n\nsizeof(3.0)\n\n8\n\n\n\nsizeof( Truple(Duple(3.0, -15.0), 3.0) )\n\n24\n\n\n\n# What happens here?\nDuple(1.5, 3)\n\n\nstruct Truple3{T,S}\n    x::Tuple{T,S}\n    z::S\nend\n\nWe can also restrict the type parameter T:\n\ntypeof(\"S\") <: Number\n\nfalse\n\n\n\ntypeof(4) <: Number\n\ntrue\n\n\n\n# T can be any subtype of Number, but nothing else\nstruct PlanarCoordinate{T<:Number}\n    x::T\n    y::T\nend\n\n\nPlanarCoordinate(\"4th Ave\", \"14th St\")\n\nMethodError: MethodError: no method matching PlanarCoordinate(::String, ::String)\n\n\n\nPlanarCoordinate(2//3, 8//9)\n\nPlanarCoordinate{Rational{Int64}}(2//3, 8//9)\n\n\nArrays are an exemple of mutable, parameterized types\n\nx = convert(Int8, 4)\n\n4\n\n\n\nfactorial(BigInt(123))\n\n12146304367025329675766243241881295855454217088483382315328918161829235892362167668831156960612640202170735835221294047782591091570411651472186029519906261646730733907419814952960000000000000000000000000000\n\n\n\n\n\nYou can write all your code without thinking about types at all. If you do this, however, you’ll be missing out on some of the biggest benefits of using Julia.\nIf you understand types, you can:\n\nWrite faster code\nWrite expressive, clear, and well-structured programs (keep this in mind when we talk about functions)\nReason more clearly about how your code works\n\nEven if you only use built-in functions and types, your code still takes advantage of Julia’s type system. That’s why it’s important to understand what types are and how to use them.\n\n# Example: writing type-stable functions\nfunction sumofsins_unstable(n::Integer)  \n    sum = 0\n    for i in 1:n  \n        sum += sin(3.4)  \n    end  \n    return sum \nend  \n\nfunction sumofsins_stable(n::Integer)  \n    sum = 0.0\n    for i in 1:n  \n        sum += sin(3.4)  \n    end  \n    return sum \nend\n\n# Compile and run\nsumofsins_unstable(Int(1e5))\nsumofsins_stable(Int(1e5))\n\n-25554.110202663698\n\n\n\n@time sumofsins_unstable(Int(1e5))\n\n  0.000237 seconds\n\n\n-25554.110202663698\n\n\n\n@time sumofsins_stable(Int(1e5))\n\n  0.000119 seconds\n\n\n-25554.110202663698\n\n\nIn sumofsins_stable, the compiler is guaranteed that sum is of type Float64 throughout; therefore, it saves time and memory. On the other hand, in sumofsins_unstable, the compiler must check the type of sum at each iteration of the loop. Let’s look at the LLVM intermediate representation.\n\n@code_warntype sumofsins_stable(Int(1e5))\n\nMethodInstance for sumofsins_stable\n\n\n(::Int64)\n  from sumofsins_stable(n::Integer) in Main at /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:10\nArguments\n  #self#::Core.Const(sumofsins_stable)\n  n::Int64\nLocals\n  @_3::Union{Nothing, Tuple{Int64, Int64}}\n  sum::Float64\n  i::Int64\nBody::Float64\n\n\n1 ─       \n\n\n(sum = 0.0)\n\n\n│   %2  = (1:n)\n\n\n::Core.PartialStruct(UnitRange{Int64}, Any[Core.Const(1), Int64])\n│         (@_3 = Base.iterate(%2))\n│   %4  = (@_3 === nothing)::Bool\n│   %5  = Base.not_int(%4)::Bool\n└──       goto #4 if not %5\n2 ┄ %7  = @_3::Tuple{Int64, Int64}\n│         (i = Core.getfield(%7, 1))\n│   %9  = Core.getfield(%7, 2)::Int64\n│   %10 = sum::Float64\n│   %11 = Main.sin(3.4)::Core.Const(-0.2555411020268312)\n│         (sum = %10 + %11)\n│         (@_3 = Base.iterate(%2, %9))\n│   %14 = (@_3 === nothing)::Bool\n│   %15 = Base.not_int(%14)::Bool\n└──       goto #4 if not %15\n3 ─       goto #2\n4 ┄       return sum\n\n\n\n\n@code_warntype sumofsins_unstable(Int(1e5))\n\nMethodInstance for sumofsins_unstable(::Int64)\n  from sumofsins_unstable(n::Integer) in Main at /workspaces/mie37/notebooks/1_Julia_Basics.ipynb:2\nArguments\n  #self#::Core.Const(sumofsins_unstable)\n  n::Int64\nLocals\n  @_3::Union{Nothing, Tuple{Int64, Int64}}\n  sum::Union{Float64, Int64}\n  i::Int64\nBody::Union{Float64, Int64}\n1 ─       (sum = 0)\n│   %2  = (1:n)::Core.PartialStruct(UnitRange{Int64}, Any[Core.Const(1), Int64])\n│         (@_3 = Base.iterate(%2))\n│   %4  = (@_3 === nothing)::Bool\n│   %5  = Base.not_int(%4)::Bool\n└──       goto #4 if not %5\n2 ┄ %7  = @_3::Tuple{Int64, Int64}\n│         (i = Core.getfield(%7, 1))\n│   %9  = Core.getfield(%7, 2)::Int64\n│   %10 = sum::Union{Float64, Int64}\n│   %11 = Main.sin(3.4)::Core.Const(-0.2555411020268312)\n│         (sum = %10 + %11)\n│         (@_3 = Base.iterate(%2, %9))\n│   %14 = (@_3 === nothing)::Bool\n│   %15 = Base.not_int(%14)::Bool\n└──       goto #4 if not %15\n3 ─       goto #2\n4 ┄       return sum\n\n\n\n\n\n\nSo far we have defined functions over argument lists of any type. Methods allow us to define functions “piecewise”. For any set of input arguments, we can define a method, a definition of one possible behavior for a function.\n\n# Define one method of the function print_type\nfunction print_type(x::Number)\n    println(\"$x is a number\")\nend\n\nprint_type (generic function with 1 method)\n\n\n\n# Define another method\nfunction print_type(x::String)\n    println(\"$x is a string\")\nend\n\nprint_type (generic function with 2 methods)\n\n\n\n# Define yet another method\nfunction print_type(x::Number, y::Number)\n    println(\"$x and $y are both numbers\")\nend\n\nprint_type (generic function with 3 methods)\n\n\n\n# See all methods for a given function\nmethods(print_type)\n\n# 3 methods for generic function print_type: print_type(x::String) in Main at In[53]:3  print_type(x::Number) in Main at In[51]:3  print_type(x::Number, y::Number) in Main at In[54]:3 \n\n\nJulia uses multiple dispatch to decide which method of a function to execute when a function is applied. In particular, Julia compares the types of all arguments to the signatures of the function’s methods in order to choose the applicable one, not just the first (hence “multiple”).\n\nprint_type(5)\n\n5 is a number\n\n\n\nprint_type(\"foo\")\n\nfoo is a string\n\n\n\nprint_type([1, 2, 3])\n\nMethodError: MethodError: no method matching print_type(::Array{Int64,1})\nClosest candidates are:\n  print_type(!Matched::String) at In[53]:3\n  print_type(!Matched::Number) at In[51]:3\n  print_type(!Matched::Number, !Matched::Number) at In[54]:3\n\n\n\n\nJulia supports a short function definition for one-liners\n\nf(x::Float64) = x^2.0\nf(x::Int64) = x^3\n\nf (generic function with 2 methods)\n\n\nAs well as a special syntax for anonymous functions\n\nu -> u^2\n\n#17 (generic function with 1 method)\n\n\n\nmap(u->u^2, [1,2,3,4])\n\n4-element Vector{Int64}:\n  1\n  4\n  9\n 16\n\n\n\nfunction fun(arg1 ; add=0.0)\n    x = arg1 + add\n    return x +add\nend\n\nfun (generic function with 3 methods)\n\n\n\n\n\n\n\nfun(1; add=12)\n\n25\n\n\n\n\n\n\nt = (1,2,4)\n\n(1, 2, 4)\n\n\n\na,b,c = t\n\n(1, 2, 4)\n\n\n\n[(1:10)...]\n\n10-element Vector{Int64}:\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n\n\n\ncat([4,3], [0,1]; dims=1)\n\n4-element Array{Int64,1}:\n 4\n 3\n 0\n 1\n\n\n\nl = [[4,3], [0,1], [0, 0], [1, 1]]\n# how do I concatenate it ?\n\ncat(l...; dims=1) ### see python's f(*s)\n\n8-element Array{Int64,1}:\n 4\n 3\n 0\n 1\n 0\n 0\n 1\n 1\n\n\n\n\n\nAs we’ve seen, you can use Julia just like you use MATLAB and get faster code. However, to write faster and better code, attempt to write in a “Julian” manner:\n\nDefine composite types as logically needed\nWrite type-stable functions for best performance\nTake advantage of multiple dispatch to write code that looks like math\nAdd methods to existing functions\n\n\n\n\nHow is Julia so fast? Julia is just-in-time (JIT) compiled, which means (according to this StackExchange answer):\n\nA JIT compiler runs after the program has started and compiles the code (usually bytecode or some kind of VM instructions) on the fly (or just-in-time, as it’s called) into a form that’s usually faster, typically the host CPU’s native instruction set. A JIT has access to dynamic runtime information whereas a standard compiler doesn’t and can make better optimizations like inlining functions that are used frequently.\n\n\nThis is in contrast to a traditional compiler that compiles all the code to machine language before the program is first run.\n\nIn particular, Julia uses type information at runtime to optimize how your code is compiled. This is why writing type-stable code makes such a difference in speed!\n\n\n\nTaken from QuantEcon’s Julia Essentials and Vectors, Arrays, and Matrices lectures.\n\nConsider the polynomial \\[p(x) = \\sum_{i=0}^n a_0 x^0\\] Using enumerate, write a function p such that p(x, coeff) computes the value of the polynomial with coefficients coeff evaluated at x.\n\n\n\n\nppp (generic function with 1 method)\n\n\n\nWrite a function solve_discrete_lyapunov that solves the discrete Lyapunov equation \\[S = ASA' + \\Sigma \\Sigma'\\] using the iterative procedure \\[S_0 = \\Sigma \\Sigma'\\] \\[S_{t+1} = A S_t A' + \\Sigma \\Sigma'\\] taking in as arguments the \\(n \\times n\\) matrix \\(A\\), the \\(n \\times k\\) matrix \\(\\Sigma\\), and a number of iterations."
  },
  {
    "objectID": "pushups/1_epidemiology.html",
    "href": "pushups/1_epidemiology.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "H.W. Heathcote: epidemiologic models are deterministic models for infectious diseases which are spread by direct person-to-person contact in a population.\nThis kind of models has been used since by a few decades by economist, for matters that have nothing to do with health.\n\ndiffusion of information, or opinions on social medias\nasset prices and fads\nexpectation formation for macroeconomic outcomes (ex: The Epidemiology of Macroeconomic Expectations by Chris Carroll)\n\nEpidemiologic models have two features that depart from standard economic models:\n\nAgent’s behaviour does not take the full system into account, an is irrational in the sense that it isn’t forward looking. Instead, an agent is given a behavioural heuristic.\nThe transitions of the whole system can be determined without solving for complex interactions first.\n\nUnderstanding why these two assumptions are very costly for economists will keep us busy during a big part of the course. Here we just consider two simple models as small programming exercises.\n\n\nThere is a continuum of agents of mass \\(1\\). Each agent can be either “Susceptible” (S), “Infected” (I) or “Recovered” (R). In each period, one agent meets another agent drawn randomly. During a meeting Susceptible agents who meet an infected agent, will always catch the disease (or the fad) but are not contagious. Infected agents, have a probability \\(\\pi\\) of being recovered. Nothing happens to Recovered agents who meet other people. No distinction is made between recovering as a healthy or a dead person.\nWe’re interested in the evolution in the number infected persons, both the speed of infection and the total amount of infected people in every period.\nWrite down the transition equations for \\(n_I\\), the number of infected people, for \\(n_R\\) the number of recovered people and \\(n_S\\) the number of susceptible people.\nCompute the transition function f for the vector state \\(s_t\\) returning \\(s_{t+1}\\).\nCompute the transitions over \\(T\\) periods. Plot the result using Plots.jl. (bonus: check against closed form solution)\nWe now assume a Susceptible person who meets an infected one has a probability \\(\\mu\\) of catching the disease. Update the transition function. Update function \\(f\\) and write a function of \\(\\mu\\), \\(\\pi\\) which returns the simulation. Compare different values of the parameters. How would you interpret “social distancing”? How would you choose parameters \\(\\Pi\\) and \\(\\mu\\)\n\n\n\nWe now consider another version of the model where agents evolve in the space \\(\\mathcal{S}=[0,1]\\times[0,1]\\). There are \\(N\\in\\mathbb{N}\\) agent. At any date, each agent \\(n \\in [0,1]\\) is located at \\((x_n,y_n)\\in \\mathcal{S}\\).\nEach agent moves follows a random walk bounded by \\(\\mathcal{S}\\): \\[x_t = \\min(\\max( x_{t-1} + \\epsilon_t, 0), 1)\\] \\[y_t = \\min(\\max( y_{t-1} + \\eta_t, 0), 1)\\] where \\(\\epsilon_t\\) and \\(\\eta_t\\) are both normally distributed with standard deviation \\(\\sigma\\).\nAt any date, the individual state of an agent is \\(s_t=(x_t, y_t, h_t)\\) where \\(h_t\\) is either “S”, “I” or “R”. \\(v_t\\) denotes the states of all agents (for instance \\(v_t=(s_{n,t})_n\\). The health status of each agent is updated in the following way:\n\nAgents \\(R\\) stay \\(R\\).\nAgents \\(I\\) have probability \\(\\pi\\) to become \\(R\\). They stay \\(I\\) otherwise.\nAn agent of type \\(S\\) in position \\((x,y)\\) has a probability \\(prob(x,y,S)\\) to be infected that is \\(\\mu\\) if there is another infected agent within a radius \\(r>0\\).\n\nDefine a type Agent, which holds the type of an agent. The state of the whole system will be held in a Vector[Agent] type.\nWrite a function spatial_transition(S::Vector{Agent})::Vector{Agent} to compute the transition of the positions. Write another function random_guess(T=100) which simulates for \\(T\\) periods in order to find a good initial guess.\nWrite a function show_population to plot all agents with different colors for different health status.\nWrite a function evolve(S::Vector[Agent])::Vector[Agent] which takes the population in a given state and returns the same population with updated health status.\nWrite a function simulate(S0::Vector[Agent], k=1) to simulate the economy starting from an initially random position with k infected individuals. The returned object should be of type Vector[Vector[Agent]].\nCompute statistics along a simulated path for \\(n_I\\), \\(n_S\\), \\(n_R\\). Plot and compare with the basic SIR model\n\n\n\nHave fun by trying to answer one of these questions: - change probability of infection so that it depends on the number of infected people in the vincinity (with some suitable functional formulation for) - compute an animation of the transition - compute an interactive visualisation (with Interact.jl if available)"
  },
  {
    "objectID": "pushups/1_epidemiology_correction.html",
    "href": "pushups/1_epidemiology_correction.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "H.W. Heathcote: epidemiologic models are deterministic models for infectious diseases which are spread by direct person-to-person contact in a population.\nThis kind of models has been used since by a few decades by economist, for matters that have nothing to do with health.\n\ndiffusion of information, or opinions on social medias\nasset prices and fads\nexpectation formation for macroeconomic outcomes (ex: The Epidemiology of Macroeconomic Expectations by Chris Carroll)\n\nEpidemiologic models have two features that depart from standard economic models:\n\nAgent’s behaviour does not take the full system into account, an is irrational in the sense that it isn’t forwards looking. Instead, an agent is given a behavioural heuristic.\nThe transitions of the whole system can be determined without solving for complex interactions first.\n\nUnderstanding why these two assumptions are very costly for economists will keep us busy during a big part of the course. Here we just consider two simple models as small programming exercises.\n\n\nThere is a continuum of agents of mass \\(1\\). Each agent can be either “Susceptible” (S), “Infected” (I) or “Recovered” (R). In each period, one agent meets another agent drawn randomly. During a meeting Susceptible agents who meet an infected agent, will always catch the disease (or the fad) but are not contagious. Infected agents, have a probability \\(\\pi\\) of being recovered. Nothing happens to Recovered agents who meet other people. No distinction is made between recovering as a healthy or a dead person.\nWe’re interested in the evolution in the number infected persons, both the speed of infection and the total amount of infected people in every period.\nWrite down the transition equations for \\(n_I\\), the number of infected people, for \\(n_R\\) the number of recovered people and \\(n_S\\) the number of susceptible people.\nCompute the transition function f for the vector state \\(s_t\\) returning \\(s_{t+1}\\).\nCompute the transitions over \\(T\\) periods. Plot the result using Plots.jl. (bonus: check against closed form solution)\nWe now assume a Susceptible person who meets an infected one has a probability \\(\\mu\\) of catching the disease. Update the transition function. Update function \\(f\\) and write a function of \\(\\mu\\), \\(\\pi\\) which returns the simulation. Compare different values of the parameters. How would you interpret “social distancing”? How would you choose parameters \\(\\Pi\\) and \\(\\mu\\)\n\n\n\nWe now consider another version of the model where agents evolve in the space \\(\\mathcal{S}=[0,1]\\times[0,1]\\). There are \\(N\\in\\mathbb{N}\\) agent. At any date, each agent \\(n \\in [0,1]\\) is located at \\((x_n,y_n)\\in \\mathcal{S}\\).\nEach agent moves follows a random walk bounded by \\(\\mathcal{S}\\): \\[x_t = \\min(\\max( x_{t-1} + \\epsilon_t, 0), 1)\\] \\[y_t = \\min(\\max( y_{t-1} + \\eta_t, 0), 1)\\] where \\(\\epsilon_t\\) and \\(\\eta_t\\) are both normally distributed with standard deviation \\(\\sigma\\).\nAt any date, the individual state of an agent is \\(s_t=(x_t, y_t, h_t)\\) where \\(h_t\\) is either “S”, “I” or “R”. \\(v_t\\) denotes the states of all agents (for instance \\(v_t=(s_{n,t})_n\\). The health status of each agent is updated in the following way:\n\nAgents \\(R\\) stay \\(R\\).\nAgents \\(I\\) have probability \\(\\pi\\) to become \\(R\\). They stay \\(I\\) otherwise.\nAn agent of type \\(S\\) in position \\((x,y)\\) has a probability \\(prob(x,y,S)\\) to be infected that is \\(\\mu\\) if there is another infected agent within a radius \\(r>0\\).\n\nDefine a type Agent, which holds the type of an agent. The state of the whole system will be held in a Vector[Agent] type.\n\nstruct Status\n    state::Int8\nend\n\nstruct Agent\n    x::Float64\n    y::Float64\n    status::Status\nend\n\nstruct Parameters\n    π::Float64\n    μ::Float64\n    σ::Float64\n    r::Float64\nend\n\n\np0 = Parameters(0.1, 0.1, 0.001, 0.1)\n\nParameters(0.1, 0.1, 0.001, 0.1)\n\n\n\nrand() # uniform between 0 and 1\n\n0.5635454056856684\n\n\n\nconst SUSCEPTIBLE = Status(0)\nconst INFECTED = Status(1)\nconst RECOVERED = Status(2)\n\nStatus(2)\n\n\n\nAgent() = Agent(rand(), rand(), SUSCEPTIBLE)\n\nAgent\n\n\n\nAgent()\n\nAgent(0.9321507308941688, 0.9125899999676816, Status(0))\n\n\n\nN = 100\n\n100\n\n\n\npopulation = [Agent() for i=1:N]\n\n100-element Vector{Agent}:\n Agent(0.628775526315654, 0.7747187339300827, Status(0))\n Agent(0.848203633522232, 0.12697652589887465, Status(0))\n Agent(0.3770730637725206, 0.6518910908952857, Status(0))\n Agent(0.5858942210794518, 0.9315300981592123, Status(0))\n Agent(0.9355437044101396, 0.01257220054159136, Status(0))\n Agent(0.0028969107658058935, 0.7063256479188373, Status(0))\n Agent(0.7093600220340011, 0.38687950925069736, Status(0))\n Agent(0.39101974989637966, 0.7702746583032465, Status(0))\n Agent(0.9780100962658798, 0.7733381341853336, Status(0))\n Agent(0.3050038561296904, 0.3910848311360444, Status(0))\n Agent(0.2852412500502912, 0.15063632807752292, Status(0))\n Agent(0.017203804189477978, 0.5087081675152239, Status(0))\n Agent(0.25535255761686404, 0.17367674782088405, Status(0))\n ⋮\n Agent(0.1941335667477393, 0.2838472092184854, Status(0))\n Agent(0.21633766104898777, 0.1966735094373273, Status(0))\n Agent(0.6831569995645241, 0.8920419336580139, Status(0))\n Agent(0.9622417953536797, 0.39182392462501126, Status(0))\n Agent(0.2739499783123305, 0.7185297915614013, Status(0))\n Agent(0.24694950085367484, 0.45228886139759705, Status(0))\n Agent(0.6297114224499756, 0.4032956358091453, Status(0))\n Agent(0.43542881147602186, 0.6126881629284924, Status(0))\n Agent(0.32287284121939286, 0.8719234425590454, Status(0))\n Agent(0.4564635812057405, 0.7335491904444007, Status(0))\n Agent(0.8603354574919857, 0.7200255521995313, Status(0))\n Agent(0.5362575560436258, 0.04291256858421444, Status(0))\n\n\nWrite a function spatial_transition(S::Vector{Agent})::Vector{Agent} to compute the transition of the positions. Write another function random_guess(T=100) which simulates for \\(T\\) periods in order to find a good initial guess.\n\nfunction move(agent::Agent, p::Parameters)\n    x = agent.x + randn()*p.σ\n    y = agent.y + randn()*p.σ\n    return Agent(x, y, agent.status)\nend\n\nmove (generic function with 1 method)\n\n\n\ns = Agent()\n\nAgent(0.47223716356217693, 0.6635242305673732, Status(0))\n\n\n\nmove(s, p0)\n\nAgent(0.47378596063805917, 0.665414145159432, Status(0))\n\n\n\nfunction spatial_transition(S::Vector{Agent}, p::Parameters)\n    out = [move(agent, p) for agent in S]\n    return out\nend\n\nspatial_transition (generic function with 1 method)\n\n\n\nspatial_transition(population, p0)\n\n100-element Vector{Agent}:\n Agent(0.6291229889106399, 0.774070225270899, Status(0))\n Agent(0.8492560959259334, 0.12722132837173722, Status(0))\n Agent(0.3767834602439938, 0.6531753736840068, Status(0))\n Agent(0.5855802186575476, 0.9314864791265248, Status(0))\n Agent(0.9355728554104757, 0.0147052591465586, Status(0))\n Agent(0.002418270849616774, 0.7063249119204462, Status(0))\n Agent(0.707917249104527, 0.38881840471119206, Status(0))\n Agent(0.39094335960860116, 0.771927538997568, Status(0))\n Agent(0.9777438556540287, 0.7739879703309611, Status(0))\n Agent(0.3059470622744458, 0.39105261241885153, Status(0))\n Agent(0.2846707776844322, 0.1501994827208828, Status(0))\n Agent(0.016439024784622704, 0.5086506588124433, Status(0))\n Agent(0.2544781760434176, 0.17513164699524292, Status(0))\n ⋮\n Agent(0.19468554246935676, 0.28493805370350134, Status(0))\n Agent(0.2157212801784034, 0.19605624712784833, Status(0))\n Agent(0.6841183772626205, 0.8917157204512453, Status(0))\n Agent(0.9619082508405186, 0.39142752163734507, Status(0))\n Agent(0.27378095520752205, 0.7174255427828923, Status(0))\n Agent(0.24756324831567023, 0.4505642735758591, Status(0))\n Agent(0.6288632551262225, 0.40536477381800373, Status(0))\n Agent(0.4356625735283086, 0.614521653730983, Status(0))\n Agent(0.324335078005909, 0.8715459749863679, Status(0))\n Agent(0.4562206598464672, 0.7341017315886411, Status(0))\n Agent(0.8597297360316762, 0.7201195225680559, Status(0))\n Agent(0.5375863325198571, 0.04339672685778972, Status(0))\n\n\n\nfunction random_guess(p0, T=100, N=100)\n    population = [Agent() for n =1:N]\n    for t in 1:T\n        population = spatial_transition(population, p0)\n    end\n    return population\n    \nend\n\nrandom_guess (generic function with 3 methods)\n\n\n\nrandom_guess(p0)\n\n100-element Vector{Agent}:\n Agent(0.021148909567778883, 0.47314918146830615, Status(0))\n Agent(0.5089546321194396, 0.28884513251996013, Status(0))\n Agent(0.10491956415835271, 0.03417556625673566, Status(0))\n Agent(0.9648890196776359, 0.29156910851739715, Status(0))\n Agent(0.9550089218945547, 0.6302006590901906, Status(0))\n Agent(0.3373709215204663, 0.5974486029242638, Status(0))\n Agent(0.039620256025278466, 0.8244282387537727, Status(0))\n Agent(0.766594069080001, 0.16756944526349368, Status(0))\n Agent(0.43998997083123537, 0.38760080495602073, Status(0))\n Agent(0.8197449310709023, 0.4049939138409894, Status(0))\n Agent(0.7023572874176478, 0.9223123325033322, Status(0))\n Agent(0.7838120506296088, 0.003946385150953381, Status(0))\n Agent(0.3499841116914357, 0.4690434636787926, Status(0))\n ⋮\n Agent(0.4706292315132484, 0.9633774556626334, Status(0))\n Agent(0.11601326441296803, 0.21677391220315914, Status(0))\n Agent(0.1360317989573984, 0.3051995851284062, Status(0))\n Agent(0.06404029710874866, 0.12963451119876945, Status(0))\n Agent(0.8819544938117289, 0.34781087309163483, Status(0))\n Agent(0.30686411620952936, 0.32898742746510923, Status(0))\n Agent(0.07350006357718267, 0.2408092420737203, Status(0))\n Agent(0.3703927194187015, 0.408806625745773, Status(0))\n Agent(0.49607150587083376, 0.6952786978677291, Status(0))\n Agent(0.3300798342640545, 0.9699989839486622, Status(0))\n Agent(0.46573276741862035, 0.975095147901568, Status(0))\n Agent(0.6679626351164057, 0.31069373176474274, Status(0))\n\n\nWrite a function show_population to plot all agents with different colors for different health status.\n\nimport Plots: plot, plot!\n\n\nusing Plots # imports all functions that are \"exported\"\n\n\nplot(\n[a.x for a in population],\n[a.y for a in population]; marker=\".\", seriestype=:scatter # should make lines invisible\n)\n\n┌ Warning: Skipped marker arg ..\n└ @ Plots /home/pablo/.julia/packages/Plots/8K4be/src/args.jl:1230\n\n\n\n\n\n\nscatter(\n[a.x for a in population],\n[a.y for a in population]; marker=\".\", seriestype=:scatter # should make lines invisible\n)\n\n┌ Warning: Skipped marker arg ..\n└ @ Plots /home/pablo/.julia/packages/Plots/8K4be/src/args.jl:1230\n\n\n\n\n\n\nimport Plots: plot\n\n\nfunction plot(population::Vector{Agent})\n    pl = scatter(\n        [a.x for a in population],\n        [a.y for a in population]; marker=\".\", seriestype=:scatter # should make lines invisible\n    )\n    return pl\nend\n\nplot (generic function with 5 methods)\n\n\n\npop0 = random_guess(p0);\n\n\nplot(pop0)\n\n┌ Warning: Skipped marker arg ..\n└ @ Plots /home/pablo/.julia/packages/Plots/8K4be/src/args.jl:1230\n\n\n\n\n\nWrite a function evolve(S::Vector[Agent])::Vector[Agent] which takes the population in a given state and returns the same population with updated health status.\n\nimport Base: -\n(-)(a1, a2) = Agent( a1.x-a2.x, a1.y - a2.y, a1.status)\n\n- (generic function with 251 methods)\n\n\n\ndistance(a1::Agent, a2::Agent) = sqrt( ((a2-a1).x)^2 + ((a2-a1).y)^2 )\n\ndistance (generic function with 1 method)\n\n\n\ndistance(Agent(), Agent())\n\n0.5090802924591714\n\n\n\nfunction evolve(S::Vector{Agent}, p::Parameters)\n    S = spatial_transition(S, p)\n    new_S = []\n    for (i,agent) in enumerate(S)\n        if agent.status == SUSCEPTIBLE\n            \n            risky_encounter = false\n            for (j,oagent) in enumerate(S)\n                if j!=i\n                    d = distance(agent, oagent)\n                    if (d<p.r) && (oagent.status == INFECTED)\n                        risky_encounter=true\n                        break\n                    end\n                end\n                break\n            end\n            \n            if risky_encounter\n                if rand() < p.μ\n                    new_agent = Agent(agent.x, agent.y, INFECTED)\n                else\n                    new_agent = agent\n                end\n            else\n                new_agent = agent\n            end\n            \n            \n        elseif agent.status == INFECTED\n            if rand() < p.π\n                new_agent = Agent(agent.x, agent.y, RECOVERED)\n            else\n                new_agent = agent\n            end\n        elseif agent.status == RECOVERED\n            new_agent = agent\n        end\n        \n        push!(new_S, new_agent)\n        \n    end\n    \n    return new_S\n    \nend\n\nevolve (generic function with 1 method)\n\n\n\ninfect(agent::Agent) = Agent(agent.x, agent.y, INFECTED)\n\ninfect (generic function with 1 method)\n\n\n\npop0[1] = infect(pop0[1])\n\nAgent(0.441866421899041, 0.8123508004680626, Status(1))\n\n\n\nevolve(pop0, p0)\n\n100-element Vector{Any}:\n Agent(0.4400986434346771, 0.8131979953305294, Status(1))\n Agent(0.78591525472586, 0.938679005088445, Status(0))\n Agent(0.9974960146827153, 0.31911388887754416, Status(0))\n Agent(0.18839463783145038, 0.5914093128283494, Status(0))\n Agent(0.10703795332461867, 0.2415377529237061, Status(0))\n Agent(0.40189765364697577, 0.3942218375197692, Status(0))\n Agent(0.6278090339114756, 0.7000969921375494, Status(0))\n Agent(0.49849882061198286, 1.002173861309859, Status(0))\n Agent(0.4012144028963097, 0.15381332262052724, Status(0))\n Agent(0.5995126013762854, 0.36860720597926183, Status(0))\n Agent(0.7150538251648496, 0.23048985018664622, Status(0))\n Agent(0.8366926540976956, 0.13098169699111353, Status(0))\n Agent(0.13405582978722977, 0.9769724778529784, Status(0))\n ⋮\n Agent(0.8009668473739764, 0.44717266736615346, Status(0))\n Agent(0.39963701899944176, 0.09695743075130865, Status(0))\n Agent(0.730909838624037, 0.8957170083296532, Status(0))\n Agent(0.12202686189475535, 0.7108331647212076, Status(0))\n Agent(0.02556974911988038, 0.6512206926550195, Status(0))\n Agent(0.8277838112049084, 0.3977093786688347, Status(0))\n Agent(0.9230871033326455, 0.5494691005216155, Status(0))\n Agent(0.8487731829698942, 0.17613479053354542, Status(0))\n Agent(0.23996260436414685, 0.7259301056175521, Status(0))\n Agent(0.8111453442558807, 0.16457921836217404, Status(0))\n Agent(0.8538287191395986, 0.5008508352686645, Status(0))\n Agent(0.7317131321638034, 0.1406695400875948, Status(0))\n\n\nWrite a function simulate(S0::Vector[Agent], k=1) to simulate the economy starting from an initially random position with k infected individuals. The returned object should be of type Vector[Vector[Agent]].\n\nfunction simulate(S0::Vector{Agent}, p::Parameters; T=1)\n    sim = [S0]\n    for t=1:T\n        pop = sim[end]\n        push!(sim, evolve(pop, p))\n    end\n    return sim\nend\n\nsimulate (generic function with 1 method)\n\n\n\nsim = simulate(pop0, p0; T=100)\n\n101-element Vector{Vector{Agent}}:\n [Agent(0.441866421899041, 0.8123508004680626, Status(1)), Agent(0.7854071329647904, 0.9372482823692116, Status(0)), Agent(0.9975409897421323, 0.31961377242256905, Status(0)), Agent(0.18872704600426163, 0.5920558147918156, Status(0)), Agent(0.10615016895963983, 0.24037685998821293, Status(0)), Agent(0.401518414766676, 0.395735934342022, Status(0)), Agent(0.6293589919021436, 0.7013413101083454, Status(0)), Agent(0.49880131265821676, 0.9999988537761209, Status(0)), Agent(0.4024638668258409, 0.15323105376578777, Status(0)), Agent(0.5999188515799793, 0.36821832736573823, Status(0))  …  Agent(0.731749120112525, 0.8948896273227398, Status(0)), Agent(0.12294544837412476, 0.7099547508813026, Status(0)), Agent(0.02690848676757432, 0.6522679193401205, Status(0)), Agent(0.8285987229784609, 0.4000528595082623, Status(0)), Agent(0.9247625025601486, 0.5503127753459159, Status(0)), Agent(0.8471722875835832, 0.17657205514505364, Status(0)), Agent(0.24002620565354532, 0.7273299291660996, Status(0)), Agent(0.8136778097822348, 0.16502056409612506, Status(0)), Agent(0.8527105933494985, 0.5001524252059384, Status(0)), Agent(0.732526185487153, 0.14100901661174903, Status(0))]\n [Agent(0.442529836076338, 0.8118853082864981, Status(1)), Agent(0.7862632773602122, 0.9383191277842291, Status(0)), Agent(0.9970697848694335, 0.32075364462559497, Status(0)), Agent(0.18856628119118946, 0.5932929653910708, Status(0)), Agent(0.10424294166135492, 0.2405480949258491, Status(0)), Agent(0.40191842703392794, 0.3946178237221266, Status(0)), Agent(0.6296232133600783, 0.701124301377656, Status(0)), Agent(0.4991187958295939, 0.9985977150520342, Status(0)), Agent(0.40157255198549363, 0.1528055005730649, Status(0)), Agent(0.6009852569159178, 0.36783563378970857, Status(0))  …  Agent(0.7326275002869485, 0.8940202682289238, Status(0)), Agent(0.12403776134999274, 0.709883156910694, Status(0)), Agent(0.0257075122830999, 0.6520990387126823, Status(0)), Agent(0.8281341756445871, 0.4005485456037277, Status(0)), Agent(0.9247203544043578, 0.5495552560441944, Status(0)), Agent(0.8469420132317502, 0.17467741635687342, Status(0)), Agent(0.23950559996022514, 0.7277666656988727, Status(0)), Agent(0.813351683659198, 0.1639834251071749, Status(0)), Agent(0.8529785706179416, 0.5006748192345365, Status(0)), Agent(0.7314903560736649, 0.14103122839908525, Status(0))]\n [Agent(0.44385741799503986, 0.8122509010936316, Status(1)), Agent(0.7864154704487637, 0.9375699248346644, Status(0)), Agent(0.9975168175778097, 0.32158389173502827, Status(0)), Agent(0.18894816490110694, 0.5917154392625062, Status(0)), Agent(0.10573235976811694, 0.24091711336275304, Status(0)), Agent(0.4020041714066411, 0.393511966903223, Status(0)), Agent(0.6294431457358413, 0.7019410824976265, Status(0)), Agent(0.5003163466648354, 0.9970328060068792, Status(0)), Agent(0.4020519419748231, 0.1535949792853815, Status(0)), Agent(0.601680542862499, 0.36667751960898654, Status(0))  …  Agent(0.7306191324904929, 0.8926855562666102, Status(0)), Agent(0.12331798578407342, 0.7106271461238091, Status(0)), Agent(0.027261635351836247, 0.6512648252926417, Status(0)), Agent(0.82806037996398, 0.3998987976430061, Status(0)), Agent(0.9250805300878677, 0.5509498037189139, Status(0)), Agent(0.8459961051899432, 0.1745809946224706, Status(0)), Agent(0.24057262348542838, 0.7258585169402969, Status(0)), Agent(0.8124883720245265, 0.16227753640987697, Status(0)), Agent(0.8545396955425436, 0.5008680309670407, Status(0)), Agent(0.732356294300309, 0.14079358267401554, Status(0))]\n [Agent(0.44143601430313795, 0.8109012527767526, Status(1)), Agent(0.7847855563513468, 0.9385605323662012, Status(0)), Agent(0.9981882560049341, 0.32305079206348347, Status(0)), Agent(0.18873719776669323, 0.590114018781142, Status(0)), Agent(0.10629160358125028, 0.2425945497187616, Status(0)), Agent(0.40172180403359264, 0.3935794841421229, Status(0)), Agent(0.628845310730426, 0.700643704410727, Status(0)), Agent(0.49981506734223097, 0.9962009041338814, Status(0)), Agent(0.40138134937030784, 0.15455146103835202, Status(0)), Agent(0.601273352054296, 0.3668590853865329, Status(0))  …  Agent(0.7300646583061547, 0.8926376302904875, Status(0)), Agent(0.12197247095019595, 0.7104821427216658, Status(0)), Agent(0.02647232988916635, 0.6509050068898048, Status(0)), Agent(0.8278654650237912, 0.39938690609025884, Status(0)), Agent(0.9255844977242286, 0.5508175849731638, Status(0)), Agent(0.8461845964016704, 0.17541016158557582, Status(0)), Agent(0.23888127382389324, 0.727075487892517, Status(0)), Agent(0.8128831033856619, 0.16442473271629224, Status(0)), Agent(0.8555546172805246, 0.4997080582404427, Status(0)), Agent(0.7299968512301199, 0.14090917313800772, Status(0))]\n [Agent(0.44143232550513156, 0.8116322806233718, Status(2)), Agent(0.7850271276115492, 0.9377850100721135, Status(0)), Agent(0.9984818211781348, 0.32289006481733745, Status(0)), Agent(0.19013415022055025, 0.5912656320891427, Status(0)), Agent(0.10721531219567947, 0.2423539301041307, Status(0)), Agent(0.40151743246022675, 0.3931825847156946, Status(0)), Agent(0.6283864914547052, 0.6997669246063346, Status(0)), Agent(0.5010837675562352, 0.995086940362378, Status(0)), Agent(0.40144208995163844, 0.15400358824281257, Status(0)), Agent(0.6031062156289628, 0.3656875957557012, Status(0))  …  Agent(0.7295286074132161, 0.8922281120525392, Status(0)), Agent(0.1213042020261773, 0.7111101463417513, Status(0)), Agent(0.026878219578959736, 0.6507413471658348, Status(0)), Agent(0.8281518299121955, 0.4000593075793267, Status(0)), Agent(0.9247818164770577, 0.5506769864086912, Status(0)), Agent(0.8460116223861153, 0.17404424873403893, Status(0)), Agent(0.2372654976950991, 0.7279115724188011, Status(0)), Agent(0.8138969346333568, 0.1644245830634919, Status(0)), Agent(0.8556271675245165, 0.5009554582182628, Status(0)), Agent(0.730390690755905, 0.13957971306075168, Status(0))]\n [Agent(0.4431575878851423, 0.8126325342166986, Status(2)), Agent(0.787024421028446, 0.937894676998416, Status(0)), Agent(0.998633819389741, 0.32393172361441774, Status(0)), Agent(0.1889873198606994, 0.5902533018665567, Status(0)), Agent(0.10842621690802556, 0.2420351925411216, Status(0)), Agent(0.40315570913514426, 0.3927643929788164, Status(0)), Agent(0.6282892549167484, 0.6989346050599804, Status(0)), Agent(0.5022257890874796, 0.9935755617044859, Status(0)), Agent(0.40253992951857714, 0.15335936323863417, Status(0)), Agent(0.6039868229654618, 0.36392657429600067, Status(0))  …  Agent(0.7293719845769621, 0.8925509079691587, Status(0)), Agent(0.12223710211500795, 0.7109965052006866, Status(0)), Agent(0.027240444842885354, 0.6510527311402889, Status(0)), Agent(0.8274600967789527, 0.40083952898801556, Status(0)), Agent(0.9235683253601106, 0.5508330657406815, Status(0)), Agent(0.84463080264382, 0.17272533425700382, Status(0)), Agent(0.2381432037563708, 0.7301545618178992, Status(0)), Agent(0.8138885513760006, 0.164311088053162, Status(0)), Agent(0.855316352863125, 0.5024662562313975, Status(0)), Agent(0.7304012007037838, 0.13848558658865961, Status(0))]\n [Agent(0.4422261921096162, 0.8125111503727651, Status(2)), Agent(0.7877822534386613, 0.9382138147070861, Status(0)), Agent(0.9983331736115238, 0.3237234031058819, Status(0)), Agent(0.18753811532563303, 0.5912216760270286, Status(0)), Agent(0.1079119536259964, 0.24208228475841426, Status(0)), Agent(0.40364664639051534, 0.39308884363207613, Status(0)), Agent(0.6295272780062282, 0.6992165022677231, Status(0)), Agent(0.5016302453095813, 0.9933584027498878, Status(0)), Agent(0.4025049384064205, 0.15343629751602844, Status(0)), Agent(0.6060131702329192, 0.36277154899495867, Status(0))  …  Agent(0.7280287588689087, 0.8917147049938571, Status(0)), Agent(0.12161514261014436, 0.712077854463025, Status(0)), Agent(0.02879380915352566, 0.6491253465783401, Status(0)), Agent(0.8262782069396624, 0.40166823222798526, Status(0)), Agent(0.9233862860072962, 0.5512033055291579, Status(0)), Agent(0.845045437947382, 0.17119887690611912, Status(0)), Agent(0.2395103752026538, 0.72869332243359, Status(0)), Agent(0.8139922235324106, 0.16375853882422403, Status(0)), Agent(0.8539983798251518, 0.5030021942885732, Status(0)), Agent(0.7305272166752103, 0.13901409362321895, Status(0))]\n [Agent(0.4417178608879097, 0.8122452603507276, Status(2)), Agent(0.7881703536556063, 0.9382540835459329, Status(0)), Agent(0.998007837540053, 0.32353414566619976, Status(0)), Agent(0.18616572069638335, 0.5925128455278946, Status(0)), Agent(0.10718228498237004, 0.2416644655343551, Status(0)), Agent(0.40419187793068956, 0.39321096853700876, Status(0)), Agent(0.6288564305547872, 0.7003087137158607, Status(0)), Agent(0.5027070212916008, 0.993344425203185, Status(0)), Agent(0.40188142590238307, 0.153836254102514, Status(0)), Agent(0.6053187789521222, 0.3631848311530929, Status(0))  …  Agent(0.7261391396505329, 0.8918533564723, Status(0)), Agent(0.12228945130139726, 0.7115205900929901, Status(0)), Agent(0.0292921547492081, 0.6505692786238139, Status(0)), Agent(0.8270726538516735, 0.4020111348943792, Status(0)), Agent(0.9224233867668022, 0.553410193118374, Status(0)), Agent(0.8445823961404897, 0.16961433475502294, Status(0)), Agent(0.23934644665704916, 0.7266488842804865, Status(0)), Agent(0.8162694447091483, 0.16366871791966067, Status(0)), Agent(0.8530398662334816, 0.5025163718673221, Status(0)), Agent(0.7293255618622704, 0.13811327624332112, Status(0))]\n [Agent(0.442485264332925, 0.8111123942890234, Status(2)), Agent(0.7874558412325513, 0.9401673353904162, Status(0)), Agent(0.9988120562101837, 0.32251265447455835, Status(0)), Agent(0.18441538374927377, 0.591485911572609, Status(0)), Agent(0.10756520675802331, 0.24051363282491278, Status(0)), Agent(0.40317703298237684, 0.3931660877843566, Status(0)), Agent(0.630217986028868, 0.7001066765524754, Status(0)), Agent(0.5026988592656141, 0.9950846911833151, Status(0)), Agent(0.402022428053736, 0.15217594558460018, Status(0)), Agent(0.6051994830394399, 0.3631935461936355, Status(0))  …  Agent(0.7262768236837008, 0.8911137644015331, Status(0)), Agent(0.12167654943701815, 0.7137730924033405, Status(0)), Agent(0.02943415980253969, 0.6501892895809172, Status(0)), Agent(0.8286468313990312, 0.40032576153857824, Status(0)), Agent(0.9225584417877741, 0.5552505418520908, Status(0)), Agent(0.846458533251086, 0.16916074926909644, Status(0)), Agent(0.2406175892863526, 0.7256268984737472, Status(0)), Agent(0.8162888246281313, 0.16340082446475673, Status(0)), Agent(0.8528634882845119, 0.5025576427557168, Status(0)), Agent(0.7301035120863177, 0.1373696283282154, Status(0))]\n [Agent(0.44322685908842174, 0.8108472980530821, Status(2)), Agent(0.7882734915769418, 0.9388941791885129, Status(0)), Agent(0.9963884625993057, 0.3206701153441409, Status(0)), Agent(0.1848125883876502, 0.5932006537475214, Status(0)), Agent(0.1068683283736011, 0.23991559086550954, Status(0)), Agent(0.4030005998380687, 0.3932203882297437, Status(0)), Agent(0.6317163301895065, 0.7003276978304297, Status(0)), Agent(0.5044924339898348, 0.9951957550162323, Status(0)), Agent(0.40196466527330404, 0.15237224588858364, Status(0)), Agent(0.6052926153727103, 0.36310717085027383, Status(0))  …  Agent(0.7248453474207927, 0.8909552391374455, Status(0)), Agent(0.1206642712971681, 0.7139955464135125, Status(0)), Agent(0.029360861545914243, 0.6518116541565286, Status(0)), Agent(0.829012049439057, 0.3996619997387848, Status(0)), Agent(0.9223417793654037, 0.5569189821270372, Status(0)), Agent(0.8473160616372989, 0.1669410271747154, Status(0)), Agent(0.23904717619229723, 0.7250900197658998, Status(0)), Agent(0.816371089988887, 0.1606654245298471, Status(0)), Agent(0.8528613606554278, 0.5026025758748991, Status(0)), Agent(0.7309130655758349, 0.13598171940211456, Status(0))]\n [Agent(0.4451064391515084, 0.81110921492663, Status(2)), Agent(0.7877628905388975, 0.9389437434742378, Status(0)), Agent(0.9967985336126756, 0.32186036233297355, Status(0)), Agent(0.18532019893473453, 0.5930075902092301, Status(0)), Agent(0.10696903519344199, 0.24107554000186807, Status(0)), Agent(0.40206437025500846, 0.39385274651765834, Status(0)), Agent(0.6299242686437513, 0.6995089039391037, Status(0)), Agent(0.5049788617892761, 0.9946490902712464, Status(0)), Agent(0.40234234539023755, 0.1530052268633083, Status(0)), Agent(0.6062882633994584, 0.3636313008399237, Status(0))  …  Agent(0.723856920668422, 0.8918804512338937, Status(0)), Agent(0.12310641137713298, 0.7141265013176181, Status(0)), Agent(0.03034362910671491, 0.6506064228644849, Status(0)), Agent(0.8296908315782956, 0.3997833365647314, Status(0)), Agent(0.921137242213987, 0.557378624040555, Status(0)), Agent(0.8476182253924445, 0.16906976194599635, Status(0)), Agent(0.2378018609005793, 0.7245191933111635, Status(0)), Agent(0.8150245475644172, 0.1608597746562974, Status(0)), Agent(0.853199147154455, 0.5015556270048845, Status(0)), Agent(0.731290918801849, 0.13552056291977949, Status(0))]\n [Agent(0.44515779347107465, 0.8120132414551714, Status(2)), Agent(0.7889479823203434, 0.9387350931980668, Status(0)), Agent(0.9981818435167096, 0.3209827249621066, Status(0)), Agent(0.18698665137589807, 0.5931208965110198, Status(0)), Agent(0.10809028228425278, 0.24164393689151897, Status(0)), Agent(0.40297520684765564, 0.39323500005132905, Status(0)), Agent(0.6293893146302708, 0.6982509506512435, Status(0)), Agent(0.5051918659136754, 0.9936347410214128, Status(0)), Agent(0.4018720062673195, 0.15438588966845102, Status(0)), Agent(0.6055420510089545, 0.3646983735142258, Status(0))  …  Agent(0.7237382318170292, 0.891285098087303, Status(0)), Agent(0.1231679174290374, 0.7127340152952455, Status(0)), Agent(0.03143523888851154, 0.6494468071578929, Status(0)), Agent(0.8287898184564868, 0.39960725247710677, Status(0)), Agent(0.921154108216751, 0.5574157689775252, Status(0)), Agent(0.848809609364808, 0.16824775856599353, Status(0)), Agent(0.2359922772997027, 0.7257769873004642, Status(0)), Agent(0.816774355121578, 0.16082216713348343, Status(0)), Agent(0.8531932224160214, 0.5014457015588222, Status(0)), Agent(0.7312592026545321, 0.13774461298470492, Status(0))]\n [Agent(0.4457091246692341, 0.8110066531085949, Status(2)), Agent(0.7892404847730928, 0.9395990683348232, Status(0)), Agent(0.9978674187832937, 0.32209342591882334, Status(0)), Agent(0.18709997865730804, 0.5938639224209723, Status(0)), Agent(0.10867070437577005, 0.24106917004093478, Status(0)), Agent(0.400082969687098, 0.39281326312069875, Status(0)), Agent(0.6294819202135417, 0.6979119121968463, Status(0)), Agent(0.5053092207374443, 0.9927818519543103, Status(0)), Agent(0.40194029469365483, 0.15527100953704143, Status(0)), Agent(0.6048119570698722, 0.36502416690121275, Status(0))  …  Agent(0.7257546287397749, 0.8912297082862555, Status(0)), Agent(0.12433793460290997, 0.712515270777967, Status(0)), Agent(0.030934710887631243, 0.6494409673371144, Status(0)), Agent(0.8311325513550206, 0.3994313562267959, Status(0)), Agent(0.9202509596097429, 0.5577058425374563, Status(0)), Agent(0.848066049346399, 0.1681642281409571, Status(0)), Agent(0.23512526350612847, 0.7259873432137083, Status(0)), Agent(0.8165706839655698, 0.1608190568674662, Status(0)), Agent(0.8515814707672584, 0.5012819322475581, Status(0)), Agent(0.7329191712778952, 0.1365845923206646, Status(0))]\n ⋮\n [Agent(0.4398434290763222, 0.8130731797279486, Status(2)), Agent(0.7852626279612421, 0.9372060272613925, Status(0)), Agent(1.0075928437415744, 0.3251082563375485, Status(0)), Agent(0.19329604818718593, 0.5937758628008794, Status(0)), Agent(0.08999487417869875, 0.2374683894450651, Status(0)), Agent(0.39917969842162326, 0.396195776761437, Status(0)), Agent(0.6284815141315211, 0.6972720882206642, Status(0)), Agent(0.4969647168369334, 0.998169006681133, Status(0)), Agent(0.4005114012238426, 0.1595804380928642, Status(0)), Agent(0.601558154040745, 0.3660629031866788, Status(0))  …  Agent(0.74091461923018, 0.9001020023952853, Status(0)), Agent(0.12594781595205476, 0.7121386428341928, Status(0)), Agent(0.036492853764586865, 0.654725165431608, Status(0)), Agent(0.8170106492900315, 0.4070644354131682, Status(0)), Agent(0.9244161446884629, 0.5656759974898161, Status(0)), Agent(0.8425933391140563, 0.16522740354220533, Status(0)), Agent(0.24053705724972232, 0.7124497408072096, Status(0)), Agent(0.8180081915954618, 0.1484365145316182, Status(0)), Agent(0.8423316114907028, 0.5062379903959389, Status(0)), Agent(0.7340634474187695, 0.1379845705419109, Status(0))]\n [Agent(0.4392156539484673, 0.811763348671184, Status(2)), Agent(0.7849583971212757, 0.9365112153915957, Status(0)), Agent(1.0081852321571763, 0.32606052640840094, Status(0)), Agent(0.19527928461182176, 0.5940446101434919, Status(0)), Agent(0.09161156100672048, 0.23787357166838113, Status(0)), Agent(0.3990964784821896, 0.3951908698994591, Status(0)), Agent(0.6290470048183306, 0.6980261752757873, Status(0)), Agent(0.49650314507789745, 0.9980910893454142, Status(0)), Agent(0.4003618534002786, 0.1595872155278906, Status(0)), Agent(0.6019681611098464, 0.3679913270612052, Status(0))  …  Agent(0.7409986078592539, 0.9000361991808946, Status(0)), Agent(0.1260765156909216, 0.7115979847116818, Status(0)), Agent(0.03746454144468891, 0.655327406329063, Status(0)), Agent(0.8164729417447567, 0.4073737932718268, Status(0)), Agent(0.926087788659693, 0.5665300362887863, Status(0)), Agent(0.8413369772125101, 0.16563034938718046, Status(0)), Agent(0.2393675220553736, 0.7146713233323678, Status(0)), Agent(0.8179269771093182, 0.14823898569338656, Status(0)), Agent(0.8425054241845531, 0.5069164535198505, Status(0)), Agent(0.7337240952649209, 0.13773392754048205, Status(0))]\n [Agent(0.439227338118714, 0.8123352027097855, Status(2)), Agent(0.7834853890829637, 0.9370241313571385, Status(0)), Agent(1.0091620014066498, 0.32545321907626307, Status(0)), Agent(0.19616243600288777, 0.5941888799455464, Status(0)), Agent(0.09122459419168762, 0.2374708568095875, Status(0)), Agent(0.3997182930481254, 0.39673903732686794, Status(0)), Agent(0.6298425266504987, 0.6978881400441301, Status(0)), Agent(0.4970711182095145, 0.9976484659945547, Status(0)), Agent(0.40101165640560266, 0.16114839882237592, Status(0)), Agent(0.6028096717922249, 0.36970919850544565, Status(0))  …  Agent(0.7402965748515359, 0.8999604214630188, Status(0)), Agent(0.12430281015516363, 0.7110062494365031, Status(0)), Agent(0.03671361649541695, 0.6559809040452571, Status(0)), Agent(0.8167510263326209, 0.40722536206598087, Status(0)), Agent(0.9252870641801766, 0.5670254804090208, Status(0)), Agent(0.8414018597100469, 0.16420462723419726, Status(0)), Agent(0.23804362206602314, 0.7144535521154051, Status(0)), Agent(0.8198552234144876, 0.14827968519112983, Status(0)), Agent(0.8420485055719842, 0.506821999145314, Status(0)), Agent(0.7321982703509006, 0.13838319146657221, Status(0))]\n [Agent(0.43810080192148554, 0.8118306753874529, Status(2)), Agent(0.782682036086891, 0.9370538153597171, Status(0)), Agent(1.0087783906630885, 0.32373250399512915, Status(0)), Agent(0.19493990876056627, 0.5946414699409198, Status(0)), Agent(0.09117874552363367, 0.2376985197357713, Status(0)), Agent(0.3998882646272509, 0.3946573322920105, Status(0)), Agent(0.6304344464399961, 0.6967668562387759, Status(0)), Agent(0.49668693505821754, 0.9980507224388557, Status(0)), Agent(0.4004394072314649, 0.16314008526584028, Status(0)), Agent(0.6020251238911148, 0.3688812189790221, Status(0))  …  Agent(0.7395399934882655, 0.9001073588654738, Status(0)), Agent(0.1251388442377291, 0.7092826148525743, Status(0)), Agent(0.037987724519309804, 0.6565674625988727, Status(0)), Agent(0.8154322425502669, 0.40671627740405336, Status(0)), Agent(0.9230882301873304, 0.5654541647359658, Status(0)), Agent(0.8426407293236821, 0.16550426965038645, Status(0)), Agent(0.2386269826042905, 0.7140105225287274, Status(0)), Agent(0.8183408865375883, 0.1479204972650557, Status(0)), Agent(0.8434579570364978, 0.5062368390748042, Status(0)), Agent(0.7335278725590169, 0.13703817330375284, Status(0))]\n [Agent(0.43819409122235226, 0.8132218488329089, Status(2)), Agent(0.7827478233331987, 0.9379879291039458, Status(0)), Agent(1.0085129653015796, 0.3266642088283069, Status(0)), Agent(0.1951502991536997, 0.5946400659376244, Status(0)), Agent(0.09036890376766628, 0.2374420339605776, Status(0)), Agent(0.39921801667529105, 0.3951579329052276, Status(0)), Agent(0.6306273347789411, 0.6978241927311339, Status(0)), Agent(0.4975792133626165, 0.9953226616166782, Status(0)), Agent(0.40005233428351544, 0.1636514861553488, Status(0)), Agent(0.6008542832622338, 0.36921640682868057, Status(0))  …  Agent(0.7388218571058214, 0.9003187404000286, Status(0)), Agent(0.12436544864091162, 0.708726277715301, Status(0)), Agent(0.036356042612690403, 0.656593058966902, Status(0)), Agent(0.8131048216733904, 0.40711156787369723, Status(0)), Agent(0.9221287251507804, 0.5679553883985176, Status(0)), Agent(0.8437055635902875, 0.16639151128720975, Status(0)), Agent(0.23911383433125138, 0.7125869940043238, Status(0)), Agent(0.8167470688661548, 0.14842700902926872, Status(0)), Agent(0.8442827694486111, 0.5054598203740532, Status(0)), Agent(0.7339811160734665, 0.1371917062261695, Status(0))]\n [Agent(0.43846863888812293, 0.8136527857381026, Status(2)), Agent(0.7825248272720242, 0.9384904560838251, Status(0)), Agent(1.0088013169047685, 0.3253201237583487, Status(0)), Agent(0.19572453406305707, 0.5949056892089403, Status(0)), Agent(0.09061470001048597, 0.2372639074679479, Status(0)), Agent(0.4001885476746242, 0.3953655492581143, Status(0)), Agent(0.6294370652764794, 0.6951696977142238, Status(0)), Agent(0.49748870523378935, 0.9955387680681281, Status(0)), Agent(0.4008554323360955, 0.16293660564285237, Status(0)), Agent(0.6003359240263713, 0.3681862617364786, Status(0))  …  Agent(0.7392590113299143, 0.9017741951814178, Status(0)), Agent(0.12540355781253976, 0.7090225226390683, Status(0)), Agent(0.036710222670873725, 0.6572926436609897, Status(0)), Agent(0.8149726955891011, 0.4075292104443696, Status(0)), Agent(0.9222294616385645, 0.5694585737096522, Status(0)), Agent(0.8423531092108231, 0.16677893980419012, Status(0)), Agent(0.24056726720927646, 0.7128936916734243, Status(0)), Agent(0.8179805673008121, 0.14800083947859793, Status(0)), Agent(0.8457468940771942, 0.5032910023227684, Status(0)), Agent(0.7338172958376615, 0.13635238236090624, Status(0))]\n [Agent(0.4384817322665405, 0.8145575587422089, Status(2)), Agent(0.7822186215866562, 0.9374691178008379, Status(0)), Agent(1.0082835702010473, 0.3249337095473684, Status(0)), Agent(0.194816644373963, 0.5955122052502505, Status(0)), Agent(0.08908408480355341, 0.23893408756942616, Status(0)), Agent(0.39993067819066863, 0.39385767315946324, Status(0)), Agent(0.6293293786102713, 0.6941718691253689, Status(0)), Agent(0.49577969062525684, 0.9950679361980109, Status(0)), Agent(0.4023822805349393, 0.16373461374840423, Status(0)), Agent(0.6002030621570295, 0.3673809295190543, Status(0))  …  Agent(0.7401415127349003, 0.9021697751668745, Status(0)), Agent(0.12450442445458329, 0.7106996103549397, Status(0)), Agent(0.037236969107168616, 0.6583308321229506, Status(0)), Agent(0.8166491757819228, 0.4086256063970911, Status(0)), Agent(0.9217921730805787, 0.5706304370987498, Status(0)), Agent(0.8412198673689512, 0.16783721086394254, Status(0)), Agent(0.24060286477689416, 0.7118218640801746, Status(0)), Agent(0.8169282496124602, 0.1479057523833763, Status(0)), Agent(0.8455114870852158, 0.5049416849833813, Status(0)), Agent(0.7336838677720915, 0.13593882973254437, Status(0))]\n [Agent(0.43802882966908646, 0.815397590050508, Status(2)), Agent(0.7829321934732368, 0.9384210573841409, Status(0)), Agent(1.0093839070202968, 0.32375028502454956, Status(0)), Agent(0.19513157857685162, 0.5951919730417315, Status(0)), Agent(0.08611132659533327, 0.2394714922557445, Status(0)), Agent(0.4006316884233861, 0.3972154854983049, Status(0)), Agent(0.6289841730910128, 0.6932328173644657, Status(0)), Agent(0.4960046964537785, 0.9956030264916814, Status(0)), Agent(0.40255633464026275, 0.16380807749677131, Status(0)), Agent(0.6002826001465114, 0.3667129562962687, Status(0))  …  Agent(0.739445731733693, 0.9021834043201518, Status(0)), Agent(0.12528214698214585, 0.7118581896103984, Status(0)), Agent(0.03586069757171176, 0.6593839193822566, Status(0)), Agent(0.8156854583799873, 0.41049570247088324, Status(0)), Agent(0.9223115775645503, 0.5707646208083953, Status(0)), Agent(0.8403425284822064, 0.1680547375439349, Status(0)), Agent(0.24285955962415237, 0.7127887787501292, Status(0)), Agent(0.817029357221291, 0.1490636230023905, Status(0)), Agent(0.845081981452403, 0.5061738680866381, Status(0)), Agent(0.7326186396946935, 0.1364755252877274, Status(0))]\n [Agent(0.43607142953995304, 0.8152098453663569, Status(2)), Agent(0.7835393697708456, 0.9385418094155858, Status(0)), Agent(1.0088535409304513, 0.32510738509370946, Status(0)), Agent(0.19529336378174542, 0.597952129974227, Status(0)), Agent(0.08661580181288077, 0.237620537190702, Status(0)), Agent(0.40009099237070517, 0.39768138845787615, Status(0)), Agent(0.6284863050490712, 0.6926996499511484, Status(0)), Agent(0.4943239965743711, 0.9945316475488202, Status(0)), Agent(0.4025548243817535, 0.16413986562279517, Status(0)), Agent(0.599768144915327, 0.3655297239443983, Status(0))  …  Agent(0.7400494468207819, 0.9029151884920059, Status(0)), Agent(0.1248120423085644, 0.7117951396605311, Status(0)), Agent(0.0347381096445276, 0.6594365998790008, Status(0)), Agent(0.8154668286485132, 0.4100146960943363, Status(0)), Agent(0.9222744887261148, 0.5715111777099923, Status(0)), Agent(0.8400620454298172, 0.1668444654639437, Status(0)), Agent(0.24349150848976048, 0.7132350722428226, Status(0)), Agent(0.8175721221845748, 0.14988269487760322, Status(0)), Agent(0.8452814224431108, 0.5055534856992605, Status(0)), Agent(0.7329802089774513, 0.1376259529897503, Status(0))]\n [Agent(0.4358186431583357, 0.8154740107924721, Status(2)), Agent(0.7831489766743316, 0.9369680534311884, Status(0)), Agent(1.0111249443859158, 0.32659678796455405, Status(0)), Agent(0.19603846269853012, 0.5976963309475658, Status(0)), Agent(0.08792432653307412, 0.23699140613620373, Status(0)), Agent(0.40070770337896017, 0.39656861556265893, Status(0)), Agent(0.628093213306971, 0.6922920457358762, Status(0)), Agent(0.493789583468707, 0.9949546394425134, Status(0)), Agent(0.3996903197018787, 0.16322055992933554, Status(0)), Agent(0.5988565310689405, 0.36466053675105786, Status(0))  …  Agent(0.7400445529433035, 0.9017663663775608, Status(0)), Agent(0.12424554126569615, 0.711265109601701, Status(0)), Agent(0.036283922678269444, 0.6584020514238422, Status(0)), Agent(0.8133283900887575, 0.4111445928256766, Status(0)), Agent(0.9237999878865827, 0.5715144823685466, Status(0)), Agent(0.8398395863474708, 0.16762700698506575, Status(0)), Agent(0.24518336564124465, 0.7131911699751711, Status(0)), Agent(0.8186014184819426, 0.14975827714166834, Status(0)), Agent(0.8454225373606199, 0.5070896885444538, Status(0)), Agent(0.7340498529822086, 0.13763117106985565, Status(0))]\n [Agent(0.4353492041537104, 0.8135623647279063, Status(2)), Agent(0.7837726026738221, 0.9381707366593354, Status(0)), Agent(1.0088997360450638, 0.32557302082853506, Status(0)), Agent(0.1987582325252235, 0.598463372098762, Status(0)), Agent(0.0878359188455921, 0.23669093601482705, Status(0)), Agent(0.4025094525744544, 0.396033858938448, Status(0)), Agent(0.6291848515696487, 0.6911996660420342, Status(0)), Agent(0.49249464975634183, 0.9932322112005854, Status(0)), Agent(0.4004265479616885, 0.16243570725201553, Status(0)), Agent(0.5993547930391107, 0.3644401165944391, Status(0))  …  Agent(0.7405405332513715, 0.9003640764090779, Status(0)), Agent(0.12480475860049448, 0.7102932040597387, Status(0)), Agent(0.03515747429519327, 0.6584542199932989, Status(0)), Agent(0.8116440828703527, 0.41085518301503965, Status(0)), Agent(0.9232570254296937, 0.5714790025230235, Status(0)), Agent(0.8391342788121922, 0.16573327950847294, Status(0)), Agent(0.24468870793190514, 0.7133161778591234, Status(0)), Agent(0.8181505881044948, 0.14909209245083282, Status(0)), Agent(0.8474400943359627, 0.5078384272366958, Status(0)), Agent(0.7338587218426641, 0.1386562969678601, Status(0))]\n [Agent(0.435079370086555, 0.8159315991465731, Status(2)), Agent(0.7830349322617804, 0.9385191019299033, Status(0)), Agent(1.0086096404812195, 0.3238428842699708, Status(0)), Agent(0.19831781763296338, 0.5985380514133444, Status(0)), Agent(0.08834826623219312, 0.23763801486915395, Status(0)), Agent(0.40125819314705297, 0.3958903557205134, Status(0)), Agent(0.6300609340922065, 0.6909358380765523, Status(0)), Agent(0.4951081271877429, 0.992296104423664, Status(0)), Agent(0.4012946520107518, 0.16324041274461554, Status(0)), Agent(0.5997953012758569, 0.36362155790279194, Status(0))  …  Agent(0.7409157888938563, 0.9002552209858768, Status(0)), Agent(0.12618150526469918, 0.70929765059573, Status(0)), Agent(0.034479892248360317, 0.6596951123646058, Status(0)), Agent(0.8120455114782488, 0.4110310650811127, Status(0)), Agent(0.9245210445446359, 0.5692097656835027, Status(0)), Agent(0.8379442929278645, 0.16514487556268984, Status(0)), Agent(0.24452878931707084, 0.7133418220416281, Status(0)), Agent(0.8189504007760263, 0.1492064899911878, Status(0)), Agent(0.848256020130256, 0.5075775541398552, Status(0)), Agent(0.7345688404042433, 0.13905143597251823, Status(0))]\n\n\nCompute statistics along a simulated path for \\(n_I\\), \\(n_S\\), \\(n_R\\). Plot and compare with the basic SIR model\n\ncount(pop::Vector{Agent}, state) = sum(a.status==state for a in pop)\n\ncount (generic function with 2 methods)\n\n\n\ncount(pop0, SUSCEPTIBLE)\ncount(pop0, SUSCEPTIBLE)\n\n99\n\n\n\np0.\n\n\np0 = Parameters(0.05, 0.5, 0.1, 0.2)\n\n\nParameters(0.05, 0.5, 0.1, 0.2)\n\n\n\nsim = simulate(pop0, p0; T=100);\n\n\nsim_n_S = [count(pop, SUSCEPTIBLE) for pop in sim]\nsim_n_I = [count(pop, INFECTED) for pop in sim]\nsim_n_R = [count(pop, RECOVERED) for pop in sim]\n\n101-element Vector{Int64}:\n  0\n  0\n  2\n  3\n  5\n  5\n  5\n  6\n  6\n  6\n  6\n  6\n  7\n  ⋮\n 27\n 27\n 27\n 27\n 27\n 27\n 27\n 27\n 27\n 27\n 27\n 27\n\n\n\npl = plot(sim_n_S, label=\"Susceptible\")\nplot!(pl, sim_n_I, label=\"Infected\")\nplot!(pl, sim_n_R, label=\"Recovered\")\n\n\n\n\n\n\n\nHave fun by trying to answer one of these questions: - change probability of infection so that it depends on the number of infected people in the vincinity (with some suitable functional formulation for) - compute an animation of the transition - compute an interactive visualisation (with Interact.jl if available)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "Check the website for the course!"
  },
  {
    "objectID": "index.html#indicative-schedule",
    "href": "index.html#indicative-schedule",
    "title": "Advanced Macro: Numerical Methods",
    "section": "Indicative Schedule",
    "text": "Indicative Schedule\n\n\n\nDate\nContent\nHomework\n\n\n\n\n02/02\nIntro, Julia Basics (1)\nEpidemiology Tutorial\n\n\n09/02\nConvergence of Sequences, Julia Basics (2), Solow Tutorial\nFinish Solow Tutorial\n\n\n16/02\nPerturbation (1)\nFinish Neoclassical\n\n\n23/03\nPerturbation (2)\n\n\n\n8/03\nStochastic Models, RBC Model, Dolo Primer\nCoursework I\n\n\n16/03\nDiscrete Dynamic Programming, McCall Model\n\n\n\n23/03\nMcCall Model (2)\n\n\n\n30/03\nInterpolation, Optimization\n\n\n\n06/04\nDiscretization, Time Iteration\nCoursework II"
  },
  {
    "objectID": "index.html#communication",
    "href": "index.html#communication",
    "title": "Advanced Macro: Numerical Methods",
    "section": "Communication",
    "text": "Communication\n\nZulip (best)\nEmail to pwinant@escp.eu starting with [mie37]\nGithub issues (PR also welcome)"
  },
  {
    "objectID": "index.html#assessments",
    "href": "index.html#assessments",
    "title": "Advanced Macro: Numerical Methods",
    "section": "Assessments",
    "text": "Assessments\n\ntutorials (optional) + projects (mandatory) (50%)\nfinal exam (50%)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "slides/interpolation.html",
    "href": "slides/interpolation.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "Two continuous sets \\(X\\in R^p\\), \\(Y \\in R^q\\).\nData set: \\((x_i, y_i)_{i\\in[1,N]} \\in X \\times Y\\)\nTake \\(\\tilde{x} \\in X \\setminus \\{x_i\\}_{i\\in[1,N]}\\). What should be the matching \\(\\tilde{y}\\) ?\nDiscover implicit relation \\(y=f(x)\\) (the model) then compute \\(\\tilde{y}=f(\\tilde{x})\\).\n\\(f\\) is chosen from a family \\(\\mathcal{F}\\) of functions parameterized by a parameter \\(\\theta\\), the approximation family.\n\n\n\n\n\nInterpolation: \\(f\\) is chosen such that \\(\\forall n, y_n=f(x_n)\\) \nRegression: \\(f\\) is chosen so as to minimize a fitness criterium such as\n\n\\(\\min_f \\sum_n \\left( y_n-f(x_n) \\right)^2\\)\nor \\(\\min_{\\theta} \\sum_n \\left( y_n-f(x_n;\\theta) \\right)^2 + \\lambda || \\theta ||^2\\) with \\(\\lambda>0\\)\n\nRemarks:\n\nsome applied mathematicians tend to mix the two (interpolate=evaluate f outside of X)\n\n\n\n\n\n\n1d Graph. Join the dots. Linear/Spline\n2d Graph: Regression\nConclusion: interpolate only if \\(f\\) is known precisely on \\(X\\)\n\n\n\n\n\n\n\\(X\\) and \\(Y\\): large databases of low and high resolutions images\n\\(\\mathcal{F}\\): neural network\n\n\n\n\n\n\nIn economics, we often solve a problem \\(\\Phi(f)=0\\) where \\(f\\) is a function: \\(\\forall s, \\Phi(f)(s) = 0\\)\nIf we approximate \\(f\\) by some element \\(f(;\\theta)\\in\\mathcal{F}\\) we just need to identify a finite set of parameters \\(\\theta \\in R^n\\)\nHow do we identify \\(\\theta\\)?\n\nchoose a finite set of \\(n\\) criteria that must be met\n\n\\(f\\) is pin down uniquely\nexample: colocation, choose \\(s_1, ..., s_n\\). Find \\(f\\) such that \\(\\forall i=1:n, \\Phi(f)(s_i) = 0\\)\n\nchoose higher number of objectives (\\(p>n\\)) that must be minimized:\n\nexample: regression, choose \\(s_1, ..., s_p\\). Find \\(f\\) such that minimize \\(\\sum_i \\Phi(f)(s_i)^2 = 0\\)\n\n\n\n\n\n\n\n\nlocal vs spectral:\n\nlocal: functions in \\(f\\) have compact support\nspectral: noncompact support\n\nlinear vs nonlinear:\n\n\\(\\mathcal{F}\\) is a vector space: \\(f(x) \\approx \\sum_{i=1}^N \\theta_n b_n(x)\\) where \\(b_n\\) is a base of \\(\\mathcal{F}\\)\nnonlinear: wavelets, neural networks, ….\n\n\n\n\n\n\n\n\n\n\n\nTake function \\(f\\) defined on an interval \\([a,b]\\). Suppose the value is known at \\((a=x_1, ... x_N=b)\\). Denote \\(y_i = f(x_i)\\).\nJoin the dots: define a piecewise linear function as \\[\\forall x \\in [x_i, x_{i+1}], \\tilde{f}(x) = y_i + \\underbrace{\\frac{x-x_i}{x_{i+1}-x_i}}\\_{\\text{barycentric coordinate}} (y_{i+1} - y_i)\\]\n\n[TODO: graph]\n\n\n\n\n\nAlternate view: \\[\\tilde{f}(x) = \\sum_{i=1}^N y_i B^i_1(x)\\] where \\(b_1^i(x)=\\frac{x-x_{i-1}}{x_i-x_{i-1}}.1_{x\\in[x_{i-1},x_i]} + (1-\\frac{x-x_{i}}{x_{i+1}-x_{i}}).1_{x\\in [x_i, x_{i+1}]}\\)\n\\((B^i)\\) is an interpolation basis\n\n\n\n\n\n\n\\(n\\)-th order spline : piecewise polynomial function that is \\(n\\) times differentiable except on a finite set of break points (aka knots), where it is \\((n-1)\\) times differentiable.\nin practice the data points are the breakpoints\nexample: order 2\n\nsuppose \\(\\tilde{f}(x_i)\\) and \\(\\tilde{f}^{\\prime}(x_i)\\) are known, choose the coefficients for the patch \\(p_{i+1}(x) = a_{i+1}x^2+b_{i+1}x + c_{i+1}\\)\nAlready two constraints. Condition \\(p_{i+1}(x_{i+1})=\\tilde{f}(x_{i+1})\\) supplies another one.\nDo it for every patch. Not that it requires to set \\(f^{\\prime}(a)\\) beforehand.\n\n\n\n\n\n\n\nDefine \\[B_{i,1}(x) = 1_{x \\in [x_i, x_{i+1}]}\\] \\[B_{i,k+1}(x) = \\frac{x-x_i}{x_{i+k}-x_i}B_{i,k}(x) +\n\\frac{x_{i+k+1}-x}{x_{i+k+1}-x_{i+1}}B_{i+1,k}(x)\\]\nTheorem: Any spline of order \\(k\\) on the knots \\((x_i)\\) can be expressed as a linear combination of the basis splines \\((B_{i,k})\\).\nAll basis splines have compact support.\nIf grid is regularly spaced there is \\(B_k\\) such that \\(B_{i,k}(x) = B_k(x-x_i)\\)\n\n\n\n\n\n\n\n\nBasis Splines\n\n\n\n\n\n\n\nUnfortunately basis splines are not “interpolating” in the sense that in general \\[f(x_i) \\neq \\sum_{n} f(x_n) B_{n,k} (x_i)\\]\nOne must choose other coefficients \\((c_n)\\) which satisfy:\n\\[y_i = \\sum_n c_n B_{n,k} (x_i)\\]\n\nthere are more coefficients than data points: requires boundary conditions\n\nf’’=0: natural spline\n\ngoing from \\(y_n\\) to \\(c_n\\) is called prefiltering\n\n\n\n\n\n\nf(x) = log(x)\nxs = 1:0.2:5\nA = [f(x) for x in xs]\n\n# linear interpolation\ninterp_linear = LinearInterpolation(xs, A)\ninterp_linear(1.3) # interpolate\n\n# cubic spline interpolation\ninterp_cubic = CubicSplineInterpolation(xs, A)\ninterp_cubic(1.3) # interpolate\n\n\n\n\nNote that in \\(y_i = \\sum_n c_n B_{n,k} (x_i)\\), \\(y_i\\) and \\(c_n\\) could perfectly well be vectors. If we use a Vector type which implements all operations (zeros, *, …) we can interpolate them with the same operations\nusing StaticArrays\n\nf(x) = SVector(log(x), exp(x))\nxs = 1:0.2:5\nA = [f(x) for x in xs]\n\n# linear interpolation\ninterp_linear = LinearInterpolation(xs, A)\ninterp_linear(1.3) # returns a 2d SVector\n\n\n\n\n\n\n\n\n\nSuppose you want to solve vector equation \\(A x=y\\). Will a small error in \\(y\\) affect a lot the value of \\(x\\)? (in particular round-off errors)\n\ncondition number: \\(\\lim_{\\epsilon\\rightarrow 0} \\sup_{\\delta y\\leq \\epsilon} \\frac{\\delta x}{\\delta y}\\)\nor \\(\\kappa(A) = ||A^{-1}|| || A||\\) where \\(|| ||\\) is a subordonate norm.\nif very-large: the matrix is ill conditioned\n\nWhat makes a matrix ill-conditioned?\n\nsome rows/columns are very small, others are gigantic\nrows/columns are almost colinear\n\n\n\n\n\n\n\n\n\n\n\nLet’s approximate: \\(f(;\\theta) = \\sum_{n=0}^K \\theta_k x^k\\).\nWe need \\((K+1)\\) points to fit a polynomial of order \\(K\\). Let’s take grid points \\((x_0, ... x_{K})\\) and denote \\(y_k=f(x_k)\\)\nWe need to solve in \\((\\theta_k)_{k=[0,K]}\\):\n\n\\[\\forall n \\in[0,K],  \\underbrace{\\sum_k \\theta_k (x_n)^{k}}_{M \\theta} = y_k\\]\n\n\n\n\n\n\\(M\\) has a special structure, a Vencermode matrix: \\[\nM =\n\\begin{bmatrix}\n1 & x_0 & x_0^2 \\cdots & x_0^K \\\\\\\\\n1 & x_1 & x_1^2  \\cdots  & x_1^K \\\\\\\\\n1 & x_2 & x_2^2  \\cdots  & x_2^K \\\\\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\n1 & x_K & x_K^2  \\cdots & x_K^K\n\\end{bmatrix}\n\\]\nVandermonde matrix is ill-conditioned if points are too close or if \\(K\\) is high.\n\n\n\n\n\n\nDefine a scalar product over functions on the domain \\([a,b]\\) by choosing a positive weight function \\(w(x)\\). \\[<P,Q> = \\int_a^b w(x) P(x)Q(x) dx\\]\nConstruct an orthogonal base \\((T_n)_{n=[1,K]}\\).\nApproximate \\[f(x)\\approx f(x; \\theta) = \\sum_{n=0}^K \\theta_n T_n(x)=\\sum_{n=0}^K <f|T_n> T_n(x)\\]\n\nthis is optimal for the norm associated to \\(<>\\) (projection on the orthogonal base)\n\n\n\n\n\n\nCoefficients can still be identified by inverting: \\[\\forall n \\in[0,K] \\underbrace{\\sum_k \\theta_k T_k(x_n)}_{M \\theta} = y_n\\]\n\\[\nM =\n\\begin{bmatrix}\nT_0(x_0) & T_1(x_0) & \\cdots & T_K(x_0) \\\\\\\\\nT_0(x_1) & T_1(x_1) &  \\cdots  & T_K(x_1) \\\\\\\\\nT_0(x_2) & T_1(x_2) &  \\cdots  & T_K(x_2) \\\\\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nT_0(x_K) & T_1(x_K) &  \\cdots & T_K(x_K)\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\nRed: Runge function \\(f(x)=\\frac{1}{1+25x^2}\\)\nBlue: interpolates at 6, regularly-spaced, points\nGreen: interpolates at 10, regularly-spaced, points\nWhat happens when interpolation order increases?\n\noscillations increase.\n\nDoes it contradict Stone-Weierstrass theorem ? No.\nSolutions:\n\nuse regression method instead\nchoose the interpolation points wisely\n\n\n\n\n\n\n\n\n\n\n\n\nThere is an optimal way to choose the interpolation points:\n\nthe roots of \\(cos(\\frac{2 k - 1}{2n} \\pi)\\) for [-1,1]\nrescale for a finite interval [a,b]\n\nfor the interpolating polynomial: \\[|f(x) - P_n(x)|  \\leq \\frac{1}{2^n (n+1)!} \\max_{\\xi \\in [-1,1]} |f^n(\\xi)|\\]\n\n\n\n\n\n\nChebychev polynomials (of the first kind) have their zeros on the nodes.\nDefinitions:\n\n\\(T_n(x) = \\cos(n \\arccos(x))\\) (in [0,1])\nrecursive: \\(T_0(x)=1\\), \\(T_1(x)=x\\), \\(T_n(x)=2 x T_{n-1}(x)-T_{n-2}(x)\\)\n\nVery good choice:\n\nmatrix \\(M\\) is well conditioned: \\(\\sqrt{2}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsider a function \\(f\\) defined on a space \\(X_1 \\times X_d\\)\nTake \\(d\\) grids \\(\\mathcal{G}_1\\subset X_1, ..., \\mathcal{G}_d\\subset X_d\\) with linear approximation bases \\(\\mathcal{B}_1=(b_1^1, ... b_1^{N_1}),..., \\mathcal{B}_d=(b_d^1, ... b_d^{N_d})\\).\nThen \\(f\\) can be approximated by \\(f(x_1, ... x_d ; \\theta) = \\sum_{i_1=1}^{N_1} ... \\sum_{i_d=1}^{N_d} \\theta_{i_1, ... i_d} \\underbrace{b_{i_1}^1(x_1) ... b_{i_d}^d(x_d)}_{\\text{Product Base}}\\)\nMorality:\n\nlinear appoximation along each dimension induces a natural (multi)-linear in many dimensions\nCoefficients are still the solution of a linear system: \\[M \\theta = y\\]\nbut \\(M\\) has a special structure (tensor product)\n\nProblem: number of coefficients to determine increases exponentially with number of dimensions:\n\n“Curse of Dimensionality”\n\n\n\n\n\n\n\nWays to mitigate the curse of dimensionality\nRemedies:\n\nsparse grids\nadaptive approximation\nneural networks\n…\n\nNo black-magic theorem: there is no solution to the curse of dimensionality\n\n.. but there are methods to adapt to problem whose intrinsic dimension is smaller than the actual number of variables\n\n\n\n\n\n\n\nInterpolations.jl can interepolate on multi-dim grids\nIf you want to construct basis matrices yourself, BasisMatrices.jl:\n\nusing BasisMatrices"
  },
  {
    "objectID": "slides/perturbation.html#today",
    "href": "slides/perturbation.html#today",
    "title": "Perturbation Analysis",
    "section": "Today",
    "text": "Today\nStudy Neoclassical Model of Growth with Deterministic Productivity Shocks\n\nDerive First Order Conditions\nComputing Derivatives Numerically\nSolution Method\n\nLinear Time Iteration\n\nImplementation"
  },
  {
    "objectID": "slides/perturbation.html#deriving-first-order-conditions",
    "href": "slides/perturbation.html#deriving-first-order-conditions",
    "title": "Perturbation Analysissss",
    "section": "Deriving First Order Conditions",
    "text": "Deriving First Order Conditions"
  },
  {
    "objectID": "slides/perturbation.html#neoclassical-growth-model",
    "href": "slides/perturbation.html#neoclassical-growth-model",
    "title": "Perturbation Analysis",
    "section": "Neoclassical Growth Model",
    "text": "Neoclassical Growth Model\n\n\n\nTransition Equation \\[\\begin{eqnarray}\nk_t & = & (1-\\delta) k_{t-1} + i_{t-1} \\\\\nz_t & = & \\rho z_{t-1}\n\\end{eqnarray}\n\\]\nDefinition: \\[c_t = \\exp(z_t) k_t^\\alpha - i_t\\]\nControl \\(i_t\\in[0, \\exp(z_t)k_t^\\alpha[\\)\n\nor equivalently \\(c_t \\in ]0, \\exp(z_t) k_t^{\\alpha}]\\)\n\nObjective: \\[\\max_{i_t} \\sum_{t\\geq0} \\beta^t U(c_t)\\]\n\n\n\nCalibration:\n\n\\(\\beta = 0.96\\)\n\\(\\delta = 0.1\\)\n\\(\\gamma = 4.0\\)\n\\(\\alpha = 0.3\\)\n\\(U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}\\)"
  },
  {
    "objectID": "slides/perturbation.html#computing-derivatives",
    "href": "slides/perturbation.html#computing-derivatives",
    "title": "Perturbation Analysissss",
    "section": "Computing Derivatives",
    "text": "Computing Derivatives"
  },
  {
    "objectID": "slides/perturbation.html#solution-using-linear-time-iteration",
    "href": "slides/perturbation.html#solution-using-linear-time-iteration",
    "title": "Perturbation Analysissss",
    "section": "Solution using Linear Time iteration",
    "text": "Solution using Linear Time iteration"
  },
  {
    "objectID": "slides/time_iteration.html",
    "href": "slides/time_iteration.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "Last week, we did discrete dynamic programming\n\ntoday, we have continuous decision rules\nwe will need to interpolate\n\nWe were using Bellman representation\n\nwe’ll use first order representation (Euler and co.)\n\nOther combinations are possible\n\nvalue function iteration with discretization\n\nWe will see the main time-iteration algorithm\n\nand mention some of its variants\n\n\n\n\n\n\n\nAll variables of the model are vectors:\n\nstates \\(s \\in \\mathcal{S} \\subset R^{n_s}\\)\ncontrols \\(x \\in \\mathcal{F}(\\mathcal{S}, R^{n_x})\\)\n\nwe assume bounds \\(a(s)\\leq x \\leq b(s)\\)\n\nshocks: \\(\\epsilon \\sim \\text{i.i.d. distrib}\\)\n\nTransition: \\[s_{t+1} = g(s_t, x_t, \\epsilon_{t+1})\\]\nValue function: \\[V(s) = E_0 \\sum_{t\\geq 0} \\beta^t \\left[ U(s_t, x_t)\\right]\\]\nSolution is a function \\(V()\\) (value) which is a fixed point of the Bellman-operator: \\[\\forall s, V(s) = \\max_{a(s)\\leq x \\leq b(s)} U(s,x) + \\beta E \\left[ V(g(s,x,\\epsilon)) \\right]\\]\nThe argmax, defines a decision rule function: \\(x = \\varphi(s)\\)\n\n\n\n\n\n\n\n\nAll variables of the model are vectors:\n\nstates \\(s_t\\)\ncontrols \\(x_t\\)\n\nwe assume bounds \\(a(s)\\leq x \\leq b(s)\\)\n\nshocks: \\(\\epsilon_t\\) (i.i.d. random law)\n\nTransition: \\[s_{t+1} = g(s_t, x_t, \\epsilon_{t+1})\\]\nDecision rule: \\[x_t = \\varphi(s_t)\\]\nArbitrage: \\[ E_t\\left[ f(s_t, x_t, s_{t+1}, x_{t+1}) \\right] = 0 \\perp a(s_t) \\leq x_t \\leq b(s_t) \\]\n\n\n\n\nThese equations must be true for any \\(s_t\\).\nRemark: time subscript are conventional. They are used to precise:\n\nwhen expectation is taken (w.r.t \\(\\epsilon_{t+1}\\))\nto avoid repeating \\(x_t=\\varphi(s_t)\\) and \\(x_{t+1} = \\varphi(s_t)\\)\n\nSometimes there are bounds on the controls\n\nwe encode them with complementarity constraints\nmore on it later\n\n\n\n\n\n\n\n\n\n\n\ncapital accumulation: \\[k_t = (1-\\delta)k_{t-1} + i_{t-1}\\]\nproduction: \\[y_t = k_t^\\alpha\\]\nconsumption: \\[c_t = (1-s_t) y_t\\] \\[i_t = s_t y_t\\]\noptimality: \\[\\beta E_t \\left[ \\frac{U^{\\prime}(c_{t+1})}{U^{\\prime}(c_{t})} (1-\\delta + k_{t+1}^{\\alpha-1}\\alpha) \\right]= 1\\]\n\n\n\n\nstates: \\(k_t\\), with one transition equation\ncontrols: \\(y_t, c_t, i_t, s_t\\), with four “arbitrage” equations\n\nit is possible but not mandatory to reduce the number of variables/equations by simple susbtitutions\n\n\n\n\n\n\n\n\n\n\nSimplest consumption/savings model:\n\nTransition: \\[w_t = \\exp(\\epsilon_t) + (w_{t-1} - c_{t-1}) \\overline{r}\\]\nObjective: \\[\\max_{0\\leq c_t \\leq w_t} E_0 \\left[ \\sum \\beta^t U(c_t) \\right]\\]\nFirst order conditions: \\[\\beta E_t \\left[  \\frac{U^{\\prime}(c\\_{t+1})}{U^{\\prime}(c_t)} \\overline{r} \\right] - 1 \\perp 0 \\leq c_t \\leq w_t\\]\n\n\n\n\nF.O.C. reads as: \\[\\beta E_t \\left[  \\frac{U^{\\prime}(c\\_{t+1})}{U^{\\prime}(c_t)} \\overline{r} \\right] - 1 \\leq 0 \\perp c_t \\leq w_t\\] and \\[0 \\leq \\beta E_t \\left[  \\frac{U^{\\prime}(c\\_{t+1})}{U^{\\prime}(c_t)} \\overline{r} \\right] - 1  \\perp 0 \\leq c_t \\]\nFirst one reads: if my marginal utility of consumption today is higher than expected mg. utility of cons. tomorrow, I’d like to consume more, but I can’t because, consumption is bounded by income (and no-borrowing constraint).\nSecond one reads: only way I could tolerate higher utility in the present, than in the future, would be if I want dissave more than I can, or equivalently, consume less than zero. This is never happening.\n\n\n\n\n\n\n\n\n\nConsider the new keynesian model we have seen in the introduction: - Assume \\(z_t\\) is an autocorrelated shock: \\[z_t = \\rho z_{t-1} + \\epsilon_t\\] - new philips curve (PC):\\[\\pi_t = \\beta \\mathbb{E}\\_t \\pi_{t+1} + \\kappa y_t\\] - dynamic investment-saving equation (IS):\\[y_t = \\beta \\mathbb{E}\\_t y_{t+1} - \\frac{1}{\\sigma}(i_t - \\mathbb{E}\\_t(\\pi_{t+1}) ) - {\\color{green} z_t}\\] - Interest Rate Setting (taylor rule): \\[i_t = \\alpha_{\\pi} \\pi_t + \\alpha_{y} y_t\\]\n\n\n\nThe model satisfies the same specification with:\n\none state \\(z_t\\) and one transition equation\nthree controls: \\(\\pi_t\\), \\(y_t\\) and \\(i_t\\) with three “arbitrage” equation\n\nThese are not real first order conditions as they are not derived from a maximization program\n\nunless one tries to microfound them…\n\nIt is possible to add a zero-lower bound constraint by replacing IRS with: \\[ \\alpha_{\\pi} \\pi_t + \\alpha_{y} y_t \\leq i_t \\perp 0 \\leq i_t\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSo we have the equation, \\(\\forall s_t\\)\n\n\\[ E_t\\left[ f(s_t, x_t, s_{t+1}, x_{t+1}) \\right] = 0 \\perp a(s_t) \\leq x_t \\leq b(s_t)\\]\\[ E_t\\left[ f(s_t, x_t, s_{t+1}, x_{t+1}) \\right] = 0\\]\n\nwhere \\[s_{t+1} = g(s_t, x_t, \\epsilon_{t+1})\\] \\[x_t = \\varphi(s_t)\\] \\[x_{t+1} = \\tilde{\\varphi}(s_{t+1})\\]\n\nLet’s leave the complementarity conditions aside for now\nIn equilibrium \\(\\tilde{\\varphi} = {\\varphi}\\)\n\n\n\n\n\n\nWe can rewrite everything as one big functional equation: \\[\\forall s, \\Phi(\\varphi, \\tilde{\\varphi})(s) = E\\left[ f(s, \\varphi(s), g(s,\\varphi(s), \\epsilon), \\tilde{\\varphi}(g(s,\\varphi(s), \\epsilon)) \\right]\\]\nA solution is \\(\\varphi\\) such that \\(\\Phi(\\varphi, \\varphi) = 0\\)\nThe Coleman operator \\(\\mathcal{T}\\) is defined implicitly by: \\[\\Phi(\\mathcal{T}(\\varphi), \\varphi)=0\\]\nThe core of the time iteration algorithm, consists in the recursion: \\[\\varphi_{n+1} = \\mathcal{T}(\\varphi_n)\\]\nIt maps future decision rules to current decision rules\n\nsame as “linear time iterations”, remember?\n\nSounds fun but how do we implement it concretely?\n\n\n\n\n\n\nWe need to find a way to:\n\ncompute expectations\nrepresent decision rules \\(\\varphi\\) and \\(\\varphi\\) with a finite number of parameters\n\n\n\n\n\n\n\nComputing expectations:\n\ndiscretize shock \\(\\epsilon\\) with finite quantization \\((w_i, e_i)_{i=1:K}\\)\nreplace optimality condition with: \\[\\forall s, \\Phi(\\varphi, \\tilde{\\varphi})(s) = \\sum_i w_i f(s, \\varphi(s), g(s,\\varphi(s), e_i), \\tilde{\\varphi}(g(s,\\varphi(s), e_i)) \\]\n\n… but we still can’t compute all the \\(\\varphi\\)\n\n\n\n\n\n\nWe’ll limit ourselves to interpolating functional spaces\n\nWe define a finite grid \\(\\mathbf{s}=(s_1, ... s_N)\\) to approximate the state space (\\(\\mathbf{s}\\) is a finite vector of points)\nIf we know the vector of values \\(\\mathbf{x}=(x_1, ..., x_N)\\) a function \\(\\varphi\\) takes on \\(\\mathbf{s}\\), we approximate \\(\\varphi\\) at any \\(s\\) using an interpolation scheme \\(\\mathcal{I}\\): \\[\\varphi(s) \\approx \\mathcal{I}(s, \\mathbf{s}, \\mathbf{x})\\]\n\nNow if we replace \\(\\varphi\\) by \\(\\mathcal{I}(s, \\mathbf{s}, \\mathbf{x})\\) and \\(\\tilde{\\varphi}\\) by \\(\\mathcal{I}(s, \\mathbf{s}, \\mathbf{\\tilde{x}})\\) the functional equation becomes: \\[\\forall s,  \\Phi(\\varphi, \\tilde{\\varphi})(s) \\approx F( \\mathbf{x}, \\mathbf{\\tilde{x}} )(s) = \\sum_i w_i f(s, x, \\tilde{s}, \\tilde{x})\\] where \\[x = \\mathcal{I}(s, \\mathbf{s}, \\mathbf{x})\\] \\[\\tilde{s} = g(s, x, e_i)\\] \\[\\tilde{x} = \\mathcal{I}(s, \\mathbf{s}, \\mathbf{\\tilde{x}})\\]\n\n\n\n\n\n\nNote that this equation must be statisfied \\(\\forall s\\).\nIn order to pin-down the \\(N\\) coefficients \\(\\mathbf{x}\\), it is enough to satisfy the equations at \\(N\\) different points.\nHence we solve the square system: \\[\\forall i\\in [1,N], F( \\mathbf{x}, \\mathbf{\\tilde{x}} )(s_i) = 0\\]\nIn vectorized form, this is just: \\[F( \\mathbf{x}, \\mathbf{\\tilde{x}} )(\\mathbf{s}) = 0\\]\nOr, since grid \\(\\mathbf{s}\\) is fixed: \\[F( \\mathbf{x}, \\mathbf{\\tilde{x}} ) = 0\\]\nNow the vector of decisions today, at each point of the grid, is determined as a function of the vector of decisions tomorrow, on the same grid.\n\n\n\n\n\n\nChoose a finite grid for states \\(\\mathbf{s} = (s_1, ..., s_N)\\)\nFor a given vector of controls tomorrow \\(\\mathbf{\\tilde{x}}\\), one can compute theoptimality of a vector of controls today by computing the value of :\\[F( \\mathbf{x}, \\mathbf{\\tilde{x}} ) = \\sum_i w_i f(\\mathbf{s}, \\mathbf{x}, \\tilde{\\mathbf{s}}, \\tilde{\\mathbf{x}})\\] \\[\\mathbf{\\tilde{s}} = g(\\mathbf{s}, \\mathbf{x}, e_i)\\] \\[\\mathbf{\\tilde{x}} = \\mathcal{I}(\\mathbf{\\tilde{s}}; \\mathbf{s}, \\mathbf{{x}})\\]\nNote that because we use interpolating approximation: \\(\\forall i, x_i = \\mathcal{I}(s, \\mathbf{s}, \\mathbf{x})\\)\nWe have enough to define an approximated time-iteration operator: implicitly defined by \\[F(T(\\mathbf{x}), \\mathbf{x}))\\]\nWe can then implement time-iteration, but…\n\nhow do we compute \\(T(x)\\)?\n\n\n\n\n\n\n\nIn each step, we have a guess, for decision rule tomorrow \\(\\mathbf{\\tilde{x}}\\)\nWe can then find the decision rule today, by solving numerically for: \\(\\mathbf{x} \\mapsto F(\\mathbf{x}, \\mathbf{\\tilde{x}})\\)\n\nusually with some variant of a Newton method\n\nIt is possible to solve for the values at each grid point separately…\n\nfor each \\(i\\), find optimal controls \\(x_i\\) in state \\(s_i\\) that satisfy \\(F(x_i, \\mathbf{\\tilde{x}}) = 0\\)\nall the problems are independent from each other\n\n…or to solve everything as a big system\n\nthe jacobian is block-diagonal: finding optimal value in state \\(i\\) or in state \\(j\\) today are two independent problems\n\n\n\n\n\n\n\n\nDiscretize state-space with grid \\(\\mathbf{s}=(s_1, ..., s_N)\\)\nChoose initial values, for the vector of controls on the grid \\(\\mathbf{x}=(x_1, ..., x_N)\\)\nSpecify tolerance levels \\(\\eta>0\\) and \\(\\epsilon>0\\)\nGiven an intial guess \\(\\mathbf{x_n}\\)\n\nfind the zero \\(\\mathbf{x_{n+1}}\\) of function \\(\\mathbf{u}\\mapsto F(u,\\mathbf{x_n})\\)\n\nthat is, such that controls on the grid are optimal given controls tomorrow\nnonlinear solver can use \\(\\mathbf{x_n}\\) as initial guess\n\ncompute norm \\(\\mathbf{\\eta_n} = |\\mathbf{x_n} - \\mathbf{x_{n+1}}|\\)\nif \\(\\eta_n<\\eta\\), stop and return \\(\\mathbf{x_{n+1}}\\)\n\nelse, set \\(\\mathbf{x_n} \\leftarrow \\mathbf{x_{n+1}}\\) and continue\n\n\nLike usual, during the iterations, it is useful to look at \\(\\mathbf{\\epsilon_n}=|F(\\mathbf{x_n},\\mathbf{x_n})|\\) and \\(\\lambda_n = \\frac{\\eta_n}{\\eta_{n-1}}\\)\n\n\n\n\n\n\nWhen there aren’t any occasionally binding constraint, we look for the of zero \\(\\mathbf{x_{n+1}}\\) of function \\(\\mathbf{u}\\mapsto F(u,\\mathbf{x_n})\\).\nIf we define the vector of constraints on all grid points as \\(\\mathbf{a}=(a(s_1), ..., a(s_N))\\) and \\(\\mathbf{b}=(b(s_1), ..., b(s_N))\\), we can rewrite the system to solve as: \\[F(u) \\perp \\mathbf{a} \\leq u \\leq \\mathbf{b}\\]\nThen we can:\n\nfeed \\(F\\), \\(a\\) and \\(b\\) to an NCP solver (like nlsolve.jl)\nor transform this relation using Fisher-Burmeister function into a smooth nonlinear system\n\n\n\n\n\n\n\nYou can check out:\n\nendogenous grid points:\n\nmathematically equivalent to TI,\nmuch faster for models that have a particular structure (consumption saving models)\nno need for a nonlinear solver\n\nimproved time iterations:\n\nsame as policy iterations for value function iterations\nconvergence is equivalent to that of TI\nmuch faster but requires correct initial guess\n\nparameterized expectations\n\nrequires that all controls are determined as a function of expectations"
  },
  {
    "objectID": "slides/optimization.html",
    "href": "slides/optimization.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "Optimization is everywhere in economics:\n\nto model agent’s behaviour: what would a rational agent do?\n\nconsumer maximizes utility from consumption\nfirm maximizes profit\n\nan economist tries to solve a model:\n\nfind prices that clear the market\n\n\n\n\n\n\n\nroot finding: \\(\\text{find  $x$ in $X$ such that $f(x)=0$}\\)\nminimization/maximization \\(\\min_{x\\in X} f(x)\\) or \\(\\max_{x\\in X} f(x)\\)\noften a minimization problem can be reformulated as a root-finding problem\n\\[x_0 = {argmin}_{x\\in X} f(x) \\overbrace{\\iff}^{??} f^{\\prime} (x_0) = 0\\]\n\n\n\n\n\n\ngeneral consideration about optimization problems\none-dimensional root-finding\none-dimensional optimization\nlocal root-finding\nlocal optimization\nconstrained optimization\nconstrained root-finding\n\n\n\n\n\n\n\n\n\n\ncontinuous versus discrete optimization\nconstrained and unconstrained optimization\nglobal and local\nstochastic and deterministic optimization\nconvexity\n\n\n\n\n\n\nChoice is picked from a given set (\\(x\\in X\\)) which can be:\n\ncontinuous: choose amount of debt \\(b_t \\in [0,\\overline{b}]\\), of capital \\(k_t \\in R^{+}\\)\ndiscrete: choose whether to repay or default \\(\\delta\\in{0,1}\\), how many machines to buy (\\(\\in N\\)), at which age to retire…\na combination of both: mixed integer programming\n\n\n\n\n\n\n\nDiscrete optimization requires a lot of combinatorial thinking. We don’t cover it.\nSometimes a discrete choice can be approximated by a mixed strategy (i.e. a random strategy).\n\nInstead of \\(\\delta\\in{0,1}\\) we choose \\(x\\) in \\(prob(\\delta=1)=\\sigma(x)\\)\nwith \\(\\sigma(x)=\\frac{2}{1+\\exp(-x)}\\)\n\n\n\n\n\n\n\nUnconstrained optimization: \\(x\\in R\\)\nConstrained optimization: \\(x\\in X\\)\n\nbudget set: \\(p_1 c_1 + p_2 c_2 \\leq I\\)\npositivity of consumption: \\(c \\geq 0\\).\n\nIn good cases, the optimization set is convex…\n\npretty much always in this course\n\n\n\n\n\n\n\nCommon case, especially in machine learning \\[f(x) = E_{\\epsilon}[ \\xi (\\epsilon, x)]\\]\nOne wants to maximize (resp solve) w.r.t. \\(x\\) but it is costly to compute expectation precisely using Monte-Carlo draws (there are other methods).\nA stochastic optimization method allows to use noisy estimates of the expectation, and will still converge in expectation.\nFor now we focus on deterministic methods. Maybe later…\n\n\n\n\n\n\nIn principle, there can be many roots (resp maxima) within the optimization set.\nAlorithma that find them all are called “global”. For instance:\n\ngrid search\nsimulated annealing\n\nWe will deal only with local algorithms, and consider local convergence properties.\n\n->then it might work or not\nto perform global optimization just restart from different points.\n\n\n\n\n\n\n\nThe full mathematical treatment will typically assume that \\(f\\) is smooth (\\(\\mathcal{C}_1\\) or \\(\\mathcal{C}_2\\) depending on the algorithm).\nIn practice we often don’t know about these properties\n\nwe still try and check we have a local optimal\n\nSo: fingers crossed\n\n\n\n\n\nHere is the surface representing the objective that a deep neural network training algorithm tries to minimize.\n\nAnd yet, neural networks do great things!\n\n\n\n\n\nbe able to handcode simple algos (Newton, Gradient Descent)\nunderstand the general principle of the various algorithms to compare them in terms of\n\nrobustness\nefficiency\naccuracy\n\nthen you can just switch the various options, when you use a library…\n\n\n\n\n\n\n\n\n\n\nFind \\(x \\in [a,b]\\) such that \\(f(x) = 0\\). Assume \\(f(a)f(b) <0\\).\nAlgorithm\n\nStart with \\(a_n, b_n\\). Set \\(c_n=(a_n+b_n)/2\\)\nCompute \\(f(c_n)\\)\n\n\nif \\(f(c_n)f(a_n)>0\\) then set \\((a_{n+1},b_{n+1})=(a_n,c_n)\\)\nelse set \\((a_{n+1},b_{n+1})=(c_n,b_n)\\)\n\n\nIf \\(f(c_n)<\\epsilon\\) and/or \\(\\frac{b-a}/2^n<\\delta\\) stop. Otherwise go back to 1.\n\n\n\n\n\n\n\nNo need for initial guess: globally convergent algorithm\n\nnot a global algorithm…\n… in the sense that it doesn’t find all solutions\n\n\\(\\delta\\) is a guaranteed accuracy on \\(x\\)\n\\(\\epsilon\\) is a measure of how good the solution is\nthink about your tradeoff: (\\(\\delta\\) or \\(\\epsilon\\) ?)\n\n\n\n\n\n\n\nFind \\(x\\) such that \\(f(x) = 0\\). Use \\(x_0\\) as initial guess.\n\\(f\\) must be \\(\\mathcal{C_1}\\) and we assume we can compute its derivative \\(f^{\\prime}\\)\nGeneral idea:\n\nobserve that the zero \\(x^{\\star}\\) must satisfy \\[f(x^{\\star})=0=f(x_0)+f^{\\prime}(x_0)(x^{\\star}-x_0) + o(x-x_0)\\]\nHence a good approximation should be \\[x^{\\star}\\approx = x_0- f(x_0)/f^{\\prime}(x_0)\\]\n\n\n\n\n\n\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n- \\frac{f(x_n}{f^{\\prime}(x_n)}=f^{\\text{newton}}(x_n)\\)\nstop if \\(|x_{n+1}-x_n|<\\eta\\) or \\(|f(x_n)| < \\epsilon\\)\n\nConvergence: quadratic\n\n\n\n\n\n\n\nWhat if we can’t compute \\(f^{\\prime}\\) or it is expensive to do so?\n\nIdea: try to approximate \\(f^{\\prime}(x_n)\\) from the last iterates\n\nsecant method: \\(f^{\\prime}(x_n)\\approx \\frac{f(x_n)-f(x_{n-1})}{x_n-x_{n-1}}\\) \\(x_{n+1} = x_n- f(x_n)\\frac{x_n-x_{n-1}}{f(x_n)-f(x_{n-1})}\\)\n\nrequires two initial guesses: \\(x_1\\) and \\(x_0\\)\nsuperlinear convergence: \\(\\lim \\frac{x_t-x^{\\star}}{x_{t-1}-x^{\\star}}\\rightarrow 0\\)\n\n\n\n\n\n\n\nHow could Newton method fail?\n\nbad guess\n\n-> start with a better guess\n\novershoot\n\n-> dampen the update (problem: much slower)\n-> backtrack\n\nstationary point\n\n-> if root of multiplicity \\(m\\) try \\(x_{n+1} = x_n- m f(x_n)/f^{\\prime}(x_n)\\) (FIX)\n\n\n\n\n\n\n\n\nSimple idea:\n\nat stage \\(n\\) given \\(f(x_n)\\) compute Newton step \\(\\Delta_n=-\\frac{f(x_n)}{f^{\\prime}(x_n)}\\)\nfind the smallest \\(k\\) such that \\(|f(x_n-\\Delta/2^k)|<|f(x_n)|\\)\nset \\(x_{n+1}=x_n-\\Delta/2^k\\)\n\n\n\n\n\n\n\n\n\n\n\nMinimize \\(f(x)\\) for \\(x \\in [a,b]\\)\nChoose \\(\\Phi \\in [0,0.5]\\)\nAlgorithm:\n\nstart with \\(a_n < b_n\\) (initially equal to \\(a\\) and \\(b\\))\ndefine \\(c_n = a_n+\\Phi(b_n-a_n)\\) and \\(d_n = a_n+(1-\\Phi)(b_n-a_n)\\)\n\nif \\(f(c_n)<f(d_n)\\) set \\(a_{n+1},b_{n+1}=a_n, d_n\\)\nelse set \\(a_{n+1}, b_{n+1}= c_n, b_n\\)\n\n\n\n\n\n\n\n\nThis is guaranteed to converge to a local minimum\nIn each step, the size of the interval is reduced by a factor \\(\\Phi\\)\nBy choosing \\(\\Phi=\\frac{\\sqrt{5}-1}{2}\\) one can save one evaluation by iteration.\nRemark that bisection is not enough\n\n\n\n\n\n\n\nMinimize \\(f(x)\\) for \\(x \\in R\\) given initial guess \\(x_0\\)\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n (1-\\lambda)- \\lambda f^{\\prime}(x_n)\\)\nstop if \\(|x_{n+1}-x_n|<\\eta\\) or \\(|f^{\\prime}(x_n)| < \\epsilon\\)\n\n\n\n\n\n\n\nUses local information\n\none needs to compute the gradient\nnote that gradient at \\(x_n\\) does not provide a better guess for the minimum than \\(x_n\\) itself\nlearning speed is crucial\n\nConvergence speed: linear\n\nrate depend on the learning speed\noptimal learning speed? the fastest for which there is convergence\n\n\n\n\n\n\n\nMinimize \\(f(x)\\) for \\(x \\in R\\) given initial guess \\(x_0\\)\nBuild a local model of \\(f\\) around \\(x_0\\) \\[f(x) = f(x_0) + f^{\\prime}(x_0)(x-x_0) + f^{\\prime\\prime}(x_0)\\frac{(x-x_0)^2}{2} + o(x-x_0)^2\\]\nAccording to this model, \\[ f(x{\\star}) = min_x f(x)\\iff \\frac{d}{d x} \\left[ f(x_0) + f^{\\prime}(x_0)(x-x_0) + f^{\\prime\\prime}(x_0)\\frac{(x-x_0)^2}{2} \\right] = 0\\] which yields: \\(x^{\\star} = x_0 - \\frac{f^{\\prime}(x_0)}{f^{\\prime\\prime}(x_0)}\\)\nthis is Newton applied to \\(f^{\\prime}(x)=0\\)\n\n\n\n\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n-\\frac{f^{\\prime}(x_0)}{f^{\\prime\\prime}(x_0)}\\)\nstop if \\(|x_{n+1}-x_n|<\\eta\\) or \\(|f^{\\prime}(x_n)| < \\epsilon\\)\n\nConvergence: quadratic\n\n\n\n\n\n\n\n\n\n\n\nMinimize \\(f(x)\\) for \\(x \\in R^n\\) given initial guess \\(x_0 \\in R^n\\)\nMany intuitions from the 1d case, still apply\n\nreplace derivatives by gradient, jacobian and hessian\nrecall that matrix multiplication is not commutative\n\nSome specific problems:\n\nupdate speed can be specific to each dimension\nsaddle-point issues (for minimization)\n\n\n\n\n\n\nFunction \\(f: R^p \\rightarrow R^q\\)\n\nJacobian: \\(J(x)\\) or \\(f^{\\prime}\\_x(x)\\), \\(p\\times q\\) matrix such that: \\[J(x)\\_{ij} = \\frac{\\partial f(x)\\_i}{\\partial x_j}\\]\nGradient: \\(\\nabla J(x)\\), gradient when \\(q=1\\)\nHessian: denoted by \\(H(x)\\) or \\(f^{\\prime\\prime}\\_{xx}(x)\\) when \\(q=1\\): \\[H(x)\\_{jk} = \\frac{\\partial f(x)}{\\partial x_j\\partial x_k}\\]\nIn the following explanations, \\(|x|\\) denotes the supremum norm, but most of the following explanations also work with other norms.\n\n\n\n\n\n\n\n\n\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n- J(x_{n})^{-1}f(x_n)=f^{\\text{newton}}(x_n)\\)\nstop if \\(|x_{n+1}-x_n|<\\eta\\) or \\(|f(x_n)| < \\epsilon\\)\n\nConvergence: quadratic\n\n\n\n\n\n\nwhat matters is the computation of the step \\(\\Delta_n = {\\color{\\red}{J(x_{n})^{-1}}} f(x_n)\\)\ndon’t compute \\(J(x_n)^{-1}\\)\n\nit takes less operations to compute \\(X\\) in \\(AX=Y\\) than \\(A^{-1}\\) then \\(A^{-1}Y\\)\n\nstrategies to improve convergence:\n\ndampening: \\(x_n = (1-\\lambda)x^{n-1} - \\lambda \\Delta_n\\)\nbacktracking: choose \\(k\\) such that \\(|f(x_n-2^{-k}\\Delta_n)|\\)<\\(|f(x_{n-1})|\\)\nlinesearch: choose \\(\\lambda\\in[0,1]\\) so that \\(|f(x_n-\\lambda\\Delta_n)|\\) is minimal\n\n\n\n\n\n\n\n\n\n\n\nMinimize \\(f(x) \\in R\\) for \\(x \\in R^n\\) given \\(x_0 \\in R^n\\)\nAlgorithm\n\nstart with \\(x_n\\) \\[x_{n+1} = (1-\\lambda) x_n - \\lambda \\nabla f(x_n)\\]\nstop if \\(|x_{n+1}-x_n|<\\eta\\) or \\(|f(x_n)| < \\epsilon\\)\n\nComments:\n\nlots of variants\nautomatic differentiation software makes gradient easy to compute\nconvergence is typically linear\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n-{\\color{\\red}{H(x_{n})^{-1}}}\\color{\\green}{ J(x_n)'}\\)\nstop if \\(|x_{n+1}-x_n|<\\eta\\) or \\(|f(x_n)| < \\epsilon\\)\n\nConvergence: quadratic\nProblem:\n\n\\(H(x_{n})\\) hard to compute efficiently\nrather unstable\n\n\n\n\n\n\n\nRecall the secant method:\n\n\\(f(x_{n-1})\\) and \\(f(x_{n-2})\\) are used to approximate \\(f^{\\prime}(x_{n-2})\\).\nIntuitively, \\(n\\) iterates would be needed to approximate a hessian of size \\(n\\)….\n\nBroyden method: takes \\(2 n\\) steps to solve a linear problem of size \\(n\\)\n\nuses past information incrementally\n\n\n\n\n\n\n\nConsider the approximation: \\[f(x_n)-f(x_{n-1}) \\approx J(x_n) (x_n - x_{n-1})\\]\n\n\\(J(x_n)\\) is unknown and cannot be determined directly as in the secant method.\nidea: \\(J(x_n)\\) as close as possible to \\(J(x_{n-1})\\) while solving the secant equation\nformula: \\[J_n = J_{n-1} + \\frac{(f(x_n)-f(x_{n-1})) - J_{n-1}(x_n-x_{n-1})}{||x_n-x_{n-1}||^2}(x_n-x_{n-1})^{\\prime}\\]\n\n\n\n\n\n\n\nRestrict to least-square minimization: $min_x _i f(x)_i^2 R $\nThen up to first order, \\(H(x_n)\\approx J(x_n)^{\\prime}J(x_n)\\)\nUse the step: \\(({J(x_n)^{\\prime}J(x_n)})^{-1}\\color{\\green}{ J(x_n)}\\)\nConvergence:\n\ncan be quadratic at best\nlinear in general\n\n\n\n\n\n\n\nLeast-square minimization: $min_x _i f(x)_i^2 R $\nreplace \\({J(x_n)^{\\prime}J(x_n)}^{-1}\\) by \\({J(x_n)^{\\prime}J(x_n)}^{-1} +\\mu I\\)\n\nadjust \\(\\lambda\\) depending on progress\n\nuses only gradient information like Gauss-Newton\nequivalent to Gauss-Newton close to the solution (\\(\\mu\\) small)\nequivalent to Gradient far from solution (\\(\\mu\\) high)\n\n\n\n\n\n\n\n\n\nConsider the optimization problem: \\[\\max U(x_1, x_2)\\]\nunder the constraint \\(p_1 x_1 + p_2 x_2 \\leq B\\)\nwhere \\(U(.)\\), \\(p_1\\), \\(p_2\\) and \\(B\\) are given.\nHow do you find a solution by hand?\n\n\n\n\n\nCompute by hand\nEasy:\n\nsince the budget constraint must be binding, get rid of it by stating \\(x_2 = B - p_1 x_1\\)\nthen maximize in \\(x_1\\), \\(U(x_1, B - p_1 x_1)\\) using the first order conditions.\n\nIt works but:\n\nbreaks symmetry between the two goods\nwhat if there are other constraints: \\(x_1\\geq \\underline{x}\\)?\nwhat if constraints are not binding?\nis there a better way to solve this problem?\n\n\n\n\n\n\n\nAnother method, which keeps the symmetry. Constraint is binding, trying to minimize along the budget line yields an implicit relation between \\(d x_1\\) and \\(d x_2\\) \\[p_1 d {x_1} + p_2 d {x_2} = 0\\]\nAt the optimal: \\(U^{\\prime}\\_{x_1}(x_1, x_2)d {x_1} + U^{\\prime}\\_{x_2}(x_1, x_2)d {x_2} = 0\\)\nEliminate \\(d {x_1}\\) and \\(d {x_2}\\) to get one condition which characterizes optimal choices for all possible budgets. Combine with the budget constraint to get a second condition.\n\n\n\n\n\n\nTake a penalty function \\(p(x)\\) such that \\(p(x)=K>0\\) if \\(x>0\\) and \\(p(x)=0\\) if \\(x \\leq 0\\). Maximize: \\(V(x_1, x_2) = U(x_1, x_2) - p( p_1 x_1 + p_2 x_2 - B)\\)\nClearly, \\(\\min U \\iff \\min V\\)\nProblem: \\(\\nabla V\\) is always equal to \\(\\nabla U\\).\nSolution: use a smooth solution function like \\(p(x) = x^2\\)\nProblem: distorts optimization\n\nSolution: adjust weight of barrier and minimize \\(U(x_1, x_2) - \\kappa p(x)\\)\n\nPossible but hard to choose the weights/constraints.\n\n\n\n\n\n\nAnother idea: is there a canonical way to choose \\(\\lambda\\) such that at the minimum it is equivalent to minimize the original problem under constraint or to minimize \\[V(x_1, x_2) = U(x_1, x_2) - \\lambda (p_1 x_1 + p_2 x_2 - B)\\]\nClearly, when the constraint is not binding we must have \\(\\lambda=0\\). What should be the value of \\(\\lambda\\) when the constraint is binding ?\n\n\n\n\n\n\nIf \\((x^{\\star},y^{\\star})\\) is optimal there exists \\(\\lambda\\) such that:\n\n\\((x^{\\star},y^{\\star})\\) maximizes \\(U(x_1, x_2) + \\lambda (B- p_1 x_1 - p_2 x_2)\\)\n\\(\\lambda \\geq 0\\)\n\\(B- p_1 x_1 - p_2 x_2 \\geq 0\\)\n\\(\\lambda (B - p_1 x_1 - p_2 x_2 ) = 0\\)\n\nThe three latest conditions are called “complementarity” or “slackness” conditions\n\nthey are equivalent to \\(\\min(\\lambda, B - p_1 x_1 - p_2 x_2)=0\\)\nwe denote \\(\\lambda \\geq 0 \\perp B- p_1 x_1 + p_2 x_2 \\geq 0\\)\n\n\\(\\lambda\\) can be interpreted as the welfare gain of relaxing the constraint.\n\n\n\n\n\n\nWe can get first order conditions that factor in the constraints:\n\n\\(U^{\\prime}_x - \\lambda p_1 = 0\\)\n\\(U^{\\prime}_y - \\lambda p_2 = 0\\)\n\\(\\lambda \\geq 0 \\perp B-p_1 x_1 -p_2 x_2 \\geq 0\\)\n\nIt is now a nonlinear system of equations with complementarities (NCP)\n\nthere are specific solution methods to deal with it\n\n\n\n\n\n\n\nGeneral formulation for vector-valued functions \\[f(x)\\geq 0 \\perp g(x)\\geq 0\\] means \\[\\forall i, f_i(x)\\geq 0 \\perp g_i(x)\\geq 0\\]\n\nNCP do not necessarily arise from a single optimization problem\n\nThere are robust (commercial) solvers for NCP problems (PATH, Knitro) for that\nHow do we solve it numerically?\n\nassume constraint is binding then non-binding then check which one is good\n\nOK if not too many constraints\n\nreformulate it as a smooth problem\napproximate the system by a series of linear complementarities problems (LCP)\n\n\n\n\n\n\n\n\nConsider the Fisher-Burmeister function \\[\\phi(a,b) = a+b-\\sqrt{a^2+b^2}\\]\nIt is infinitely differentiable, except at \\((0,0)\\)\nShow that \\(\\phi(a,b) = 0 \\iff \\min(a,b)=0 \\iff a\\geq 0 \\perp b \\geq 0\\)\nAfter substitution in the original system one can use regular non-linear solver\n\nfun fact: the formulation with a \\(\\min\\) is nonsmooth but also works quite often\n\n\n\n\n\n\n\n\n\n\nRobust optimization code is contained in the following libraries:\n\nRoots.jl: one-dimensional root finding\nNLSolve.jl: multidimensional root finding (+complementarities)\nOptim.jl: minimization\n\nThe two latter libraries have a somewhat peculiar API, but it’s worth absorbing it.\n\nin particular they provide non-alocating algorithms for functions that modify arguments in place\nthey are compatible with automatic differentiation\n\n\njulia> f(x) = [x[1] - x[2] - 1, x[1] + x[2]]\nf (generic function with 1 method)\n\njulia> NLsolve.nlsolve(f, [0., 0.0])\nResults of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [0.0, 0.0]\n * Zero: [0.5000000000009869, -0.5000000000009869]\n * Inf-norm of residuals: 0.000000       \n * Iterations: 1                       \n * Convergence: true\n   * |x - x'| < 0.0e+00: false\n   * |f(x)| < 1.0e-08: true                           \n * Function Calls (f): 2\n * Jacobian Calls (df/dx): 2"
  },
  {
    "objectID": "slides/ddp.html",
    "href": "slides/ddp.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "The imperialism of Dynamic Programming\n— Recursive Macroeconomic Theory (Ljunqvist & Sargent)\n\n\nI spent the Fall quarter (of 1950) at RAND. My first task was to find a name for multistage decision processes. An interesting question is, “Where did the name, dynamic programming, come from?” The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word “research”. I’m not using the term lightly; I’m using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term research in his presence. You can imagine how he felt, then, about the term mathematical. The RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? In the first place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word “programming”. I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. I thought, let’s kill two birds with one stone. Let’s take a word that has an absolutely precise meaning, namely dynamic, in the classical physical sense. It also has a very interesting property as an adjective, and that is it’s impossible to use the word dynamic in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It’s impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.\n\n— Richard Bellman, Eye of the Hurricane: An Autobiography (1984, page 159)\n\n\n\n\n\n\n\n\n\nStochastic process: family of random variables indexed by time\nA stochastic process has the Markov property if its future evolution depends only on its current state.\nSpecial cases:\n\n\n\n\n\n\n\n\n\n\nDiscrete States\nContinuous States\n\n\n\n\nDiscrete Time\nDiscrete Markov Chain\nContinuous Markov Chain\n\n\nContinuous Time\nMarkov Jump Process\nMarkov Process\n\n\n\n\n\n\n\na matrix \\(M \\in R^n\\times R^n\\) matrix is said to be stochastic if\n\nall coefficents are non-negative\nall the lines lines sum to 1 (\\(\\forall i, \\sum_j M_{ij} = 1\\))\n\na probability density is a vector \\(\\mu \\in R^n\\) such that :\n\nall components are non-negative\nall coefficients sum to 1 (\\(\\sum_{i=1}^n \\mu_{i} = 1\\))\n\na distribution is a vector with such that:\n\nall components are non-negative\n\n\n\n\n\n\n\nConsider: \\(\\mu_{i,t+1}' =\\mu_t' P\\)\nWe have \\(\\mu_{i,t+1} = \\sum_{k=1}^n \\mu_{k,t} P_{k, i}\\)\nAnd: \\(\\sum_i\\mu_{i,t+1} = \\sum_i \\mu_{i,t}\\)\nPostmultiplication by a stochastic matrix preserves the mass.\nInterpretation: \\(P_{ij}\\) is the fraction of the mass initially in state \\(i\\) which ends up in \\(j\\)\n\n\n\n\n\n\\[\\underbrace{\n\\begin{pmatrix}\n? & ? & ?\n\\end{pmatrix}\n}\\_{\\mu_{t+1}'} = \\underbrace{\n\\begin{pmatrix}\n0.5 & 0.3 & 0.2\n\\end{pmatrix}\n}\\_{\\mu_t'} \\begin{pmatrix}\n0.4 & 0.6 & 0.0 \\\\\\\\\n0.2 & 0.5 & 0.3 \\\\\\\\\n0 & 0 & 1.0\n\\end{pmatrix}\\]\n\n\n\n\n\n\n\n\n\n\nDenote by \\(S=(s_1,...s_n)\\) a finite set with \\(n\\) elements (\\(|S|=n\\)).\nA Markov Chain with values in \\(S\\) and with transitions given by a stochastic matrix \\(P\\in R^n\\times R^n\\) identfies a stochastic process \\((X\\_t)\\_{t\\geq 0}\\) such that \\[P\\_{ij} = Prob(X\\_{t+1}=s\\_j|X\\_t=s_i)\\]\nIn words, line \\(i\\) describes the conditional distribution of \\(X_{t+1}\\) conditional on \\(X_t=s_i\\). [TODO: illustration]\n\n\n\n\n\n\nIt is easy to show that for any \\(k\\), \\(P^k\\) is a stochastic matrix.\n\\(P^k_{ij}\\) denotes the probability of ending in \\(j\\), after \\(k\\) periods, starting from \\(i\\)\nGiven an initial distribution \\(\\mu_0\\in R^{+ n}\\)\n\nWhich states will visited with positive probability between t=0 and t=k?\nWhat happens in the very long run?\n\nWe need to study a little bit the properties of Markov Chains\n\n\n\n\n\n\nTwo states \\(s_i\\) and \\(s_j\\) are connected if \\(P_{ij}>0\\)\nWe call incidence matrix: \\(\\mathcal{I}(P)=(\\delta_{P_{ij}>0})_{ij}\\)\nTwo states \\(i\\) and \\(j\\) communicate with each other if there are \\(k\\) and \\(l\\) such that: \\((P^k)_ {i,j}>0\\) and \\((P^l)_ {j,i}>0\\)\n\nit is an equivalence relation\nwe can define equivalence classes\n\nA stochastic matrix \\(P\\) is irreducible if all states communicate\n\nthere is a unique communication class\n\n\n\n\n\n\n\n\n\n\n\nAll states can be reached with positive probably from any other initial state.\n\n\n\n\n\n\nThere is a subset of states (poor), which absorbs all the mass coming in.\n\n\n\n\n\n\n\n\nAre there cycles? Starting from a state \\(i\\), how long does it take to return to \\(i\\)?\nThe period of a state is defined as \\[gcd( {k\\geq 1 | (P^k)_{i,i}>0} )\\]\nIf a state has a period d>1 the chain returns to the state only at dates multiple of d.\n\n\n\n\n\n\n\n\n\n\nIf you start from some states, you return to it, but not before two periods.\n\n\n\n\n\n\nIf some mass leaves a state, some of it returns to the state in the next period.\n\n\n\n\n\n\n\n\n\\(\\mu\\) is a stationary distribution if \\(\\mu' = \\mu' P\\)\nTheorem: there always exists such a distribution\n\nproof: Brouwer theorem (fixed-point result for compact-convex set)\n\\(f: \\mu\\rightarrow (\\mu'P)'\\)\n\nTheorem:\n\nif P is irreducible the fixed point \\(\\mu^{\\star}\\) is unique\nif P is irreducible and aperiodic \\(|\\mu_0' P^k - \\mu^{\\star}| \\underset{k\\to+\\infty}{\\longrightarrow} 0\\) for any initial distribution \\(\\mu_0\\)\n\nWe then say the Markov chain is ergodic\n\\(\\mu^{\\star}\\) is the ergodic distribution\n\nit is the best guess, one can do for the state of the chain in the very far future\n\n\n\n\n\n\n\nBrouwer’s theorem: Let \\(\\mathcal{C}\\) be a compact convex subset of \\(R^n\\) and \\(f\\) a continuous mapping \\(\\mathcal{C}\\rightarrow \\mathcal{C}\\). Then there exists a fixed point \\(x_0\\in \\mathcal{C}\\) such that \\(f(x_0)=x_0\\)\nResult hinges on:\n\ncontinuity of \\(f: \\mu \\mapsto \\mu P\\)\nconvexity of \\(\\\\{x \\in R^n | |x|=1 \\\\}\\) (easy to check)\ncompactness of \\(\\\\{x \\in R^n | |x|=1 \\\\}\\)\n\nit is bounded\nand closed (the inverse image of 1 for \\(u\\mapsto |u|\\) which is continuous)\n\n\n\n\n\n\n\nHow do we compute the stationary distribution?\n\nSimulation\nLinear algebra\nDecomposition\n\n\n\n\n\n\nVery simple idea:\n\nstart with any \\(\\mu_0\\) and compute the iterates recursively\n\\(\\mu_{n+1}' = \\mu_n' P\\)\nconvergence is linear:\n\n\\(|\\mu_{n+1} - \\mu_n| \\leq |P| |\\mu_n - \\mu_{n-1}|\\)\n\n\n\n\n\n\n\n\nFind the solution of \\(\\mu'(P-I) = 0\\) ?\n\nnot well defined, 0 is a solution\nwe need to incorporate the constraint \\(\\sum_i(\\mu_i)=1\\)\n\nMethod:\n\nDefine \\(M_{ij} = \\begin{cases} 1 &\\text{if} & j =0 \\\\\\\\ (P-I)_{ij} & \\text{if} & j> 1 \\end{cases}\\)\nDefine \\(D_i = \\begin{cases} 1 & \\text{if} & j = 0 \\\\\\\\0 & \\text{if} & j>0 \\end{cases}\\)\nWith a linear algebra solver\n\nlook for a solution \\(\\mu\\) of \\(\\mu' M = D\\)\nor \\(M^{\\prime} \\mu = D\\prime\\)\nif you find a solution, it is unique (theorem)\n\n\nAlternative:\n\nminimize residual squares of overidentified system\n\n\n\n\n\n\n\njulia [1-2|3-6|7-9|10-12|13-14|15-17] # we use the identity matrix and the \\ operator using LinearAlgebra: I, \\ # define a stochastic matrix (lines sum to 1) P = [  0.9  0.1 0.0  ;        0.05 0.9 0.05 ;        0.0  0.9 0.1  ] # define an auxiliary matrix M = P' - I M[end,:] .= 1.0 # define rhs R = zeros(3) R[end] = 1 # solve the system μ = M\\R # check that you have a solution: @assert sum(μ) == 1 @assert all(abs.(μ'P - μ').<1e-10)\n\n\n\n\n\nKnowledge about the structure of the Markov Chain can help speedup the calculations\nThere are methods for potentially very-large linear system\n\nNewton-Krylov based methods, GMRES\n\nBasic algorithms are easy to implement by hand\nQuantEcon toolbox has very good methods to study markov chains\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Decision Problem\n\nstates: \\(s \\in S\\)\nactions: \\(x \\in X(s)\\)\ntransitions: \\(\\pi(s'| s, x) \\in S\\)\n\n\\(probability\\) of going to \\(s'\\) in state \\(s\\)…\n… given action \\(x\\)\n\n\n\n\n\n\n\nReward: \\(r(s,x) \\in R\\)\n\naka felicity, intratemporal utility\n\n\nPolicy: \\(x(): s \\rightarrow x\\in X(s)\\)\n\na.k.a. decision rule\nwe consider deterministic policy\ngiven \\(x()\\), the evolution of \\(s\\) is a Markov process\n\n\\(\\pi(. |s, x())\\) is a distribution for \\(s'\\) over \\(S\\)\nit depends only on \\(s\\)"
  },
  {
    "objectID": "slides/ddp.html#finite-horizon-dmdp",
    "href": "slides/ddp.html#finite-horizon-dmdp",
    "title": "Advanced Macro: Numerical Methods",
    "section": "Finite horizon DMDP",
    "text": "Finite horizon DMDP\n\n\nFinite horizon DMDP\nWhen \\(T<\\infty\\). With discrete action the problem can be represented by a tree.\n[GRAPH]\n\n\n\nFinite horizon DMDP\n\nIntuition: backward induction.\n\nFind optimal policy \\(x_T(s_T)\\) in all terminal states \\(s_T\\). Set \\(V_T(s_T)\\) equal to \\(r(s_T, \\pi_T)\\)\nFor each state \\(s_{k-1}\\in S\\) find \\(x_{k-1}\\in X(s_{k-1})\\) which maximizes \\[V_{k-1}(s_{k-1}) = \\max_{x_{k-1}(s_{k-1})\\in X(s_{k-1})}r(s_{k-1},x_{k-1}) + \\delta \\underbrace{ \\sum_{s_k\\in S} \\pi(s_k | s_{k-1}, x_{k-1} ) V_k(s_k)} _{ \\textit{expected continuation value} }\\]\n\nPolicies \\(x_0(), ... x_T()\\) are Markov-perfect:\n\nthey maximize utility on all subsets of the “game”\nalso from t=0\n\n\n\n\n\nRemarks\n\nCan we do better than this naive algorithm?\n\nnot really\nbut we can try to limit \\(S\\) to make the maximization step faster\nexclude a priori some branches in the tree using knowledge of the problem"
  },
  {
    "objectID": "slides/ddp.html#infinite-horizon-dmdp",
    "href": "slides/ddp.html#infinite-horizon-dmdp",
    "title": "Advanced Macro: Numerical Methods",
    "section": "Infinite horizon DMDP",
    "text": "Infinite horizon DMDP\n\n\nInfinite horizon DMDP\n\nHorizon is infinite: \\[V(s; x()) =  \\max E_0 \\sum_{t=0}^{\\infty} \\delta^t r(s_t, x_t) \\]\nIntuition:\n\nlet’s consider the finite horizon version \\(T<\\infty\\) and \\(T >> 1\\)\ncompute the solution, increase \\(T\\) until the solution doesn’t change\nin practice: take an initial guess for \\(V_{T}\\) then compute optimal \\(V_{T-1}\\), \\(V_{T_2}\\) and so on, until convergence of the \\(V\\)s\n\n\n\n\n\nInfinite horizon DMDP (2)\n\nThis is possible, it’s called Successive Approximation or Value Function Iteration\n\nhow fast does it converge? linearly\ncan we do better? yes, quadratically\n\nwith howard improvement steps\n\n\n\n\n\n\nSuccessive Approximation\n\nConsider the decomposition: \\[V(s; x()) = E_0 \\sum_{t=0}^{\\infty} \\delta^t r(s_t, x_t) = E_0 \\left[ r(s, x(s)) + \\sum_{t=1}^{\\infty} \\delta^t r(s_t, x_t) \\right]\\]\n\nor\n\\[V(s; x()) =  r(s, x(s)) + \\delta \\sum_{s'} p(s'|s,x(s)) V(s'; x()) \\]\n\n\n\nSuccessive Approximation (2)\n\nTaking continuation value as given we can certainly improve the value in every state \\(\\tilde{V}\\) by choosing \\(\\tilde{x}()\\) so as to maximze \\[\\tilde{V}(s; x(), \\tilde{x}()) =  r(s, \\tilde{x}(s)) + \\delta \\sum_{s'} \\pi(s'|s,\\tilde{x}(s) )V(s'; x()) \\]\nBy construction: \\(\\forall s, \\tilde{V}(s, \\tilde{x}(), x()) > {V}(s, x())\\)\n\nit is an improvement step\n\nCan \\({V}(s, \\tilde{x}())\\) be worse for some states than \\({V}(s, \\tilde{x}())\\) ?\n\nactually no\n\n\n\n\n\nBellman equation\n\nIdea:\n\nit should not be possible to improve upon the optimal solution.\nHence the optimal value \\(V\\) and policy \\(x^{\\star}\\) should satisfy: \\[\\forall s\\in S, V(s) = \\max_{y(s)} r(s, y(s)) + \\delta \\sum_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, y(s)) V(s^{\\prime})\\] with the maximum attained at \\(x(s)\\).\n\nThis is referred to as the Bellman equation.\nConversely, it is possible to show that a solution to the Bellman equation is also an optimal solution to the initial problem.\n\n\n\n\nBellman operator\n\nThe function \\(G\\) is known as the Bellman operator: \\[G: V \\rightarrow \\max\\_{y(s)} r(s, y(s)) + \\delta \\sum\\_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, y(s)) V(s^{\\prime})\\]\nDefine sequence \\(V_n = G(V_{n-1})\\)\n\nit goes back in time\nbut is not the time-iteration operator\n\nOptimal value is a fixed point of G\nDoes \\(G\\) converges to it ? Yes, if \\(G\\) is a contraction mapping.\n\n\n\n\nBlackwell’s theorem\n\nLet \\(X\\subset R^n\\) and let \\(\\mathcal{C}(X)\\) be a space of bounded functions \\(f: X\\rightarrow R\\), with the sup-metric. \\(B: \\mathcal{C}(X)\\rightarrow \\mathcal{C}(X)\\) be an operator satisfying two conditions:\n\n(monotonicity) if \\(f,g \\in \\mathcal{C}(X)\\) and \\(\\forall x\\in X, f(x)\\leq g(x)\\) then\n\n\\(\\forall x \\in X (Bf)(x)\\leq(Bg)(x)\\)\n\n(discounting) there exists some \\(\\delta\\in]0,1[\\) such that: \\(B.(f+a)(x)\\leq (B.f)(x) + \\delta a, \\forall f \\in \\mathcal{C}(X), a\\geq 0, x\\in X\\)\n\nThen \\(B\\) is a contraction mapping with modulus \\(\\delta\\).\n\n\n\n\nSuccessive Approximation\n\nUsing the Blackwell’s theorem, we can prove the Bellman operator is a contraction mapping (do it).\nThis justifies the Value Function Iteration algorithm:\n\nchoose an initial \\(V_0\\)\ngiven \\(V_n\\) compute \\(V_{n+1} = G(V_n)\\)\niterate until \\(|V_{n+1}- V_n|\\leq \\eta\\)\n\nPolicy rule is deduced from \\(V\\) as the maximand in the Bellman step\n\n\n\n\nSuccessive Approximation (2)\n\nNote that convergence of \\(V_n\\) is geometric\nBut \\(x_n\\) converges after a finite number of iterations (\\(X\\) is finite)\n\nsurely the latest iterations are suboptimal\nthey serve only to evaluate the value of \\(x^{\\star}\\)\n\nIn fact:\n\n\\(V_n\\) is never the value of \\(x_n()\\)\nshould we try to keep both in sync?\n\n\n\n\n\nPolicy iteration for DMDP\n\nChoose initial policy \\(x_0()\\)\nGiven initial guess \\(x_n()\\)\n\ncompute the value function \\(V_n=V( ;x_n)\\) which satisfies\n\\(\\forall s, V_n(s) = r(s, x_n(s)) + \\delta \\sum_{s'} \\pi(s'| s, x_n(s)) V_n(s')\\)\nimprove policy by maximizing in \\(x_n()\\) \\[\\max_{x_n()} r(s, x_n(s)) + \\delta \\sum_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, x_n(s)) V_{n-1}(s^{\\prime})\\]\n\nRepeat until convergence, i.e. \\(x_n=x_{n+1}\\)\nOne can show the speed of convergence (for \\(V_n\\)) is quadratic\n\nit corresponds the Newton-Raphson steps applied to \\(V\\rightarrow G(V)-V\\)\nwe’ll see Newton-Raphson in the next session (on optimization)\n\n\n\n\n\nHow do we compute the value of a policy?\n\nGiven \\(x_n\\), goal is to find \\(V_n(s)\\) in \\[\\forall s,  V_n(s) = r(s, x_n(s)) + \\delta \\sum_{s'} \\pi(s'| s, x_n(s)) V_n(s')\\]\nTwo(three) approaches:\n\nsimulate the policy rule and compute \\(E\\left[ \\sum_t \\delta^t r(s_t, x_t) \\right]\\) with Monte-Carlo draws\nsuccessive approximation:\n\nput \\(V_k\\) in the rhs and recompute the lhs \\(V_{k+1}\\), replace \\(V_k\\) by \\(V_{k+1}\\) and iterate until convergence\n\nsolve a linear system in \\(V_n\\)\n\nFor 2 and 3 it helps representing a linear operator \\(M\\) such that \\(V_{n+1} = R_n + \\delta M_n . V_n\\)"
  },
  {
    "objectID": "slides/ddp.html#example-the-mccall-model",
    "href": "slides/ddp.html#example-the-mccall-model",
    "title": "Advanced Macro: Numerical Methods",
    "section": "Example : the McCall Model",
    "text": "Example : the McCall Model\n\n\nIdea\n\nMcCall model:\n\nwhen should an unemployed person accept a job offer?\nchoice between:\n\nwait for a better offer (and receive low unemp. benefits)\naccept a suboptimal job offer\n\n\nWe present a variant of it, with a small probability of loosing a job.\n\n\n\n\nFormalization\n\nWhen unemployed in date, a job-seeker\n\nconsumes unemployment benefit \\(c_t = \\underline{c}\\)\nreceives in every date \\(t\\) a job offer \\(w_t\\)\n\n\\(w_t\\) is i.i.d.,\ntakes values \\(w_1, w_2, w_3\\) with probabilities \\(p_1, p_2, p_3\\)\n\nif job-seeker accepts, becomes employed at rate \\(w_t\\) in the next period\nelse he stays unemployed\n\nWhen employed at rate \\(w\\)\n\nworker consumes salary \\(c_t = w\\)\nwith small probability \\(\\lambda>0\\) looses his job:\n\nstarts next period unemployed\n\notherwise stays employed at same rate\n\nObjective: \\(\\max E_0 \\left\\\\{ \\sum \\beta^t \\log(w_t) \\right\\\\}\\)\n\n\n\n\nStates / reward\n\nWhat are the states?\n\nemployement status: Unemployed / Employed\nif Unemployed:\n\nthe level \\(w\\in {w_1, w_2, w_3}\\) of the salary that is currently proposed\n\nif Employed:\n\nthe level \\(w\\in {w_1, w_2, w_3}\\) at which worker was hired\n\ncurrent state, can be represented by a 2x3 index\n\nWhat are the actions?\n\nif Unemployed:\n\nreject (false) / accept (true)\n\nif Employed: None\nactions (when unemployed) are represented by a 3 elements binary vector\n\nWhat is the (intratemporal) reward?\n\nif Unemployed: \\(U(c)\\)\nif Employed at rate w: \\(U(w)\\)\nhere it doesn’t depend on the action\n\n\n\n\n\nValue function\n\\(\\newcommand{\\E}{\\mathbb{E}}\\)\n\nWhat is the value of being in a given state?\nIf Unemployed, facing current offer \\(w\\):\n\\[V^U(w) = U(\\underline{c}) + \\max_{a} \\begin{cases} \\beta V^E(w) & \\text{if $a(w)$ is true} \\\\\\\\ \\beta  \\\\E_{a'}\\left[ V^U(a^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\]\nIf Employed, at rate \\(w\\) \\[V^E(w) = U(w) +  (1-\\lambda) \\beta V^E(w) +  \\lambda \\beta \\\\E_{a'}\\left[ V^U(a^{\\prime}) \\right] \\]\nWe can represent value as two functions \\(V^U\\) and \\(V^E\\) of the states as\n\ntwo vectors of Floats, with three elements (recall: value-function is real valued)\n\n\n\n\n\nValue function iteration\n\nTake a guess for value function \\(\\tilde{V^E}\\), \\(\\tilde{V^U}\\), tomorrow\nUse it to compute value function today: \\[V^U(w) = U(\\underline{c}) + \\max_{a(w)} \\begin{cases} \\beta \\tilde{V}^E(w) & \\text{if $a(w)$ is true} \\\\\\\\ \\beta  \\\\E_{a'}\\left[ \\tilde{V}^U(a^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V^E(w) = U(w) +  (1-\\lambda) \\beta \\tilde{V}^E(w) +  \\lambda \\beta \\\\E_{a'}\\left[\\tilde{V}^U(a^{\\prime}) \\right] \\]\n\\((\\tilde{V}^E, \\tilde{V}^U)\\mapsto (V^E, V^U)\\) is one value iteration step\nNote that we don’t have to keep track of policies tomorrow\n\nall information about future decisions is contained in \\(\\tilde{V}^E, \\tilde{V}^U\\)\nbut we can keep track of current policy: \\(a(w): \\arg\\max \\cdots\\)\n\n\n\n\n\nValue evaluation\n\nSuppose we take a policy \\(a(w)\\) as given. What is the value of following this policy forever?\nThe value function \\(V_a^E\\), \\(V_a^U\\) satisfies \\[V_a^U(w) = U(\\underline{c}) + \\begin{cases} \\beta \\tilde{V}^E_a(w) & \\text{if $a(w)$ is true} \\\\\\\\ \\beta  \\\\E_{w'}\\left[ \\tilde{V}^U_a(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V_a^E(w) = U(w) +  (1-\\lambda) \\beta \\tilde{V}^E_a(w) +  \\lambda \\beta \\\\E_{w'}\\left[\\tilde{V}^U_a(w^{\\prime}) \\right] \\]\nNote the absence of the max function: we don’t reoptimize\n\n\n\n\nValue evaluation (2)\n\nHow do you compute value of policy \\(a(w)\\) recursively?\nIterate: \\((\\tilde{V}^E_a, \\tilde{V}^U)\\mapsto (V^E_a, V^U_a)\\) \\[V_a^U(w) \\leftarrow U(\\underline{c}) + \\begin{cases} \\beta \\tilde{V}^E_a(w) & \\text{if $a(w)$ is true} \\\\\\\\ \\beta  \\\\E_{w'}\\left[ \\tilde{V}^U_a(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V_a^E(w) \\leftarrow U(w) +  (1-\\lambda) \\beta \\tilde{V}^E_a(w) +  \\lambda \\beta \\\\E_{w'}\\left[\\tilde{V}^U_a(w^{\\prime}) \\right] \\]\nNote the absence of the max function:\n\nwe don’t reoptimize\nwe we keep the same policy all along\n\n\n\n\n\nPolicy iteration\n\nstart with policy \\(a(w)\\)\nevaluate the value of this policy \\(V^E_a, V^U_a\\)\n\ncompute the optimal policy in the Bellman iteration\nkeep the improved policy \\(a(w)\\)\n\nhere: \\(a(w) = \\arg\\max_{a(w)} \\begin{cases} \\beta \\tilde{V}^E(w)\\\\\\\\ \\beta \\\\E_{a'}\\left[ \\tilde{V}^U(a^{\\prime}) \\right] \\end{cases}\\)\n\n\niterate until \\(a(w)\\) converges"
  },
  {
    "objectID": "slides/rbc.html",
    "href": "slides/rbc.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "Like the neo-classical growth model\nWith shocks\nWith labour\nWith a decentralized interpretation\n\n\n\n\n\n\n\n\nstates:\n\nproductivity: \\(z_t\\)\ncapital: \\(k_t\\)\n\ntwo independent control variables:\n\nconsumption: \\(c_t \\in [0,y_t], c_t\\geq 0, c_t\\leq y_t\\)\nlabor: \\(n_t\\)\n\nshock:\n\ntfp shock: \\(\\epsilon_t \\sim \\mathcal{N}(0,\\sigma)\\)\n\nobjective: \\[\\max_{\\begin{matrix}c_t, n_t\\\\\\\\c_t \\geq 0, y_t \\geq c_t, n_t \\geq 0, 1 \\geq n_t\\end{matrix}} \\mathbb{E}_0 \\left[ \\sum \\beta^t \\left( U(c_t) + \\chi V(1-n_t) \\right) \\right]\\]\nU and V satisfy Inada conditions, ie \\(U^{\\prime}>0, U^{\\prime \\prime}<0, U^{\\prime}(0)=\\infty\\)\n\n\n\n\ndefinitions:\n\nproduction: \\[y_t  = \\exp(z_t) k_t^{\\alpha} n^{1-\\alpha} + i_t\\]\ninvestment: \\[i_t = y_t - c_t\\]\n\ntransitions: \\[\\begin{eqnarray}\nz_t = (1-\\rho) z_{t-1} + \\epsilon_t\\\\\\\\\nk_t = (1-\\delta) k_{t-1} + i_{t-1}\n\\end{eqnarray}\\]\n\n\n\n\n\n\n\nTwo variables optimization: \\[\\max_{\\begin{matrix}c_1, c_2\\\\\\\\p_1 c_1 + p_2 c_t \\leq B\\end{matrix}} U(c_1, c_2)\\]\nDeterministic opimization (finite horizon) \\[\\max_{\\begin{matrix}c_0, c_1, c_2, ... c_T \\\\\\\\ c_0 + c_1 + \\cdots + c_T \\leq B\\\\\\\\c_0\\geq0, \\cdots c_T \\geq 0 \\end{matrix}} \\sum_{i=1}^{T} \\beta^i U(c_i)\\]\nDeterministic opimization (infinite horizon) \\[\\max_{\\begin{matrix}c_0, c_1, ... \\\\\\\\ c_0 + c_1 + \\cdots \\leq B\\\\\\\\c_0\\geq0, c_1\\geq 0, \\cdots \\end{matrix}} \\sum_{i=1}^{\\infty} \\beta^i U(c_i)\\]\n\n\n\n\n\n\n\n\nexogenous process defines an event tree \\((s)\\)\n\nit is a very useful concept to understand stochastic optimization, complete markets, etc.\nmath for continuous processes a bit involved (filtrations, …), but most intuition can be gained from discrete process\n\n\n\n\n\nhead or tail\n\n\n\n\n\nconsider a discrete process (for instance \\(\\epsilon_t \\in [ \\overline{\\epsilon}, \\underline{\\epsilon}]\\))\n\nan event is defined as the history of the shocks so far\nex: \\((\\overline{\\epsilon} , \\overline{\\epsilon}, \\underline{\\epsilon}, \\overline{\\epsilon})\\)\n\nif \\(s^{\\prime}\\) is the sucessor of \\(s\\) we denote \\(s \\subset s^{\\prime}\\)\n\n\\(s\\) is in the history of \\(s^{\\prime}\\)\ntransition probabilities \\(\\tau(s,s^{\\prime})\\)\n\n\\(1 = \\sum_{s^{\\prime} | s\\subset s^{\\prime}} \\tau(s, s^{\\prime})\\)\n\n\neach node has a given probability \\(p(s)\\). By construction:\n\n\\(p(s^{\\prime}) = p(s) \\tau(s,s^{\\prime})\\)\n\nsometimes, we keep time subscript:\n\nex: \\(s_4 = (\\overline{\\epsilon} , \\overline{\\epsilon}, \\underline{\\epsilon}, \\overline{\\epsilon})\\)\nbut for each \\(t\\) there are many possible \\(s_t\\)\n\n\n\n\n\n\n\n\n\nStochastic optimization (infinite horizon) \\[\\max_{ c_t } \\mathbb{E_0} \\left[ \\sum_{t=1}^{\\infty} \\beta^i U(c_t) \\right]\\]\nWhat it really means (\\(|s|\\) is time of event \\(s\\)) \\[\\max_{ \\forall s,  c(s)} \\sum_{s} p(s) \\beta^{|s|} U(c(s))\\]\nOr: \\[\\max_{ c(s_t) } \\sum_{t}  \\beta^{t} \\sum_{s_t} p(s_t)U(c(s_t))\\]\nThink of it as a regular sum\nWhen you differentiate the lagrangian, you are differentiating w.r.t. all \\(c(s_t)\\), i.e the values of \\(c\\) on each of the nodes.\nExample: cake eating\n\n\n\n\n\n\\[\\max_{\\begin{matrix}c_t, n_t\\\\\\\\c_t \\geq 0\\\\\\\\ y_t \\geq c_t\\\\\\\\n_t \\geq 0\\\\\\\\1 \\geq n_t\\end{matrix}} \\mathbb{E}_0 \\left[ \\sum \\beta^t \\left( U(c_t) + \\chi V(1-n_t) \\right) \\right]\\]\n\nWe know that optimally \\(c_t>0\\), \\(c_t<y_t\\) and \\(n_t>0\\)\n\nequality cases lead to zero production, i.e. infinite marginal utility\n\n\n\n\n\n\n\\[\\max_{\\begin{matrix}c_t, n_t\\\\\\\\c_t \\geq 0\\\\\\\\ k_{t+1} \\geq 0 \\\\\\\\n_t \\geq 0\\\\\\\\1 \\geq n_t \\\\\\\\ y_t \\geq c_t - i_t  \\\\\\\\ k_{t+1} = (1-\\delta) k_t  + i_t \\\\\\\\ y_t = e^{z_t} k_t^{\\alpha} n_t^{1-\\alpha} \\end{matrix}} \\mathbb{E}_0 \\left[ \\sum_t \\beta^t \\left( U(c_t) + \\chi V(1-n_t) \\right) \\right]\\]\n\nWe know that optimally \\(c_t>0\\), and \\(n_t>0\\), \\(k_{t+1}>0\\)\n\nequality cases lead to zero production, i.e. infinite marginal utility\nwe can drop the corresponding constraints\n\nWe assume \\(n_t=1\\) is never binding (this would correspond to unemployment)\n\n\n\n\n\n\\[\\mathcal{L} = \\mathbb{E}\\_0 \\left[ \\sum_t \\beta^t \\left\\\\{ \\begin{matrix} U(c_t) + \\chi V(1-n_t) \\\\\\\\ + \\lambda_t (y_t - c_t) \\\\\\\\  + q_t (k\\_{t+1} - (1-\\delta) k_t - i_{t} ) \\\\\\\\ + \\nu_t (y_t - e^{z_t} k_t^{\\alpha}n_t^{1-\\alpha})  \\end{matrix} \\right\\\\} \\right]\\]\n\nLet’s derive w.r.t. all nonpredetermined values within the sum:\n\n… explain\n\n\n\n\n\n\n\\[\\begin{eqnarray}\nU^{\\prime}(c_t) & = & \\beta \\mathbb{E}\\_t \\left[  U^{\\prime} (c_{t+1}) \\left(\n         (1-\\delta) + \\alpha e^{z\\_{t+1}} k\\_{t+1}^{\\alpha-1} n\\_{t+1}^{1-\\alpha}\n             \\right) \\right] \\\\\\\\\n\\chi V^{\\prime} (1-n_t) & = &  (1-\\alpha) e^{z_t} k_t^{\\alpha} (n_t)^{-\\alpha} U^{\\prime}(c_t)\n\\end{eqnarray}\\]\n\n\n\n\n\nSet \\(U(x) = \\frac{c_t^{1-\\gamma}}{1-\\gamma}\\), \\(V(x) = \\frac{(1-x)^{1-\\eta}}{1-\\eta}\\)\nTry to find the steady state\n\nit is impossible to do so in closed-form\n\nSet \\(\\overline{n} = 0.33\\) and adjust \\(\\chi\\) so that it is a steady-state\n\n\n\n\n\n\n\n\n\n\nSo far, we have assumed, that the same agent decides on consumption and labour supply\nWhat if some decisions are taken in some decentralized markets?\nNew structure:\n\ndecentralized competitive firms\n\nrent capital and workers\nsell goods\n\na representative household\n\nsupplies labour\naccumulates capital and rents it to firms\nconsume goods\n\n\n\n\n\n\n\n\nFirm \\(i\\)\n\nchooses capital \\(k^i\\) and labour \\(n^i\\)\n\nCobb Douglas production: \\(y_i = f(k_i, n_i) = (k_i)^{\\alpha} (n_i)^(1-\\alpha)\\)\nSince there is only one good, its price can be set to \\(1\\)\nFirm takes wages \\(w\\) and rental price of capital \\(r\\) as given: \\[max_{k_i, n_i} \\pi(k_i, n_i) =  f(k_i, n_i) - r  k_i - w n_i\\]\nOptimally:\n\n\\(f_k^{\\prime}(k_i, n_i) = \\alpha k_i^{\\alpha-1} n_i^{1-\\alpha} = r\\)\n\\(f_n^{\\prime}(k_i, n_i) = (1-\\alpha) k_i^{\\alpha-1} n_i^{-\\alpha} = w\\)\n\nRemark:\n\ncapital share: \\(\\frac{r k_i}{y_i} = \\alpha\\)\nlabour share: \\(\\frac{w n_i}{y_i} = 1- \\alpha\\)\nprofits are zero\n\n\n\n\n\n\n\nWhat is the production of all firms if total capital is \\(K\\) and total labour is \\(L\\) ?\nNote that for each firm \\[(1 - \\alpha) \\frac{k_i}{l_i} = \\alpha \\frac{w}{r}\\]\nWe can sum over all firms to get: \\[(1-\\alpha){K} = \\alpha \\frac{w}{r}L\\]\nwe can write: \\[y_i = (k_i)^{\\alpha} (n_i)^{1-\\alpha} = k_i \\left( \\frac{k_i}{n_i} \\right)^{1-\\alpha} = k_i (K/L)^{1-\\alpha}\\]\nand sum over all firms: \\[Y = K (K/L)^{1-\\alpha} = K^\\alpha L ^{1-\\alpha}\\]\nThe sum of many cobb douglas-firms is a big cobb-douglas firm !\n\n\n\n\n\n\nOur representative agent takes \\(w_t\\) and \\(r_t\\) as given.\nHe supplies labour and capital, and decides how much to save so as to maximize: \\[\\max_{\\begin{matrix} c_t, n_t \\\\\\\\ c_t \\leq \\pi_t + r_t k_t + w_t n_t - i_t \\\\\\\\ k_{t+1} = (1-\\delta) k_t + i_t \\\\\\\\ c_t \\geq 0 \\end{matrix}} \\sum_t \\beta^t \\left(U(c_t)  + V(n_t) \\right)\\]\n\nResult: \\[\\begin{eqnarray} \\beta\nU^{\\prime}(c_t) & = & \\beta \\mathbb{E}\\_t \\left[  U^{\\prime} (c_{t+1}) \\left( (1-\\delta) + r_{t+1}\\right) \\right] \\\\\\\\\n\\chi V^{\\prime} (1-n_t) & = & w_t U^{\\prime}(c_t)\n\\end{eqnarray}\\]\n\nResult:\n\nexactly the same equations as in the central planner version (in this case)\nthis formulation can be used to study distortionary taxes:\n\nex: labour income tax \\(\\tau\\)\n\n\n\n\n\n\n\n\nOur representative agent takes \\(w_t\\) and \\(r_t\\) as given.\nHe supplies labour and capital, and decides how much to save so as to maximize: \\[\\max_{\\begin{matrix} c_t, n_t \\\\\\\\ c_t \\leq \\pi_t + (1-\\tau) w_t n_t + r_t k_t - i_t + g_t \\\\\\\\ k_{t+1} = (1-\\delta) k_t + i_t \\\\\\\\ c_t \\geq 0 \\end{matrix}} \\sum_t \\beta^t \\left(U(c_t)  + V(n_t) \\right)\\]\nNote the new budget constraint\n\nlabour income is taxed, but a lump-sum subsidy ensures nothing is destroyed\n\\(g_t =\\tau w_t k_t\\) is not taken into account for intertemporal optimization\n\n\nResult: \\[\\begin{eqnarray} \\beta\nU^{\\prime}(c_t) & = & \\beta \\mathbb{E}\\_t \\left[  U^{\\prime} (c_{t+1}) \\left( (1-\\delta) + r_{t+1}\\right) \\right] \\\\\\\\\n\\chi V^{\\prime} (1-n_t) & = & (1-\\tau) w_t U^{\\prime}(c_t)\n\\end{eqnarray}\\]\n\nResult:\n\nexactly the same equations as in the central planner version (in this case)\nthis formulation can be used to study distortionary taxes:\n\nex: labour income tax \\(\\tau\\)"
  },
  {
    "objectID": "slides/discretization.html",
    "href": "slides/discretization.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "approximate operator with a finite number of iterations:\n\ncompute \\(\\int_a^b f(x) dx\\)\ncompute \\(E_\\omega f(\\omega)\\)\n\nrepresent an infinite dimensional object with a finite set of parameters:\n\n\\(f \\equiv (f(x_i))_{i=1:N}\\) with \\(x_i=a+\\frac{i-1}{N-1}(b-a)\\)\n\ndiscretize arguments\n\n\\(\\omega \\equiv (\\mu_i, \\omega_i)_{i=1:N}\\) such that \\(E_\\omega f(\\omega) \\approx \\sum_i \\mu_i f(\\omega_i)\\) (quantization)\n\ndiscretize continous process by a discrete one:\n\ncontinuous markov chain to discrete markov Chain\n\n\n\n\n\n\n\n\n\n\n\nTake \\(AR1\\) process \\[x_t = \\rho x_{t-1} + \\epsilon_t\\]\n\nwith \\(|\\rho| <1\\) and \\(\\epsilon \\sim N(0,\\sigma)\\)\n\nCan we replace \\((x_t)\\) by a discrete markov chain?\n\napproximate version:\n\ngood time \\(x^g\\) and bad time \\(x^b\\). Probability \\(\\pi\\) of staying in the same, \\(1-\\pi\\) of switching.\n\ntwo systematic methods (available in QuantEcon.jl)\n\nTauchen\nRouwenhorst\n\n\n\n\n\n\n\n\nThe unconditional distribution of an AR1 is a normal law \\(\\mathcal{N}(0,\\frac{\\sigma}{\\sqrt{1-\\rho^2}})\\)\nChoose \\(m>0\\), typically \\(m=3\\)\nBound the process: \\(\\underline{x} = -m \\frac{\\sigma}{\\sqrt{1-\\rho^2}}\\) and \\(\\overline{x} = m \\frac{\\sigma}{\\sqrt{1-\\rho^2}}\\)\nDefine the \\(N\\) discretized points (\\(i\\in[1,n]\\)): \\(y_i = \\underline{x} + \\frac{i-1}{N-1}(\\overline{x}-\\underline{x})\\)\nDefine the transitions:\n\n\\[\\begin{eqnarray}\n\\pi_{ij} & = & prob \\\\left( y_{t+1}=y_j|y_t=y_i\\\\right)\\\\\\\\\n         & = & prob \\\\left( |y_{t+1}-x_j| = \\inf_k |y_{t+1}-x_k| \\left| y_t=y_i \\right. \\right)\n\\end{eqnarray}\\]\n\n\n\n\n\nFormulas \\(\\delta=\\frac{\\overline{x}-\\underline{x}}{N}\\):\n\nif \\(1<k<N-1\\)\n\\[\\pi_{jk} = F(\\frac{y_k + \\delta/2-\\rho y_j}{\\sigma_{\\epsilon}}) - F(y_k + \\delta/2-\\rho y_j)\\]\nif \\(k=1\\)\n\\[\\pi_{j} = F(\\frac{y_k + \\delta/2-\\rho y_j}{\\sigma_{\\epsilon}}) \\]\nif \\(k=N\\)\n\\[\\pi_{j} = 1- F(\\frac{y_k - \\delta/2-\\rho y_j}{\\sigma_{\\epsilon}}) \\]\n\n\n\n\n\n\n\ncompare generated stationary moments between discretized process and true AR1:\n\nE(), Var(), ACor()\n\nby looking at the exact ergodic distribution or by doing some simulations\nnot very precise when then process is very persistent \\(\\rho\\approx 1\\)\n\n\n\n\n\n\nN = 2\n\nchoose \\(y_1=-\\psi\\), \\(y_2=\\psi\\)\ndefine transition matrix: \\[\\Theta_2 = \\begin{bmatrix}\np & 1-p\\\\\\\\\n1-q & q\n\\end{bmatrix}\\]\nchoose \\(p\\), \\(q\\) and \\(\\psi\\) to match some moments: \\(E()\\), \\(Var()\\), \\(ACor()\\)\n\nthey can be computed analytically for AR1 and for discretized version.\n\n\n\n\n\n\n\n\nN >2 \\[\\Theta_N =\np \\begin{bmatrix}  \n\\Theta_{N-1}  & 0\\\\\\\\\n0 & 0\n\\end{bmatrix} +\n(1-p) \\begin{bmatrix}  \n0 & \\Theta_{N-1} \\\\\\\\\n0 & 0\n\\end{bmatrix} +\n(1-q) \\begin{bmatrix}  \n0 & 0\\\\\\\\\n\\Theta_{N-1} & 0\n\\end{bmatrix} +\nq \\begin{bmatrix}  \n0 & 0\\\\\\\\\n0 & \\Theta_{N-1}\n\\end{bmatrix}\n\\]\nNormalize all lines\n\n\n\n\n\n\nProcedure converges to Bernouilli distribution.\nMoments can be computed in closed form:\n\n\\(E() = \\frac{(q-p)\\psi}{2-(p+q)}\\)\n\\(Var() = \\psi^2 \\left[ 1-4 s (1-s) + \\frac{4s(1-s)}{N-1}\\right]\\)\n\\(Acor()= p+q-1\\)\n\nRouwenhorst method performs better for highly correlated processes\n\n\n\n\n\n\n\n\n\n\nGiven \\(f\\), and an iid process \\(\\epsilon \\sim N(0,\\sigma^2)\\), how to approximate \\(E_{\\epsilon} f(\\epsilon)\\) ?\nIdeas:\n\ndraw lots of random \\((\\epsilon\\_n)\\_{n=1:N}\\) and compute \\[\\frac{1}{N}\\sum_{n=1}^N f(\\epsilon_n)\\]\n\naka Monte-Carlo simulations\n\ngiven a method to approximate integrals, compute \\[\\int_{u=-\\infty}^{\\infty} f(u) \\mu(u) du\\] with \\(\\mu(u)=\\frac{1}{\\sigma\\sqrt{2 \\pi}}e^{-\\frac{u^2}{2\\sigma^2}}\\)\ndiscretize (or quantize) the signal \\(\\epsilon\\) as \\((w_i, \\epsilon_i)_{i=1:N}\\) and compute:\n\n\n\\[\\frac{1}{N} \\sum_n w_n f(\\epsilon_n)\\]\n\n\n\n\n\nLet’s take an exemple:\n\nconsumption is \\(C(\\epsilon)=U(e^{\\epsilon})\\)\nwith \\({\\sigma}\\_{\\epsilon}=0.05\\) and \\(U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}\\) and \\(\\gamma=40\\).\n\nLet’s compute \\(E_{\\epsilon}(C(\\epsilon))\\) precisely.\nDiscuss value of \\(\\gamma\\): is it crazy? (risk return)\n\n\n\n\n\nCompute expectation\n```julia [1-3|4-7|9-10|14|16-17] # imports: using Distributions: Normal"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "A few things to remember\nHere is a very important one: \\[e^{-i \\pi} = -1\\]"
  },
  {
    "objectID": "students/pablo/1_epidemiology.html",
    "href": "students/pablo/1_epidemiology.html",
    "title": "Advanced Macro: Numerical Methods",
    "section": "",
    "text": "H.W. Heathcote: epidemiologic models are deterministic models for infectious diseases which are spread by direct person-to-person contact in a population.\nThis kind of models has been used since by a few decades by economist, for matters that have nothing to do with health.\n\ndiffusion of information, or opinions on social medias\nasset prices and fads\nexpectation formation for macroeconomic outcomes (ex: The Epidemiology of Macroeconomic Expectations by Chris Carroll)\n\nEpidemiologic models have two features that depart from standard economic models:\n\nAgent’s behaviour does not take the full system into account, an is irrational in the sense that it isn’t forward looking. Instead, an agent is given a behavioural heuristic.\nThe transitions of the whole system can be determined without solving for complex interactions first.\n\nUnderstanding why these two assumptions are very costly for economists will keep us busy during a big part of the course. Here we just consider two simple models as small programming exercises.\n\n\nThere is a continuum of agents of mass \\(1\\). Each agent can be either “Susceptible” (S), “Infected” (I) or “Recovered” (R). In each period, one agent meets another agent drawn randomly. During a meeting Susceptible agents who meet an infected agent, will always catch the disease (or the fad) but are not contagious. Infected agents, have a probability \\(\\pi\\) of being recovered. Nothing happens to Recovered agents who meet other people. No distinction is made between recovering as a healthy or a dead person.\nWe’re interested in the evolution in the number infected persons, both the speed of infection and the total amount of infected people in every period.\nWrite down the transition equations for \\(n_I\\), the number of infected people, for \\(n_R\\) the number of recovered people and \\(n_S\\) the number of susceptible people.\nCompute the transition function f for the vector state \\(s_t\\) returning \\(s_{t+1}\\).\nCompute the transitions over \\(T\\) periods. Plot the result using Plots.jl. (bonus: check against closed form solution)\nWe now assume a Susceptible person who meets an infected one has a probability \\(\\mu\\) of catching the disease. Update the transition function. Update function \\(f\\) and write a function of \\(\\mu\\), \\(\\pi\\) which returns the simulation. Compare different values of the parameters. How would you interpret “social distancing”? How would you choose parameters \\(\\Pi\\) and \\(\\mu\\)\n\n\n\nWe now consider another version of the model where agents evolve in the space \\(\\mathcal{S}=[0,1]\\times[0,1]\\). There are \\(N\\in\\mathbb{N}\\) agent. At any date, each agent \\(n \\in [0,1]\\) is located at \\((x_n,y_n)\\in \\mathcal{S}\\).\nEach agent moves follows a random walk bounded by \\(\\mathcal{S}\\): \\[x_t = \\min(\\max( x_{t-1} + \\epsilon_t, 0), 1)\\] \\[y_t = \\min(\\max( y_{t-1} + \\eta_t, 0), 1)\\] where \\(\\epsilon_t\\) and \\(\\eta_t\\) are both normally distributed with standard deviation \\(\\sigma\\).\nAt any date, the individual state of an agent is \\(s_t=(x_t, y_t, h_t)\\) where \\(h_t\\) is either “S”, “I” or “R”. \\(v_t\\) denotes the states of all agents (for instance \\(v_t=(s_{n,t})_n\\). The health status of each agent is updated in the following way:\n\nAgents \\(R\\) stay \\(R\\).\nAgents \\(I\\) have probability \\(\\pi\\) to become \\(R\\). They stay \\(I\\) otherwise.\nAn agent of type \\(S\\) in position \\((x,y)\\) has a probability \\(prob(x,y,S)\\) to be infected that is \\(\\mu\\) if there is another infected agent within a radius \\(r>0\\).\n\nDefine a type Agent, which holds the type of an agent. The state of the whole system will be held in a Vector[Agent] type.\nWrite a function spatial_transition(S::Vector{Agent})::Vector{Agent} to compute the transition of the positions. Write another function random_guess(T=100) which simulates for \\(T\\) periods in order to find a good initial guess.\nWrite a function show_population to plot all agents with different colors for different health status.\nWrite a function evolve(S::Vector[Agent])::Vector[Agent] which takes the population in a given state and returns the same population with updated health status.\nWrite a function simulate(S0::Vector[Agent], k=1) to simulate the economy starting from an initially random position with k infected individuals. The returned object should be of type Vector[Vector[Agent]].\nCompute statistics along a simulated path for \\(n_I\\), \\(n_S\\), \\(n_R\\). Plot and compare with the basic SIR model\n\n\n\nHave fun by trying to answer one of these questions: - change probability of infection so that it depends on the number of infected people in the vincinity (with some suitable functional formulation for) - compute an animation of the transition - compute an interactive visualisation (with Interact.jl if available)"
  },
  {
    "objectID": "slides/ddp.html#introduction",
    "href": "slides/ddp.html#introduction",
    "title": "Discrete Dynamic Programming",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "slides/ddp.html#section",
    "href": "slides/ddp.html#section",
    "title": "Discrete Dynamic Programming",
    "section": "",
    "text": "The imperialism of Dynamic Programming\n— Recursive Macroeconomic Theory (Ljunqvist & Sargent)"
  },
  {
    "objectID": "slides/ddp.html#section-1",
    "href": "slides/ddp.html#section-1",
    "title": "Discrete Dynamic Programming",
    "section": "",
    "text": "I spent the Fall quarter (of 1950) at RAND. My first task was to find a name for multistage decision processes. An interesting question is, “Where did the name, dynamic programming, come from?” The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word “research”. I’m not using the term lightly; I’m using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term research in his presence. You can imagine how he felt, then, about the term mathematical. The RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? In the first place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word “programming”. I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. I thought, let’s kill two birds with one stone. Let’s take a word that has an absolutely precise meaning, namely dynamic, in the classical physical sense. It also has a very interesting property as an adjective, and that is it’s impossible to use the word dynamic in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It’s impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.\n\n— Richard Bellman, Eye of the Hurricane: An Autobiography (1984, page 159)"
  },
  {
    "objectID": "slides/ddp.html#markov-chain-and-markov-process-1",
    "href": "slides/ddp.html#markov-chain-and-markov-process-1",
    "title": "Discrete Dynamic Programming",
    "section": "Markov chain and Markov process",
    "text": "Markov chain and Markov process\n\nStochastic process: family of random variables indexed by time\nA stochastic process has the Markov property if its future evolution depends only on its current state.\nSpecial cases:\n\n\n\n\n\n\n\n\n\n\nDiscrete States\nContinuous States\n\n\n\n\nDiscrete Time\nDiscrete Markov Chain\nContinuous Markov Chain\n\n\nContinuous Time\nMarkov Jump Process\nMarkov Process"
  },
  {
    "objectID": "slides/ddp.html#stochastic-matrices",
    "href": "slides/ddp.html#stochastic-matrices",
    "title": "Discrete Dynamic Programming",
    "section": "Stochastic matrices",
    "text": "Stochastic matrices\n\na matrix \\(M \\in R^n\\times R^n\\) matrix is said to be stochastic if\n\nall coefficents are non-negative\nall the lines lines sum to 1 (\\(\\forall i, \\sum_j M_{ij} = 1\\))\n\na probability density is a vector \\(\\mu \\in R^n\\) such that :\n\nall components are non-negative\nall coefficients sum to 1 (\\(\\sum_{i=1}^n \\mu_{i} = 1\\))\n\na distribution is a vector with such that:\n\nall components are non-negative"
  },
  {
    "objectID": "slides/ddp.html#simulation",
    "href": "slides/ddp.html#simulation",
    "title": "Discrete Dynamic Programming",
    "section": "Simulation",
    "text": "Simulation\n\nConsider: \\(\\mu_{i,t+1}' =\\mu_t' P\\)\nWe have \\(\\mu_{i,t+1} = \\sum_{k=1}^n \\mu_{k,t} P_{k, i}\\)\nAnd: \\(\\sum_i\\mu_{i,t+1} = \\sum_i \\mu_{i,t}\\)\nPostmultiplication by a stochastic matrix preserves the mass.\nInterpretation: \\(P_{ij}\\) is the fraction of the mass initially in state \\(i\\) which ends up in \\(j\\)"
  },
  {
    "objectID": "slides/ddp.html#example",
    "href": "slides/ddp.html#example",
    "title": "Discrete Dynamic Programming",
    "section": "Example",
    "text": "Example\n\\[\\underbrace{\n\\begin{pmatrix}\n? & ? & ?\n\\end{pmatrix}\n}_{\\mu_{t+1}'} = \\underbrace{\n\\begin{pmatrix}\n0.5 & 0.3 & 0.2\n\\end{pmatrix}\n}_{\\mu_t'} \\begin{pmatrix}\n0.4 & 0.6 & 0.0 \\\\\\\\\n0.2 & 0.5 & 0.3 \\\\\\\\\n0 & 0 & 1.0\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "slides/ddp.html#representation-as-a-graph",
    "href": "slides/ddp.html#representation-as-a-graph",
    "title": "Discrete Dynamic Programming",
    "section": "Representation as a graph",
    "text": "Representation as a graph"
  },
  {
    "objectID": "slides/ddp.html#probabilistic-interpretation",
    "href": "slides/ddp.html#probabilistic-interpretation",
    "title": "Discrete Dynamic Programming",
    "section": "Probabilistic interpretation",
    "text": "Probabilistic interpretation\n\nDenote by \\(S=(s_1,...s_n)\\) a finite set with \\(n\\) elements (\\(|S|=n\\)).\nA Markov Chain with values in \\(S\\) and with transitions given by a stochastic matrix \\(P\\in R^n\\times R^n\\) identfies a stochastic process \\((X_t)_{t\\geq 0}\\) such that \\[P_{ij} = Prob(X_{t+1}=s_j|X_t=s_i)\\]\nIn words, line \\(i\\) describes the conditional distribution of \\(X_{t+1}\\) conditional on \\(X_t=s_i\\). [TODO: illustration]"
  },
  {
    "objectID": "slides/ddp.html#what-about-longer-horizons",
    "href": "slides/ddp.html#what-about-longer-horizons",
    "title": "Discrete Dynamic Programming",
    "section": "What about longer horizons?",
    "text": "What about longer horizons?\n\nIt is easy to show that for any \\(k\\), \\(P^k\\) is a stochastic matrix.\n\\(P^k_{ij}\\) denotes the probability of ending in \\(j\\), after \\(k\\) periods, starting from \\(i\\)\nGiven an initial distribution \\(\\mu_0\\in R^{+ n}\\)\n\nWhich states will visited with positive probability between t=0 and t=k?\nWhat happens in the very long run?\n\nWe need to study a little bit the properties of Markov Chains"
  },
  {
    "objectID": "slides/ddp.html#connectivity",
    "href": "slides/ddp.html#connectivity",
    "title": "Discrete Dynamic Programming",
    "section": "Connectivity",
    "text": "Connectivity\n\nTwo states \\(s_i\\) and \\(s_j\\) are connected if \\(P_{ij}>0\\)\nWe call incidence matrix: \\(\\mathcal{I}(P)=(\\delta_{P_{ij}>0})_{ij}\\)\nTwo states \\(i\\) and \\(j\\) communicate with each other if there are \\(k\\) and \\(l\\) such that: \\((P^k)_ {i,j}>0\\) and \\((P^l)_ {j,i}>0\\)\n\nit is an equivalence relation\nwe can define equivalence classes\n\nA stochastic matrix \\(P\\) is irreducible if all states communicate\n\nthere is a unique communication class"
  },
  {
    "objectID": "slides/ddp.html#connectivity-and-irreducibility-example-from-qe",
    "href": "slides/ddp.html#connectivity-and-irreducibility-example-from-qe",
    "title": "Discrete Dynamic Programming",
    "section": "Connectivity and irreducibility (example from QE)",
    "text": "Connectivity and irreducibility (example from QE)\n\n\nIrreducible\n\n\nAll states can be reached with positive probably from any other initial state.\n\n\nNot irreducible\n\n\nThere is a subset of states (poor), which absorbs all the mass coming in."
  },
  {
    "objectID": "slides/ddp.html#aperiodicity",
    "href": "slides/ddp.html#aperiodicity",
    "title": "Discrete Dynamic Programming",
    "section": "Aperiodicity",
    "text": "Aperiodicity\n\nAre there cycles? Starting from a state \\(i\\), how long does it take to return to \\(i\\)?\nThe period of a state is defined as \\[gcd( {k\\geq 1 | (P^k)_{i,i}>0} )\\]\nIf a state has a period d>1 the chain returns to the state only at dates multiple of d."
  },
  {
    "objectID": "slides/ddp.html#aperiodicity-example-from-qe",
    "href": "slides/ddp.html#aperiodicity-example-from-qe",
    "title": "Discrete Dynamic Programming",
    "section": "Aperiodicity (example from QE)",
    "text": "Aperiodicity (example from QE)\n\n\nPeriodic\n\n\nIf you start from some states, you return to it, but not before two periods.\n\n\nAperiodic\n\n\nIf some mass leaves a state, some of it returns to the state in the next period."
  },
  {
    "objectID": "slides/ddp.html#stationary-distribution",
    "href": "slides/ddp.html#stationary-distribution",
    "title": "Discrete Dynamic Programming",
    "section": "Stationary distribution",
    "text": "Stationary distribution\n\n\\(\\mu\\) is a stationary distribution if \\(\\mu' = \\mu' P\\)\nTheorem: there always exists such a distribution\n\nproof: Brouwer theorem (fixed-point result for compact-convex set)\n\\(f: \\mu\\rightarrow (\\mu'P)'\\)\n\nTheorem:\n\nif P is irreducible the fixed point \\(\\mu^{\\star}\\) is unique\nif P is irreducible and aperiodic \\(|\\mu_0' P^k - \\mu^{\\star}| \\underset{k\\to+\\infty}{\\longrightarrow} 0\\) for any initial distribution \\(\\mu_0\\)\n\nWe then say the Markov chain is ergodic\n\\(\\mu^{\\star}\\) is the ergodic distribution\n\nit is the best guess, one can do for the state of the chain in the very far future"
  },
  {
    "objectID": "slides/ddp.html#stationary-distribution-proof",
    "href": "slides/ddp.html#stationary-distribution-proof",
    "title": "Discrete Dynamic Programming",
    "section": "Stationary distribution (proof)",
    "text": "Stationary distribution (proof)\n\nBrouwer’s theorem: Let \\(\\mathcal{C}\\) be a compact convex subset of \\(R^n\\) and \\(f\\) a continuous mapping \\(\\mathcal{C}\\rightarrow \\mathcal{C}\\). Then there exists a fixed point \\(x_0\\in \\mathcal{C}\\) such that \\(f(x_0)=x_0\\)\nResult hinges on:\n\ncontinuity of \\(f: \\mu \\mapsto \\mu P\\)\nconvexity of \\(\\\\{x \\in R^n | |x|=1 \\\\}\\) (easy to check)\ncompactness of \\(\\\\{x \\in R^n | |x|=1 \\\\}\\)\n\nit is bounded\nand closed (the inverse image of 1 for \\(u\\mapsto |u|\\) which is continuous)"
  },
  {
    "objectID": "slides/ddp.html#stationary-distribution-1",
    "href": "slides/ddp.html#stationary-distribution-1",
    "title": "Discrete Dynamic Programming",
    "section": "Stationary distribution?",
    "text": "Stationary distribution?\nHow do we compute the stationary distribution?\n\nSimulation\nLinear algebra\nDecomposition"
  },
  {
    "objectID": "slides/ddp.html#simulating-a-markov-chain",
    "href": "slides/ddp.html#simulating-a-markov-chain",
    "title": "Discrete Dynamic Programming",
    "section": "Simulating a Markov Chain",
    "text": "Simulating a Markov Chain\n\nVery simple idea:\n\nstart with any \\(\\mu_0\\) and compute the iterates recursively\n\\(\\mu_{n+1}' = \\mu_n' P\\)\nconvergence is linear:\n\n\\(|\\mu_{n+1} - \\mu_n| \\leq |P| |\\mu_n - \\mu_{n-1}|\\)"
  },
  {
    "objectID": "slides/ddp.html#using-linear-algebra",
    "href": "slides/ddp.html#using-linear-algebra",
    "title": "Discrete Dynamic Programming",
    "section": "Using Linear Algebra",
    "text": "Using Linear Algebra\n\nFind the solution of \\(\\mu'(P-I) = 0\\) ?\n\nnot well defined, 0 is a solution\nwe need to incorporate the constraint \\(\\sum_i(\\mu_i)=1\\)\n\nMethod:\n\nDefine \\(M_{ij} = \\begin{cases} 1 &\\text{if} & j =0 \\\\\\\\ (P-I)_{ij} & \\text{if} & j> 1 \\end{cases}\\)\nDefine \\(D_i = \\begin{cases} 1 & \\text{if} & j = 0 \\\\\\\\0 & \\text{if} & j>0 \\end{cases}\\)\nWith a linear algebra solver\n\nlook for a solution \\(\\mu\\) of \\(\\mu' M = D\\)\nor \\(M^{\\prime} \\mu = D\\prime\\)\nif you find a solution, it is unique (theorem)\n\n\nAlternative:\n\nminimize residual squares of overidentified system"
  },
  {
    "objectID": "slides/ddp.html#code-example",
    "href": "slides/ddp.html#code-example",
    "title": "Discrete Dynamic Programming",
    "section": "Code example",
    "text": "Code example\njulia [1-2|3-6|7-9|10-12|13-14|15-17] # we use the identity matrix and the \\ operator using LinearAlgebra: I, \\ # define a stochastic matrix (lines sum to 1) P = [  0.9  0.1 0.0  ;        0.05 0.9 0.05 ;        0.0  0.9 0.1  ] # define an auxiliary matrix M = P' - I M[end,:] .= 1.0 # define rhs R = zeros(3) R[end] = 1 # solve the system μ = M\\R # check that you have a solution: @assert sum(μ) == 1 @assert all(abs.(μ'P - μ').<1e-10)"
  },
  {
    "objectID": "slides/ddp.html#further-comments",
    "href": "slides/ddp.html#further-comments",
    "title": "Discrete Dynamic Programming",
    "section": "Further comments",
    "text": "Further comments\n\nKnowledge about the structure of the Markov Chain can help speedup the calculations\nThere are methods for potentially very-large linear system\n\nNewton-Krylov based methods, GMRES\n\nBasic algorithms are easy to implement by hand\nQuantEcon toolbox has very good methods to study markov chains"
  },
  {
    "objectID": "slides/ddp.html#general-formulation",
    "href": "slides/ddp.html#general-formulation",
    "title": "Discrete Dynamic Programming",
    "section": "General Formulation",
    "text": "General Formulation\n\n\n\n\nMarkov Decision Problem\n\nstates: \\(s \\in S\\)\nactions: \\(x \\in X(s)\\)\ntransitions: \\(\\pi(s'| s, x) \\in S\\)\n\n\\(probability\\) of going to \\(s'\\) in state \\(s\\)…\n… given action \\(x\\)\n\n\n\n\n\n\nReward: \\(r(s,x) \\in R\\)\n\naka felicity, intratemporal utility\n\n\nPolicy: \\(x(): s \\rightarrow x\\in X(s)\\)\n\na.k.a. decision rule\nwe consider deterministic policy\ngiven \\(x()\\), the evolution of \\(s\\) is a Markov process\n\n\\(\\pi(. |s, x())\\) is a distribution for \\(s'\\) over \\(S\\)\nit depends only on \\(s\\)"
  },
  {
    "objectID": "slides/ddp.html#objective",
    "href": "slides/ddp.html#objective",
    "title": "Discrete Dynamic Programming",
    "section": "Objective",
    "text": "Objective\n\nexpected lifetime reward:\n\nvalue of following policy \\(x()\\) starting from \\(s\\): \\[R(s; x()) =  E_0 \\sum_t^T \\delta^t \\left[ r_t\\right]\\]\n\\(\\delta \\in [0,1[\\): discount factor\nhorizon: \\(T \\in \\\\{N, \\infty\\\\}\\)\n\nvalue of a state \\(s\\)\n\nvalue of following the optimal policy starting from \\(s\\) \\[V(s) = \\max_{ x()} R(s, x())\\]\n\\(V()\\) is the value function (t.b.d.)"
  },
  {
    "objectID": "slides/ddp.html#classes-of-dynamic-optimization",
    "href": "slides/ddp.html#classes-of-dynamic-optimization",
    "title": "Discrete Dynamic Programming",
    "section": "Classes of Dynamic Optimization",
    "text": "Classes of Dynamic Optimization\n\nThe formulation so far is very general. It encompasses several variants of the problem:\n\nfinite horizon vs infinite horizon\ndiscrete-space problem vs continuous-state space problem\nsome learning problems (reinforcement learning…)\n\nThere are also variants not included:\n\nnon time-separable problems\nnon time-homogenous problems\nsome learning problems (bayesian updating, …)"
  },
  {
    "objectID": "slides/ddp.html#finite-horizon-vs-infinite-horizon",
    "href": "slides/ddp.html#finite-horizon-vs-infinite-horizon",
    "title": "Discrete Dynamic Programming",
    "section": "Finite horizon vs infinite horizon",
    "text": "Finite horizon vs infinite horizon\n\nRecall objective: \\(V(s; x()) = \\max E_0\\sum_{t=0}^T \\delta^t \\left[ r(s_t, x_t) \\right]\\)\nIf \\(T<\\infty\\), the decision in the last periods, will be different from the periods before\n\none must find a decision rule \\(\\pi_t()\\) per period\nor, equivalently, add \\(t\\) to the state space: \\(\\tilde{S}=S\\times[0,T]\\)\n\nIf \\(T=\\infty\\), the continuation value of being in state \\(s_t\\) is independent from \\(t\\)\n\n\\[V(s; x()) = E_0 \\max \\sum_ {t=0}^{T_0} \\delta^t \\left[ r(s_t, x_t) \\right] + \\delta^{T_0} E_0  \\sum_ {t=T_0}^{\\infty} \\delta^t \\left[ r(s_t, x_t) \\right]\\]\n\\[ = E_0 \\left[ \\max \\sum_ {t=0}^{T_0} \\delta^t \\left[ r(s_t, x_t) \\right] +  \\delta^{T_0} V(s_ {T_0}; x()) \\right]\\]"
  },
  {
    "objectID": "slides/ddp.html#continuous-vs-discrete",
    "href": "slides/ddp.html#continuous-vs-discrete",
    "title": "Discrete Dynamic Programming",
    "section": "Continuous vs discrete",
    "text": "Continuous vs discrete\n\nDiscrete Dynamic Programming (today)\n\ndiscrete states: \\(s \\in {s_1, \\cdots, s_N}\\)\ndiscrete controls: \\(|X(s)|<\\infty\\)\nthere is a finite number of policies, the can be represented exactly\nunless \\(|S|\\) is very large (cf go game)\n\nContinuous problem:\n\n\\(x(s)\\), \\(V(s; \\pi)\\) require an infinite number of coefficients\nsame general approach but different implementation\ntwo main variants:\n\ndiscretize the initial problem: back to DDP\nuse approximation techniques (i.e. interpolation)"
  },
  {
    "objectID": "slides/ddp.html#non-time-separable-example",
    "href": "slides/ddp.html#non-time-separable-example",
    "title": "Discrete Dynamic Programming",
    "section": "Non time separable example",
    "text": "Non time separable example\n\nFor instance Epstein-Zin preferences: \\[\\max V(;c())\\] where \\[V_t = (1-\\delta) \\frac{c_t^{1-\\sigma}}{1-\\sigma} + \\delta \\left[ E_t V_{t+1}^{\\alpha} \\right]^{\\frac{1}{\\alpha}}\\]\nWhy would you do that?\n\nto disentangle risk aversion and elasticity of intertemporal substitution\nrobust control\n\nYou can still use ideas from Dynamic Programming."
  },
  {
    "objectID": "slides/ddp.html#non-homogenous-preference",
    "href": "slides/ddp.html#non-homogenous-preference",
    "title": "Discrete Dynamic Programming",
    "section": "Non homogenous preference",
    "text": "Non homogenous preference\n\nLook at the \\(\\alpha-\\beta\\) model. \\[V_t = \\max \\sum_t^{\\infty} \\beta_t U(c_t)\\] where \\(\\delta_0 = 1\\), \\(\\delta_1=\\alpha\\), \\(\\delta_k=\\alpha\\beta^{k-1}\\)\nMakes the problem time-inconsistent:\n\nthe optimal policy you would choose for the continuation value after \\(T\\) is not the same if you maximize it in expectation from \\(0\\) or at \\(T\\)."
  },
  {
    "objectID": "slides/ddp.html#learning-problems",
    "href": "slides/ddp.html#learning-problems",
    "title": "Discrete Dynamic Programming",
    "section": "Learning problems",
    "text": "Learning problems\n\nBayesian learning: Uncertainty about some model parameters\n\nex: variance and return of a stock market\nagent models this uncertainty as a distribution\nagent updates his priors after observing the result of his actions\nactions are taken optimally taken into account the revelation power of some actions\n\nIs it good?\n\nclean: the rational thing to do with uncertainty\nsuper hard: the state-space should contain all possible priors\nmathematical cleanness comes with many assumptions\n\nUsed to estimate rather big (mostly linear) models"
  },
  {
    "objectID": "slides/ddp.html#learning-problems-2",
    "href": "slides/ddp.html#learning-problems-2",
    "title": "Discrete Dynamic Programming",
    "section": "Learning problems (2)",
    "text": "Learning problems (2)\n\nReinforcement learning\n\nmodel can be partially or totally unknown\ndecision rule is updated by observing the reward from actions\n\nno priors\n\nsolution does not derive directly from model\n\ncan be used to solve dynamic programming problems\n\n\nGood solutions maximize a criterion similar to lifetime reward but are usually not optimal:\n\nusually evaluated by replaying the game many times\ntradeoff exploration / exploitations"
  },
  {
    "objectID": "slides/ddp.html#finite-horizon-dmdp-1",
    "href": "slides/ddp.html#finite-horizon-dmdp-1",
    "title": "Discrete Dynamic Programming",
    "section": "Finite horizon DMDP",
    "text": "Finite horizon DMDP\nWhen \\(T<\\infty\\). With discrete action the problem can be represented by a tree.\n[GRAPH]"
  },
  {
    "objectID": "slides/ddp.html#finite-horizon-dmdp-2",
    "href": "slides/ddp.html#finite-horizon-dmdp-2",
    "title": "Discrete Dynamic Programming",
    "section": "Finite horizon DMDP",
    "text": "Finite horizon DMDP\n\nIntuition: backward induction.\n\nFind optimal policy \\(x_T(s_T)\\) in all terminal states \\(s_T\\). Set \\(V_T(s_T)\\) equal to \\(r(s_T, \\pi_T)\\)\nFor each state \\(s_{k-1}\\in S\\) find \\(x_{k-1}\\in X(s_{k-1})\\) which maximizes \\[V_{k-1}(s_{k-1}) = \\max_{x_{k-1}(s_{k-1})\\in X(s_{k-1})}r(s_{k-1},x_{k-1}) + \\delta \\underbrace{ \\sum_{s_k\\in S} \\pi(s_k | s_{k-1}, x_{k-1} ) V_k(s_k)} _{ \\textit{expected continuation value} }\\]\n\nPolicies \\(x_0(), ... x_T()\\) are Markov-perfect:\n\nthey maximize utility on all subsets of the “game”\nalso from t=0"
  },
  {
    "objectID": "slides/ddp.html#remarks",
    "href": "slides/ddp.html#remarks",
    "title": "Discrete Dynamic Programming",
    "section": "Remarks",
    "text": "Remarks\n\nCan we do better than this naive algorithm?\n\nnot really\nbut we can try to limit \\(S\\) to make the maximization step faster\nexclude a priori some branches in the tree using knowledge of the problem"
  },
  {
    "objectID": "slides/ddp.html#infinite-horizon-dmdp-1",
    "href": "slides/ddp.html#infinite-horizon-dmdp-1",
    "title": "Discrete Dynamic Programming",
    "section": "Infinite horizon DMDP",
    "text": "Infinite horizon DMDP\n\nHorizon is infinite: \\[V(s; x()) =  \\max E_0 \\sum_{t=0}^{\\infty} \\delta^t r(s_t, x_t) \\]\nIntuition:\n\nlet’s consider the finite horizon version \\(T<\\infty\\) and \\(T >> 1\\)\ncompute the solution, increase \\(T\\) until the solution doesn’t change\nin practice: take an initial guess for \\(V_{T}\\) then compute optimal \\(V_{T-1}\\), \\(V_{T_2}\\) and so on, until convergence of the \\(V\\)s"
  },
  {
    "objectID": "slides/ddp.html#infinite-horizon-dmdp-2",
    "href": "slides/ddp.html#infinite-horizon-dmdp-2",
    "title": "Discrete Dynamic Programming",
    "section": "Infinite horizon DMDP (2)",
    "text": "Infinite horizon DMDP (2)\n\nThis is possible, it’s called Successive Approximation or Value Function Iteration\n\nhow fast does it converge? linearly\ncan we do better? yes, quadratically\n\nwith howard improvement steps"
  },
  {
    "objectID": "slides/ddp.html#successive-approximation",
    "href": "slides/ddp.html#successive-approximation",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation",
    "text": "Successive Approximation\n\nConsider the decomposition: \\[V(s; x()) = E_0 \\sum_{t=0}^{\\infty} \\delta^t r(s_t, x_t) = E_0 \\left[ r(s, x(s)) + \\sum_{t=1}^{\\infty} \\delta^t r(s_t, x_t) \\right]\\]\n\nor\n\\[V(s; x()) =  r(s, x(s)) + \\delta \\sum_{s'} p(s'|s,x(s)) V(s'; x()) \\]"
  },
  {
    "objectID": "slides/ddp.html#successive-approximation-2",
    "href": "slides/ddp.html#successive-approximation-2",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation (2)",
    "text": "Successive Approximation (2)\n\nTaking continuation value as given we can certainly improve the value in every state \\(\\tilde{V}\\) by choosing \\(\\tilde{x}()\\) so as to maximze \\[\\tilde{V}(s; x(), \\tilde{x}()) =  r(s, \\tilde{x}(s)) + \\delta \\sum_{s'} \\pi(s'|s,\\tilde{x}(s) )V(s'; x()) \\]\nBy construction: \\(\\forall s, \\tilde{V}(s, \\tilde{x}(), x()) > {V}(s, x())\\)\n\nit is an improvement step\n\nCan \\({V}(s, \\tilde{x}())\\) be worse for some states than \\({V}(s, \\tilde{x}())\\) ?\n\nactually no"
  },
  {
    "objectID": "slides/ddp.html#bellman-equation",
    "href": "slides/ddp.html#bellman-equation",
    "title": "Discrete Dynamic Programming",
    "section": "Bellman equation",
    "text": "Bellman equation\n\nIdea:\n\nit should not be possible to improve upon the optimal solution.\nHence the optimal value \\(V\\) and policy \\(x^{\\star}\\) should satisfy: \\[\\forall s\\in S, V(s) = \\max_{y(s)} r(s, y(s)) + \\delta \\sum_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, y(s)) V(s^{\\prime})\\] with the maximum attained at \\(x(s)\\).\n\nThis is referred to as the Bellman equation.\nConversely, it is possible to show that a solution to the Bellman equation is also an optimal solution to the initial problem."
  },
  {
    "objectID": "slides/ddp.html#bellman-operator",
    "href": "slides/ddp.html#bellman-operator",
    "title": "Discrete Dynamic Programming",
    "section": "Bellman operator",
    "text": "Bellman operator\n\nThe function \\(G\\) is known as the Bellman operator: \\[G: V \\rightarrow \\max_{y(s)} r(s, y(s)) + \\delta \\sum_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, y(s)) V(s^{\\prime})\\]\nDefine sequence \\(V_n = G(V_{n-1})\\)\n\nit goes back in time\nbut is not the time-iteration operator\n\nOptimal value is a fixed point of G\nDoes \\(G\\) converges to it ? Yes, if \\(G\\) is a contraction mapping."
  },
  {
    "objectID": "slides/ddp.html#blackwells-theorem",
    "href": "slides/ddp.html#blackwells-theorem",
    "title": "Discrete Dynamic Programming",
    "section": "Blackwell’s theorem",
    "text": "Blackwell’s theorem\n\nLet \\(X\\subset R^n\\) and let \\(\\mathcal{C}(X)\\) be a space of bounded functions \\(f: X\\rightarrow R\\), with the sup-metric. \\(B: \\mathcal{C}(X)\\rightarrow \\mathcal{C}(X)\\) be an operator satisfying two conditions:\n\n(monotonicity) if \\(f,g \\in \\mathcal{C}(X)\\) and \\(\\forall x\\in X, f(x)\\leq g(x)\\) then\n\n\\(\\forall x \\in X (Bf)(x)\\leq(Bg)(x)\\)\n\n(discounting) there exists some \\(\\delta\\in]0,1[\\) such that: \\(B.(f+a)(x)\\leq (B.f)(x) + \\delta a, \\forall f \\in \\mathcal{C}(X), a\\geq 0, x\\in X\\)\n\nThen \\(B\\) is a contraction mapping with modulus \\(\\delta\\)."
  },
  {
    "objectID": "slides/ddp.html#successive-approximation-1",
    "href": "slides/ddp.html#successive-approximation-1",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation",
    "text": "Successive Approximation\n\nUsing the Blackwell’s theorem, we can prove the Bellman operator is a contraction mapping (do it).\nThis justifies the Value Function Iteration algorithm:\n\nchoose an initial \\(V_0\\)\ngiven \\(V_n\\) compute \\(V_{n+1} = G(V_n)\\)\niterate until \\(|V_{n+1}- V_n|\\leq \\eta\\)\n\nPolicy rule is deduced from \\(V\\) as the maximand in the Bellman step"
  },
  {
    "objectID": "slides/ddp.html#successive-approximation-2-1",
    "href": "slides/ddp.html#successive-approximation-2-1",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation (2)",
    "text": "Successive Approximation (2)\n\nNote that convergence of \\(V_n\\) is geometric\nBut \\(x_n\\) converges after a finite number of iterations (\\(X\\) is finite)\n\nsurely the latest iterations are suboptimal\nthey serve only to evaluate the value of \\(x^{\\star}\\)\n\nIn fact:\n\n\\(V_n\\) is never the value of \\(x_n()\\)\nshould we try to keep both in sync?"
  },
  {
    "objectID": "slides/ddp.html#policy-iteration-for-dmdp",
    "href": "slides/ddp.html#policy-iteration-for-dmdp",
    "title": "Discrete Dynamic Programming",
    "section": "Policy iteration for DMDP",
    "text": "Policy iteration for DMDP\n\nChoose initial policy \\(x_0()\\)\nGiven initial guess \\(x_n()\\)\n\ncompute the value function \\(V_n=V( ;x_n)\\) which satisfies\n\\(\\forall s, V_n(s) = r(s, x_n(s)) + \\delta \\sum_{s'} \\pi(s'| s, x_n(s)) V_n(s')\\)\nimprove policy by maximizing in \\(x_n()\\) \\[\\max_{x_n()} r(s, x_n(s)) + \\delta \\sum_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, x_n(s)) V_{n-1}(s^{\\prime})\\]\n\nRepeat until convergence, i.e. \\(x_n=x_{n+1}\\)\nOne can show the speed of convergence (for \\(V_n\\)) is quadratic\n\nit corresponds the Newton-Raphson steps applied to \\(V\\rightarrow G(V)-V\\)\nwe’ll see Newton-Raphson in the next session (on optimization)"
  },
  {
    "objectID": "slides/ddp.html#how-do-we-compute-the-value-of-a-policy",
    "href": "slides/ddp.html#how-do-we-compute-the-value-of-a-policy",
    "title": "Discrete Dynamic Programming",
    "section": "How do we compute the value of a policy?",
    "text": "How do we compute the value of a policy?\n\nGiven \\(x_n\\), goal is to find \\(V_n(s)\\) in \\[\\forall s,  V_n(s) = r(s, x_n(s)) + \\delta \\sum_{s'} \\pi(s'| s, x_n(s)) V_n(s')\\]\nTwo(three) approaches:\n\nsimulate the policy rule and compute \\(E\\left[ \\sum_t \\delta^t r(s_t, x_t) \\right]\\) with Monte-Carlo draws\nsuccessive approximation:\n\nput \\(V_k\\) in the rhs and recompute the lhs \\(V_{k+1}\\), replace \\(V_k\\) by \\(V_{k+1}\\) and iterate until convergence\n\nsolve a linear system in \\(V_n\\)\n\nFor 2 and 3 it helps representing a linear operator \\(M\\) such that \\(V_{n+1} = R_n + \\delta M_n . V_n\\)"
  },
  {
    "objectID": "slides/ddp.html#idea",
    "href": "slides/ddp.html#idea",
    "title": "Discrete Dynamic Programming",
    "section": "Idea",
    "text": "Idea\n\nMcCall model:\n\nwhen should an unemployed person accept a job offer?\nchoice between:\n\nwait for a better offer (and receive low unemp. benefits)\naccept a suboptimal job offer\n\n\nWe present a variant of it, with a small probability of loosing a job."
  },
  {
    "objectID": "slides/ddp.html#formalization",
    "href": "slides/ddp.html#formalization",
    "title": "Discrete Dynamic Programming",
    "section": "Formalization",
    "text": "Formalization\n\nWhen unemployed in date, a job-seeker\n\nconsumes unemployment benefit \\(c_t = \\underline{c}\\)\nreceives in every date \\(t\\) a job offer \\(w_t\\)\n\n\\(w_t\\) is i.i.d.,\ntakes values \\(w_1, w_2, w_3\\) with probabilities \\(p_1, p_2, p_3\\)\n\nif job-seeker accepts, becomes employed at rate \\(w_t\\) in the next period\nelse he stays unemployed\n\nWhen employed at rate \\(w\\)\n\nworker consumes salary \\(c_t = w\\)\nwith small probability \\(\\lambda>0\\) looses his job:\n\nstarts next period unemployed\n\notherwise stays employed at same rate\n\nObjective: \\(\\max E_0 \\left\\\\{ \\sum \\beta^t \\log(w_t) \\right\\\\}\\)"
  },
  {
    "objectID": "slides/ddp.html#states-reward",
    "href": "slides/ddp.html#states-reward",
    "title": "Discrete Dynamic Programming",
    "section": "States / reward",
    "text": "States / reward\n\nWhat are the states?\n\nemployement status: Unemployed / Employed\nif Unemployed:\n\nthe level \\(w\\in {w_1, w_2, w_3}\\) of the salary that is currently proposed\n\nif Employed:\n\nthe level \\(w\\in {w_1, w_2, w_3}\\) at which worker was hired\n\ncurrent state, can be represented by a 2x3 index\n\nWhat are the actions?\n\nif Unemployed:\n\nreject (false) / accept (true)\n\nif Employed: None\nactions (when unemployed) are represented by a 3 elements binary vector\n\nWhat is the (intratemporal) reward?\n\nif Unemployed: \\(U(c)\\)\nif Employed at rate w: \\(U(w)\\)\nhere it doesn’t depend on the action"
  },
  {
    "objectID": "slides/ddp.html#value-function",
    "href": "slides/ddp.html#value-function",
    "title": "Discrete Dynamic Programming",
    "section": "Value function",
    "text": "Value function\n\\(\\newcommand{\\E}{\\mathbb{E}}\\)\n\nWhat is the value of being in a given state?\nIf Unemployed, facing current offer \\(w\\):\n\\[V^U(w) = U(\\underline{c}) + \\max_{a} \\begin{cases} \\beta V^E(w) & \\text{if $a(w)$ is true} \\\\\\\\ \\beta  \\\\E_{a'}\\left[ V^U(a^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\]\nIf Employed, at rate \\(w\\) \\[V^E(w) = U(w) +  (1-\\lambda) \\beta V^E(w) +  \\lambda \\beta \\\\E_{a'}\\left[ V^U(a^{\\prime}) \\right] \\]\nWe can represent value as two functions \\(V^U\\) and \\(V^E\\) of the states as\n\ntwo vectors of Floats, with three elements (recall: value-function is real valued)"
  },
  {
    "objectID": "slides/ddp.html#value-function-iteration",
    "href": "slides/ddp.html#value-function-iteration",
    "title": "Discrete Dynamic Programming",
    "section": "Value function iteration",
    "text": "Value function iteration\n\nTake a guess for value function \\(\\tilde{V^E}\\), \\(\\tilde{V^U}\\), tomorrow\nUse it to compute value function today: \\[V^U(w) = U(\\underline{c}) + \\max_{a(w)} \\begin{cases} \\beta \\tilde{V}^E(w) & \\text{if $a(w)$ is true} \\\\\\\\ \\beta  \\\\E_{a'}\\left[ \\tilde{V}^U(a^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V^E(w) = U(w) +  (1-\\lambda) \\beta \\tilde{V}^E(w) +  \\lambda \\beta \\\\E_{a'}\\left[\\tilde{V}^U(a^{\\prime}) \\right] \\]\n\\((\\tilde{V}^E, \\tilde{V}^U)\\mapsto (V^E, V^U)\\) is one value iteration step\nNote that we don’t have to keep track of policies tomorrow\n\nall information about future decisions is contained in \\(\\tilde{V}^E, \\tilde{V}^U\\)\nbut we can keep track of current policy: \\(a(w): \\arg\\max \\cdots\\)"
  },
  {
    "objectID": "slides/ddp.html#value-evaluation",
    "href": "slides/ddp.html#value-evaluation",
    "title": "Discrete Dynamic Programming",
    "section": "Value evaluation",
    "text": "Value evaluation\n\nSuppose we take a policy \\(a(w)\\) as given. What is the value of following this policy forever?\nThe value function \\(V_a^E\\), \\(V_a^U\\) satisfies \\[V_a^U(w) = U(\\underline{c}) + \\begin{cases} \\beta \\tilde{V}^E_a(w) & \\text{if $a(w)$ is true} \\\\\\\\ \\beta  \\\\E_{w'}\\left[ \\tilde{V}^U_a(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V_a^E(w) = U(w) +  (1-\\lambda) \\beta \\tilde{V}^E_a(w) +  \\lambda \\beta \\\\E_{w'}\\left[\\tilde{V}^U_a(w^{\\prime}) \\right] \\]\nNote the absence of the max function: we don’t reoptimize"
  },
  {
    "objectID": "slides/ddp.html#value-evaluation-2",
    "href": "slides/ddp.html#value-evaluation-2",
    "title": "Discrete Dynamic Programming",
    "section": "Value evaluation (2)",
    "text": "Value evaluation (2)\n\nHow do you compute value of policy \\(a(w)\\) recursively?\nIterate: \\((\\tilde{V}^E_a, \\tilde{V}^U)\\mapsto (V^E_a, V^U_a)\\) \\[V_a^U(w) \\leftarrow U(\\underline{c}) + \\begin{cases} \\beta \\tilde{V}^E_a(w) & \\text{if $a(w)$ is true} \\\\\\\\ \\beta  \\\\E_{w'}\\left[ \\tilde{V}^U_a(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V_a^E(w) \\leftarrow U(w) +  (1-\\lambda) \\beta \\tilde{V}^E_a(w) +  \\lambda \\beta \\\\E_{w'}\\left[\\tilde{V}^U_a(w^{\\prime}) \\right] \\]\nNote the absence of the max function:\n\nwe don’t reoptimize\nwe we keep the same policy all along"
  },
  {
    "objectID": "slides/ddp.html#policy-iteration",
    "href": "slides/ddp.html#policy-iteration",
    "title": "Discrete Dynamic Programming",
    "section": "Policy iteration",
    "text": "Policy iteration\n\nstart with policy \\(a(w)\\)\nevaluate the value of this policy \\(V^E_a, V^U_a\\)\n\ncompute the optimal policy in the Bellman iteration\nkeep the improved policy \\(a(w)\\)\n\nhere: \\(a(w) = \\arg\\max_{a(w)} \\begin{cases} \\beta \\tilde{V}^E(w)\\\\\\\\ \\beta \\\\E_{a'}\\left[ \\tilde{V}^U(a^{\\prime}) \\right] \\end{cases}\\)\n\n\niterate until \\(a(w)\\) converges"
  }
]